{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf1158-aa87-488d-be26-3b89452a8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FULL CORRECTED MOTOR LEARNING ANALYSIS CLASSES\n",
    "# Fixed column naming bugs and simplified age handling\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import tempfile\n",
    "import webbrowser\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, mannwhitneyu\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, confusion_matrix, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the motor learning analysis.\"\"\"\n",
    "    \n",
    "    # File processing\n",
    "    MIN_COMPLETE_STRIDES = 20\n",
    "    PROCESSED_DATA_FILE = 'processed_data.pkl'\n",
    "    FIGURES_DIR = Path('figures')\n",
    "    \n",
    "    # Trial type mappings\n",
    "    TRIAL_TYPE_MAPPING = {\n",
    "        'primer': 'vis1',\n",
    "        'trial': 'invis', \n",
    "        'vis': 'vis2',\n",
    "        'pref': 'pref'\n",
    "    }\n",
    "    \n",
    "    # Analysis parameters\n",
    "    MOTOR_NOISE_THRESHOLD = 0.3\n",
    "    SUCCESS_RATE_THRESHOLD = 0.68\n",
    "    TARGET_SIZE_THRESHOLD = 0.31\n",
    "    MAX_STRIDES_THRESHOLD = 415\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "class DataUtils:\n",
    "    \"\"\"Utility functions for data processing.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_and_validate_file(file_path: Path, required_cols: set = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load and validate a single data file.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            if required_cols and not required_cols.issubset(df.columns):\n",
    "                return None\n",
    "                \n",
    "            # Basic cleaning\n",
    "            if 'Stride Number' in df.columns:\n",
    "                df['Stride Number'] = pd.to_numeric(df['Stride Number'], errors='coerce')\n",
    "                df = df.dropna(subset=['Stride Number'])\n",
    "                df = df.drop_duplicates(subset=['Stride Number'])\n",
    "            \n",
    "            return df if not df.empty else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file_path.name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_anomalies(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Detect and flag anomalies in stride data.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return df, {}\n",
    "\n",
    "        df = df.copy()\n",
    "        df['Anomalous'] = False\n",
    "        anomalies = {}\n",
    "\n",
    "        # Time-based anomalies\n",
    "        time_col = next((col for col in ['Time', 'Timestamp', 'Time (s)'] \n",
    "                        if col in df.columns), None)\n",
    "        if time_col:\n",
    "            df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "            time_diff = df[time_col].diff()\n",
    "            jump_mask = time_diff > time_diff.quantile(0.99) * 5\n",
    "            \n",
    "            for idx in df.index[jump_mask.fillna(False)]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('time_jump')\n",
    "\n",
    "        # Sum of gains and steps anomalies\n",
    "        if 'Sum of gains and steps' in df.columns:\n",
    "            high_mask = df['Sum of gains and steps'] > 4\n",
    "            zero_mask = df['Sum of gains and steps'] == 0\n",
    "\n",
    "            for idx in df.index[high_mask]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('sum_gain_step_high')\n",
    "            \n",
    "            for idx in df.index[zero_mask]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('sum_gain_step_zero')\n",
    "\n",
    "        # Duplicate rows\n",
    "        duplicated_mask = df.duplicated()\n",
    "        for idx in df.index[duplicated_mask]:\n",
    "            df.at[idx, 'Anomalous'] = True\n",
    "            anomalies.setdefault(idx, []).append('duplicate_row')\n",
    "\n",
    "        return df, anomalies\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. TRIAL PROCESSING\n",
    "# ==============================================================================\n",
    "\n",
    "class TrialProcessor:\n",
    "    \"\"\"Handles loading, combining, and processing of trial data.\"\"\"\n",
    "    \n",
    "    def __init__(self, debug: bool = True):\n",
    "        self.debug = debug\n",
    "        \n",
    "    def find_and_combine_trial_files(self, subject_dir: Path, trial_prefix: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Find and combine trial files for a given trial type.\"\"\"\n",
    "        all_files = sorted(subject_dir.glob(f\"{trial_prefix}*.txt\"))\n",
    "        \n",
    "        if not all_files:\n",
    "            if self.debug:\n",
    "                print(f\"  ⚠️ No files found for {trial_prefix}\")\n",
    "            return None\n",
    "        \n",
    "        # Special handling for preference trials\n",
    "        if trial_prefix == 'pref':\n",
    "            return self._handle_pref_trial(all_files)\n",
    "        \n",
    "        # Single file case\n",
    "        if len(all_files) == 1:\n",
    "            return DataUtils.load_and_validate_file(all_files[0])\n",
    "        \n",
    "        # Multiple files - combine them\n",
    "        return self._combine_trial_fragments(all_files)\n",
    "    \n",
    "    def _handle_pref_trial(self, files: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Handle preference trial - select largest file.\"\"\"\n",
    "        largest_file = max(files, key=lambda f: f.stat().st_size)\n",
    "        if self.debug and len(files) > 1:\n",
    "            print(f\"  ⚡ pref trial - selected largest of {len(files)} files\")\n",
    "        return DataUtils.load_and_validate_file(largest_file)\n",
    "    \n",
    "    def _combine_trial_fragments(self, files: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Combine multiple trial fragments intelligently.\"\"\"\n",
    "        file_info = []\n",
    "        \n",
    "        for f in files:\n",
    "            try:\n",
    "                with open(f, 'r') as file:\n",
    "                    header = file.readline().strip().split('\\t')\n",
    "                    stride_col = next((i for i, col in enumerate(header) \n",
    "                                     if 'stride' in col.lower() and \n",
    "                                     ('num' in col.lower() or 'no' in col.lower())), None)\n",
    "                    \n",
    "                    if stride_col is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get stride range\n",
    "                    lines = file.readlines()\n",
    "                    first_stride = float(lines[0].split('\\t')[stride_col])\n",
    "                    last_stride = float(lines[-1].split('\\t')[stride_col])\n",
    "                    \n",
    "                    file_info.append({\n",
    "                        'path': f,\n",
    "                        'first': first_stride,\n",
    "                        'last': last_stride,\n",
    "                        'size': f.stat().st_size\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not file_info:\n",
    "            return DataUtils.load_and_validate_file(max(files, key=lambda f: f.stat().st_size))\n",
    "        \n",
    "        # Find best continuous sequence\n",
    "        file_info.sort(key=lambda x: x['first'])\n",
    "        best_sequence = self._find_best_sequence(file_info)\n",
    "        \n",
    "        if len(best_sequence) >= 2:\n",
    "            return self._merge_files([f['path'] for f in best_sequence])\n",
    "        \n",
    "        # Fallback to largest file\n",
    "        return DataUtils.load_and_validate_file(max(file_info, key=lambda x: x['size'])['path'])\n",
    "    \n",
    "    def _find_best_sequence(self, file_info: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find the best continuous sequence of files.\"\"\"\n",
    "        best_sequence = []\n",
    "        current_sequence = [file_info[0]]\n",
    "        \n",
    "        for file_data in file_info[1:]:\n",
    "            if file_data['first'] == current_sequence[-1]['last'] + 1:\n",
    "                current_sequence.append(file_data)\n",
    "            else:\n",
    "                if len(current_sequence) > len(best_sequence):\n",
    "                    best_sequence = current_sequence\n",
    "                current_sequence = [file_data]\n",
    "        \n",
    "        return max([best_sequence, current_sequence], key=len)\n",
    "    \n",
    "    def _merge_files(self, file_paths: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Merge multiple files into a single DataFrame.\"\"\"\n",
    "        dfs = []\n",
    "        for f in file_paths:\n",
    "            df = DataUtils.load_and_validate_file(f)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "        \n",
    "        if not dfs:\n",
    "            return None\n",
    "        \n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Clean and sort\n",
    "        if 'Stride Number' in combined.columns:\n",
    "            combined = combined.sort_values('Stride Number')\n",
    "            combined = combined.drop_duplicates('Stride Number')\n",
    "        \n",
    "        return combined\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAIN DATA MANAGER\n",
    "# ==============================================================================\n",
    "\n",
    "class MotorLearningDataManager:\n",
    "    \"\"\"Main class for managing motor learning data.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path: str, data_root_dir: str, \n",
    "                 force_reprocess: bool = False, debug: bool = True):\n",
    "        self.metadata_path = metadata_path\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Initialize components\n",
    "        self.trial_processor = TrialProcessor(debug=debug)\n",
    "        self.config = Config()\n",
    "        \n",
    "        # Data storage\n",
    "        self.metadata = None\n",
    "        self.processed_data = {}\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.config.FIGURES_DIR, exist_ok=True)\n",
    "        \n",
    "        # Load or process data\n",
    "        if not force_reprocess and os.path.exists(self.config.PROCESSED_DATA_FILE):\n",
    "            self._load_processed_data()\n",
    "        else:\n",
    "            self._process_all_data()\n",
    "            self._save_processed_data()\n",
    "    \n",
    "    def _load_processed_data(self):\n",
    "        \"\"\"Load previously processed data.\"\"\"\n",
    "        with open(self.config.PROCESSED_DATA_FILE, 'rb') as f:\n",
    "            self.processed_data = pickle.load(f)\n",
    "            \n",
    "        # Rebuild metadata DataFrame\n",
    "        self.metadata = pd.DataFrame.from_dict(\n",
    "            {subj: data['metadata'] for subj, data in self.processed_data.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "    \n",
    "    def _save_processed_data(self):\n",
    "        \"\"\"Save processed data.\"\"\"\n",
    "        with open(self.config.PROCESSED_DATA_FILE, 'wb') as f:\n",
    "            pickle.dump(self.processed_data, f)\n",
    "    \n",
    "    def _process_all_data(self):\n",
    "        \"\"\"Process all subject data.\"\"\"\n",
    "        self._load_metadata()\n",
    "        total_subjects = len(self.metadata)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"🔄 Processing {total_subjects} subjects...\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(self.metadata.iterrows(), 1):\n",
    "            subject_id = row['ID']\n",
    "            if self.debug:\n",
    "                print(f\"\\n[{i}/{total_subjects}] Processing {subject_id}...\")\n",
    "            \n",
    "            subject_data = self._process_subject_data(subject_id, row)\n",
    "            if subject_data:\n",
    "                self.processed_data[subject_id] = subject_data\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load and clean metadata.\"\"\"\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        self.metadata['DOB'] = pd.to_datetime(self.metadata['DOB'], errors='coerce')\n",
    "        self.metadata['Session Date'] = pd.to_datetime(self.metadata['Session Date'], errors='coerce')\n",
    "        self.metadata = self.metadata.dropna(subset=['ID', 'age_months'])\n",
    "    \n",
    "    def _process_subject_data(self, subject_id: str, metadata_row: pd.Series) -> Optional[Dict]:\n",
    "        \"\"\"Process data for a single subject.\"\"\"\n",
    "        subject_dir = Path(self.data_root_dir) / subject_id\n",
    "        if not subject_dir.exists():\n",
    "            if self.debug:\n",
    "                print(f\"❌ Directory not found: {subject_dir}\")\n",
    "            return None\n",
    "        \n",
    "        trial_data = {}\n",
    "        \n",
    "        for original_type in ['primer', 'trial', 'vis', 'pref']:\n",
    "            try:\n",
    "                # Load trial data\n",
    "                df = self.trial_processor.find_and_combine_trial_files(subject_dir, original_type)\n",
    "                \n",
    "                if df is not None:\n",
    "                    # Process the data\n",
    "                    processed_df, anomalies = self._process_trial_data(df, original_type)\n",
    "                    \n",
    "                    # Store with mapped name\n",
    "                    new_type = self.config.TRIAL_TYPE_MAPPING[original_type]\n",
    "                    trial_data[new_type] = {\n",
    "                        'data': processed_df,\n",
    "                        'anomalies': anomalies\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {subject_id}/{original_type}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            'metadata': metadata_row.to_dict(),\n",
    "            'trial_data': trial_data\n",
    "        } if trial_data else None\n",
    "    \n",
    "    def _process_trial_data(self, df: pd.DataFrame, trial_type: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Process trial data and calculate metrics.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return None, {}\n",
    "        \n",
    "        # Skip processing for pref trials (just clean duplicates)\n",
    "        if trial_type == 'pref':\n",
    "            df = df.drop_duplicates(subset='Left heel strike', keep='last')\n",
    "            return df, {}\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = ['Stride Number', 'Success', 'Upper bound success', \n",
    "                        'Lower bound success', 'Constant']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Missing required columns: {missing_cols}\")\n",
    "            return None, {}\n",
    "        \n",
    "        try:\n",
    "            # Process trial data\n",
    "            df = df.sort_values('Stride Number')\n",
    "            df['Target size'] = df['Upper bound success'] - df['Lower bound success']\n",
    "            df = df.drop_duplicates(subset='Stride Number', keep='last')\n",
    "            \n",
    "            # Scale sum of gains and steps\n",
    "            if 'Sum of gains and steps' in df.columns:\n",
    "                df['Sum of gains and steps'] = 1.5 * df['Sum of gains and steps']\n",
    "            \n",
    "            # Detect anomalies\n",
    "            df, anomalies = DataUtils.detect_anomalies(df)\n",
    "            \n",
    "            return df, anomalies\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {trial_type} data: {str(e)}\")\n",
    "            return None, {}\n",
    "    \n",
    "    def filter_trials(self, max_target_size=None, min_age=None, max_age=None, \n",
    "                     required_trial_types=None, min_strides=None, max_strides=None):\n",
    "        \"\"\"Filter trials based on specified criteria.\"\"\"\n",
    "        filtered_data = {}\n",
    "        \n",
    "        for subject_id, subject_data in self.processed_data.items():\n",
    "            # SIMPLIFIED AGE FILTERING - always use age_months/12\n",
    "            age = subject_data['metadata']['age_months'] / 12\n",
    "            if min_age is not None and age < min_age:\n",
    "                continue\n",
    "            if max_age is not None and age > max_age:\n",
    "                continue\n",
    "            \n",
    "            # Check required trial types\n",
    "            if required_trial_types:\n",
    "                missing_trials = [\n",
    "                    t for t in required_trial_types \n",
    "                    if t not in subject_data['trial_data'] or \n",
    "                       subject_data['trial_data'][t] is None or\n",
    "                       subject_data['trial_data'][t]['data'] is None\n",
    "                ]\n",
    "                if missing_trials:\n",
    "                    continue\n",
    "            \n",
    "            # Check other criteria\n",
    "            valid_subject = True\n",
    "            filtered_trial_data = {}\n",
    "            \n",
    "            for trial_type, trial_dict in subject_data['trial_data'].items():\n",
    "                if trial_dict and trial_dict['data'] is not None:\n",
    "                    df = trial_dict['data']\n",
    "                    \n",
    "                    # Apply filters\n",
    "                    if (max_target_size is not None and \n",
    "                        'Target size' in df.columns and \n",
    "                        df['Target size'].min() > max_target_size):\n",
    "                        valid_subject = False\n",
    "                        break\n",
    "                    \n",
    "                    n_strides = len(df)\n",
    "                    if ((min_strides is not None and n_strides < min_strides) or\n",
    "                        (max_strides is not None and n_strides > max_strides)):\n",
    "                        valid_subject = False\n",
    "                        break\n",
    "                    \n",
    "                    # Include valid trial\n",
    "                    filtered_trial_data[trial_type] = {\n",
    "                        'data': df.copy(),\n",
    "                        'anomalies': trial_dict['anomalies'].copy()\n",
    "                    }\n",
    "            \n",
    "            if valid_subject and filtered_trial_data:\n",
    "                filtered_data[subject_id] = {\n",
    "                    'metadata': subject_data['metadata'].copy(),\n",
    "                    'trial_data': filtered_trial_data\n",
    "                }\n",
    "        \n",
    "        # Create new instance with filtered data\n",
    "        new_instance = MotorLearningDataManager.__new__(MotorLearningDataManager)\n",
    "        new_instance.config = self.config\n",
    "        new_instance.trial_processor = self.trial_processor\n",
    "        new_instance.metadata_path = self.metadata_path\n",
    "        new_instance.data_root_dir = self.data_root_dir\n",
    "        new_instance.debug = self.debug\n",
    "        new_instance.processed_data = filtered_data\n",
    "        new_instance.metadata = pd.DataFrame.from_dict(\n",
    "            {subj: data['metadata'] for subj, data in filtered_data.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "        \n",
    "        return new_instance\n",
    "    \n",
    "    def get_trial_data(self, subject_id: str, trial_type: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get trial data for a specific subject and trial type.\"\"\"\n",
    "        try:\n",
    "            trial_dict = self.processed_data[subject_id]['trial_data'][trial_type]\n",
    "            return trial_dict['data'] if trial_dict else None\n",
    "        except KeyError:\n",
    "            return None\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary statistics of the dataset.\"\"\"\n",
    "        print(f\"📊 Dataset Summary:\")\n",
    "        print(f\"Total subjects: {len(self.processed_data)}\")\n",
    "        \n",
    "        if self.metadata is not None:\n",
    "            # SIMPLIFIED AGE DISPLAY - always use age_months/12\n",
    "            ages = self.metadata['age_months'] / 12\n",
    "            print(f\"Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "            print(f\"Mean age: {ages.mean():.1f} years\")\n",
    "        \n",
    "        # Trial type counts\n",
    "        trial_counts = defaultdict(int)\n",
    "        for subject_data in self.processed_data.values():\n",
    "            for trial_type in subject_data['trial_data'].keys():\n",
    "                trial_counts[trial_type] += 1\n",
    "        \n",
    "        print(f\"Trial type counts: {dict(trial_counts)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. CORRECTED METRICS CALCULATOR\n",
    "# ==============================================================================\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Handles calculation of performance metrics with corrected column naming.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_manager: MotorLearningDataManager):\n",
    "        self.data_manager = data_manager\n",
    "        self.config = Config()\n",
    "    \n",
    "    def calculate_all_metrics(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate comprehensive performance metrics for all subjects.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"🧮 Calculating metrics for {len(self.data_manager.processed_data)} subjects...\")\n",
    "        \n",
    "        for i, (subject_id, data) in enumerate(self.data_manager.processed_data.items(), 1):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"   Processed {i}/{len(self.data_manager.processed_data)} subjects...\")\n",
    "            \n",
    "            try:\n",
    "                subject_result = self._calculate_subject_metrics(subject_id, data)\n",
    "                if subject_result:\n",
    "                    results.append(subject_result)\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error processing {subject_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            print(\"❌ No valid metrics calculated!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(results).infer_objects()\n",
    "        print(f\"✅ Successfully calculated metrics for {len(df)} subjects\")\n",
    "        \n",
    "        # Print age and column summary\n",
    "        if 'age' in df.columns:\n",
    "            ages = df['age'].dropna()\n",
    "            print(f\"📊 Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "        \n",
    "        sr_cols = [col for col in df.columns if '_sr_' in col]\n",
    "        print(f\"🎯 Success rate columns created: {sr_cols}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _calculate_subject_metrics(self, subject_id: str, subject_data: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Calculate metrics for a single subject with simplified age handling.\"\"\"\n",
    "        \n",
    "        # SIMPLIFIED: Only store age as age_months/12, call it 'age'\n",
    "        result = {\n",
    "            'ID': subject_id,\n",
    "            'age': subject_data['metadata'].get('age_months', np.nan) / 12,  # Single age field\n",
    "            'session_date': subject_data['metadata'].get('Session Date')\n",
    "        }\n",
    "        \n",
    "        # Process each trial type\n",
    "        for trial_type in ['vis1', 'invis', 'vis2']:\n",
    "            try:\n",
    "                trial_dict = subject_data['trial_data'].get(trial_type)\n",
    "                df = trial_dict['data'] if trial_dict else None\n",
    "                \n",
    "                if df is None or df.empty or 'Success' not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate metrics for both conditions\n",
    "                for condition in ['max', 'min']:\n",
    "                    period_data, indices = self._get_period_data(df, condition)\n",
    "                    if period_data is not None and not period_data.empty:\n",
    "                        metrics = self._calculate_period_metrics(period_data, trial_type, condition)\n",
    "                        result.update(metrics)\n",
    "                        result[f'{trial_type}_{condition}_const_indices'] = indices\n",
    "                \n",
    "                # Add trial metadata - ONLY if df is not None\n",
    "                if df is not None:\n",
    "                    result.update({\n",
    "                        f'{trial_type}_min_target_size': df['Target size'].min() if 'Target size' in df.columns else None,\n",
    "                        f'{trial_type}_max_constant': df['Constant'].max() if 'Constant' in df.columns else None,\n",
    "                        f'{trial_type}_min_constant': df['Constant'].min() if 'Constant' in df.columns else None\n",
    "                    })\n",
    "                    \n",
    "                    # Order information for invis trials\n",
    "                    if trial_type == 'invis':\n",
    "                        result.update(self._calculate_condition_order(df))\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Process preference trial\n",
    "        try:\n",
    "            pref_metrics = self._calculate_preference_metrics(subject_data['trial_data'].get('pref'))\n",
    "            result.update(pref_metrics)\n",
    "        except Exception as e:\n",
    "            result.update({'mot_noise': None, 'pref_asymmetry': None})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_period_data(self, df: pd.DataFrame, condition: str, length: int = 20):\n",
    "        \"\"\"Extract data for specific condition period.\"\"\"\n",
    "        try:\n",
    "            if df is None or df.empty:\n",
    "                return None, None\n",
    "            \n",
    "            if 'Target size' not in df.columns or 'Constant' not in df.columns:\n",
    "                return None, None\n",
    "            \n",
    "            min_target = df['Target size'].min()\n",
    "            target_tolerance = 0.001\n",
    "            \n",
    "            min_target_periods = df[df['Target size'] <= min_target + target_tolerance]\n",
    "            if min_target_periods.empty:\n",
    "                return None, None\n",
    "            \n",
    "            const_value = (min_target_periods['Constant'].max() if condition == 'max'\n",
    "                          else min_target_periods['Constant'].min())\n",
    "            \n",
    "            period_data = min_target_periods[\n",
    "                np.isclose(min_target_periods['Constant'], const_value, rtol=1e-5)\n",
    "            ]\n",
    "            \n",
    "            if period_data.empty:\n",
    "                return None, None\n",
    "            \n",
    "            return period_data.tail(length), period_data.index\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def _calculate_period_metrics(self, period_data: pd.DataFrame, trial_type: str, condition: str) -> Dict:\n",
    "        \"\"\"Calculate metrics for a specific period with CORRECTED naming.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # CORRECTED: Use the format that analysis expects\n",
    "            # Success rate - THE KEY METRIC  \n",
    "            metrics[f'{trial_type}_sr_{condition}_const'] = period_data['Success'].mean()\n",
    "            \n",
    "            # Other metrics\n",
    "            if 'Sum of gains and steps' in period_data.columns:\n",
    "                sogs = period_data['Sum of gains and steps']\n",
    "                metrics[f'{trial_type}_sd_{condition}_const'] = sogs.std()\n",
    "                metrics[f'{trial_type}_msl_{condition}_const'] = sogs.mean()\n",
    "                metrics[f'{trial_type}_error_{condition}_const'] = (sogs - period_data['Constant']).mean()\n",
    "            \n",
    "            # Asymmetry\n",
    "            if all(col in period_data.columns for col in ['Right step length', 'Left step length']):\n",
    "                right_steps = period_data['Right step length']\n",
    "                left_steps = period_data['Left step length']\n",
    "                denominator = right_steps + left_steps\n",
    "                \n",
    "                valid_mask = denominator != 0\n",
    "                if valid_mask.any():\n",
    "                    asymmetry = ((right_steps - left_steps) / denominator).abs()[valid_mask].mean()\n",
    "                    metrics[f'{trial_type}_asymmetry_{condition}_const'] = asymmetry\n",
    "            \n",
    "            # Strides between successes\n",
    "            strides_between = self._calculate_strides_between_successes(period_data)\n",
    "            if strides_between is not None:\n",
    "                metrics[f'{trial_type}_strides_between_success_{condition}_const'] = strides_between\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_strides_between_successes(self, df: pd.DataFrame) -> Optional[float]:\n",
    "        \"\"\"Calculate average strides between successful trials.\"\"\"\n",
    "        try:\n",
    "            if df is None or 'Success' not in df.columns:\n",
    "                return None\n",
    "            \n",
    "            df = df.reset_index(drop=True)\n",
    "            success_positions = df.index[df['Success'] == 1].tolist()\n",
    "            \n",
    "            if len(success_positions) < 2:\n",
    "                return None\n",
    "            \n",
    "            return np.mean(np.diff(success_positions))\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _calculate_condition_order(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Determine which condition came first for invis trials.\"\"\"\n",
    "        try:\n",
    "            all_max_indices = df.index[df['Constant'] == df['Constant'].max()].tolist()\n",
    "            all_min_indices = df.index[df['Constant'] == df['Constant'].min()].tolist()\n",
    "            \n",
    "            if all_max_indices and all_min_indices:\n",
    "                first_max = min(all_max_indices)\n",
    "                first_min = min(all_min_indices)\n",
    "                return {\n",
    "                    'invis_max_first': first_max < first_min,\n",
    "                    'invis_min_first': first_min < first_max\n",
    "                }\n",
    "            \n",
    "            return {'invis_max_first': False, 'invis_min_first': False}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'invis_max_first': False, 'invis_min_first': False}\n",
    "    \n",
    "    def _calculate_preference_metrics(self, pref_trial_dict: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Calculate metrics from preference trial.\"\"\"\n",
    "        metrics = {'mot_noise': None, 'pref_asymmetry': None}\n",
    "        \n",
    "        try:\n",
    "            if not pref_trial_dict or pref_trial_dict.get('data') is None:\n",
    "                return metrics\n",
    "            \n",
    "            pref_df = pref_trial_dict['data']\n",
    "            if pref_df is None or pref_df.empty:\n",
    "                return metrics\n",
    "            \n",
    "            # Check for required columns\n",
    "            if not all(col in pref_df.columns for col in ['Right step length', 'Left step length']):\n",
    "                return metrics\n",
    "            \n",
    "            # Get non-zero steps\n",
    "            right_steps = pref_df['Right step length']\n",
    "            left_steps = pref_df['Left step length']\n",
    "            \n",
    "            # Remove zeros and NaNs\n",
    "            right_clean = right_steps[(right_steps != 0) & (right_steps.notna())]\n",
    "            left_clean = left_steps[(left_steps != 0) & (left_steps.notna())]\n",
    "            \n",
    "            if len(right_clean) < 20 or len(left_clean) < 20:\n",
    "                return metrics\n",
    "            \n",
    "            # Calculate motor noise\n",
    "            final_right = right_clean.iloc[-1]\n",
    "            final_left = left_clean.iloc[-1]\n",
    "            \n",
    "            if final_right <= 0 or final_left <= 0:\n",
    "                return metrics\n",
    "            \n",
    "            # Normalize steps\n",
    "            norm_right = right_clean / final_right\n",
    "            norm_left = left_clean / final_left\n",
    "            \n",
    "            # Calculate sum of normalized steps\n",
    "            min_length = min(len(norm_right), len(norm_left))\n",
    "            if min_length < 20:\n",
    "                return metrics\n",
    "            \n",
    "            sum_steps = norm_right.iloc[:min_length] + norm_left.iloc[:min_length]\n",
    "            \n",
    "            # Motor noise from last 20 points\n",
    "            if len(sum_steps) >= 20:\n",
    "                noise = sum_steps.tail(20).std()\n",
    "                if not pd.isna(noise) and noise > 0:\n",
    "                    metrics['mot_noise'] = noise\n",
    "            \n",
    "            # Calculate step length asymmetry\n",
    "            if len(right_clean) >= 20 and len(left_clean) >= 20:\n",
    "                last_20_right = right_clean.tail(20) / final_right\n",
    "                last_20_left = left_clean.tail(20) / final_left\n",
    "                \n",
    "                if len(last_20_right) == len(last_20_left):\n",
    "                    denominator = last_20_right.values + last_20_left.values\n",
    "                    valid_mask = denominator != 0\n",
    "                    \n",
    "                    if valid_mask.any():\n",
    "                        asymmetry_vals = np.abs((last_20_right.values - last_20_left.values) / denominator)[valid_mask]\n",
    "                        if len(asymmetry_vals) > 0:\n",
    "                            metrics['pref_asymmetry'] = np.mean(asymmetry_vals)\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. CORRECTED STATISTICAL ANALYZER\n",
    "# ==============================================================================\n",
    "\n",
    "class StatisticalAnalyzer:\n",
    "    \"\"\"Handles all statistical analyses with corrected column naming.\"\"\"\n",
    "    \n",
    "    def __init__(self, metrics_df: pd.DataFrame):\n",
    "        self.metrics_df = metrics_df\n",
    "        \n",
    "        # Apply motor noise filter if available\n",
    "        if 'mot_noise' in metrics_df.columns:\n",
    "            self.filtered_df = metrics_df[metrics_df['mot_noise'] <= 0.3]\n",
    "            print(f\"📊 Filtered to {len(self.filtered_df)}/{len(metrics_df)} subjects (motor noise ≤ 0.3)\")\n",
    "        else:\n",
    "            self.filtered_df = metrics_df\n",
    "    \n",
    "    def run_regression_analysis(self, trial_type: str = 'invis', condition: str = 'max',\n",
    "                               predictors: List[str] = None, model_type: str = 'linear') -> Dict:\n",
    "        \"\"\"Run regression analysis with CORRECTED target column naming.\"\"\"\n",
    "        \n",
    "        if predictors is None:\n",
    "            predictors = ['age', 'mot_noise', 'pref_asymmetry']\n",
    "        \n",
    "        # Filter available predictors\n",
    "        available_predictors = [p for p in predictors if p in self.filtered_df.columns]\n",
    "        \n",
    "        # CORRECTED: Use the exact column format that exists\n",
    "        target_col = f'{trial_type}_sr_{condition}_const'\n",
    "        \n",
    "        if target_col not in self.filtered_df.columns:\n",
    "            available_cols = [col for col in self.filtered_df.columns if '_sr_' in col]\n",
    "            raise ValueError(f\"Target column {target_col} not found. Available: {available_cols}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        valid_data = self.filtered_df[available_predictors + [target_col]].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            raise ValueError(f\"Insufficient data: only {len(valid_data)} valid samples\")\n",
    "        \n",
    "        X = valid_data[available_predictors]\n",
    "        y = valid_data[target_col]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'linear':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', LinearRegression())\n",
    "            ])\n",
    "        elif model_type == 'random_forest':\n",
    "            model = RandomForestRegressor(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'linear' or 'random_forest'\")\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        # Feature importance\n",
    "        if model_type == 'random_forest':\n",
    "            importances = dict(zip(available_predictors, model.feature_importances_))\n",
    "        else:\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 np.abs(model.named_steps['regressor'].coef_)))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'metrics': {\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                'n_samples': len(valid_data),\n",
    "                'trial_type': trial_type,\n",
    "                'condition': condition,\n",
    "                'predictors': available_predictors\n",
    "            },\n",
    "            'feature_importances': importances\n",
    "        }\n",
    "    \n",
    "    def run_classification_analysis(self, trial_type: str = 'invis', condition: str = 'max',\n",
    "                                   threshold: float = 0.68, model_type: str = 'logistic') -> Dict:\n",
    "        \"\"\"Run binary classification with CORRECTED target column naming.\"\"\"\n",
    "        \n",
    "        predictors = ['age', 'mot_noise']\n",
    "        available_predictors = [p for p in predictors if p in self.filtered_df.columns]\n",
    "        \n",
    "        # CORRECTED: Use the exact column format that exists\n",
    "        target_col = f'{trial_type}_sr_{condition}_const'\n",
    "        \n",
    "        if target_col not in self.filtered_df.columns:\n",
    "            available_cols = [col for col in self.filtered_df.columns if '_sr_' in col]\n",
    "            raise ValueError(f\"Target column {target_col} not found. Available: {available_cols}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        valid_data = self.filtered_df[available_predictors + [target_col]].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            raise ValueError(f\"Insufficient data: only {len(valid_data)} valid samples\")\n",
    "        \n",
    "        # Create binary target\n",
    "        y = (valid_data[target_col] >= threshold).astype(int)\n",
    "        X = valid_data[available_predictors]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'logistic':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', LogisticRegression(random_state=42))\n",
    "            ])\n",
    "        elif model_type == 'random_forest':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', RandomForestClassifier(random_state=42))\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'logistic' or 'random_forest'\")\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'n_samples': len(valid_data),\n",
    "            'threshold': threshold,\n",
    "            'trial_type': trial_type,\n",
    "            'condition': condition,\n",
    "            'predictors': available_predictors\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if model_type == 'random_forest':\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 model.named_steps['classifier'].feature_importances_))\n",
    "        else:\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 model.named_steps['classifier'].coef_[0]))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'y_proba': y_proba,\n",
    "            'metrics': metrics,\n",
    "            'feature_importances': importances\n",
    "        }\n",
    "    \n",
    "    def run_mixed_effects_analysis(self, trial_types: Union[str, List[str]] = 'all') -> object:\n",
    "        \"\"\"Run mixed-effects analysis with CORRECTED column naming.\"\"\"\n",
    "        \n",
    "        # Prepare long-format data\n",
    "        long_rows = []\n",
    "        \n",
    "        if trial_types == 'all':\n",
    "            trials_to_include = ['vis1', 'invis', 'vis2']\n",
    "        elif isinstance(trial_types, str):\n",
    "            trials_to_include = [trial_types]\n",
    "        else:\n",
    "            trials_to_include = trial_types\n",
    "        \n",
    "        for trial in trials_to_include:\n",
    "            for condition in ['max_const', 'min_const']:\n",
    "                # CORRECTED: Use the exact column format\n",
    "                sr_col = f\"{trial}_sr_{condition}\"\n",
    "                if sr_col in self.filtered_df.columns:\n",
    "                    sub_df = self.filtered_df[['ID', sr_col, 'mot_noise', 'age']].copy()\n",
    "                    sub_df = sub_df.rename(columns={sr_col: 'success_rate'})\n",
    "                    sub_df['trial_type'] = trial\n",
    "                    sub_df['condition'] = condition\n",
    "                    long_rows.append(sub_df)\n",
    "        \n",
    "        if not long_rows:\n",
    "            raise ValueError(\"No success rate data available\")\n",
    "        \n",
    "        df_long = pd.concat(long_rows, ignore_index=True)\n",
    "        df_long.dropna(subset=['success_rate', 'mot_noise', 'age'], inplace=True)\n",
    "        \n",
    "        # Fit model\n",
    "        if len(trials_to_include) == 1:\n",
    "            formula = 'success_rate ~ C(condition) + mot_noise + age'\n",
    "        else:\n",
    "            formula = 'success_rate ~ C(trial_type) + C(condition) + mot_noise + age'\n",
    "        \n",
    "        model = smf.ols(formula, data=df_long).fit()\n",
    "        return model\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. CORRECTED VISUALIZER\n",
    "# ==============================================================================\n",
    "\n",
    "class MotorLearningVisualizer:\n",
    "    \"\"\"Handles all visualization with corrected column naming.\"\"\"\n",
    "    \n",
    "    def __init__(self, metrics_df: pd.DataFrame, figures_dir: Path = Path('figures')):\n",
    "        self.metrics_df = metrics_df\n",
    "        self.figures_dir = figures_dir\n",
    "        self.figures_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Apply motor noise filter for most plots\n",
    "        if 'mot_noise' in metrics_df.columns:\n",
    "            self.filtered_df = metrics_df[metrics_df['mot_noise'] <= 0.3]\n",
    "        else:\n",
    "            self.filtered_df = metrics_df\n",
    "        \n",
    "        # Set colors\n",
    "        self.colors = {\n",
    "            'primary': '#667eea',\n",
    "            'secondary': '#764ba2',\n",
    "            'success': '#28a745',\n",
    "            'warning': '#ffc107',\n",
    "            'danger': '#dc3545'\n",
    "        }\n",
    "    \n",
    "    def plot_age_performance_relationship(self, trial_type: str = 'invis', \n",
    "                                        metric: str = 'sr', save: bool = True) -> None:\n",
    "        \"\"\"Plot relationship between age and performance with CORRECTED column naming.\"\"\"\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # CORRECTED: Use the exact column format\n",
    "        max_col = f'{trial_type}_{metric}_max_const'\n",
    "        min_col = f'{trial_type}_{metric}_min_const'\n",
    "        \n",
    "        for ax, col, title in zip([ax1, ax2], [max_col, min_col], \n",
    "                                 ['Max Constant', 'Min Constant']):\n",
    "            \n",
    "            valid_data = self.filtered_df[self.filtered_df[col].notna()]\n",
    "            if valid_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Scatter plot\n",
    "            scatter = ax.scatter(\n",
    "                valid_data['age'],\n",
    "                valid_data[col],\n",
    "                c=valid_data['mot_noise'] if 'mot_noise' in valid_data.columns else self.colors['primary'],\n",
    "                cmap='plasma',\n",
    "                alpha=0.7,\n",
    "                edgecolors='white',\n",
    "                linewidths=0.5\n",
    "            )\n",
    "            \n",
    "            # Trend line\n",
    "            self._add_trendline(ax, valid_data['age'], valid_data[col])\n",
    "            \n",
    "            ax.set_title(f'{trial_type.upper()}: {title}')\n",
    "            ax.set_xlabel('Age (years)')\n",
    "            ax.set_ylabel(metric.upper().replace('_', ' '))\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Add colorbar if using motor noise coloring\n",
    "        if 'mot_noise' in self.filtered_df.columns:\n",
    "            plt.colorbar(scatter, ax=[ax1, ax2], label='Motor Noise')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.figures_dir / f'{trial_type}_{metric}_vs_age.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def create_summary_dashboard(self, save: bool = True) -> None:\n",
    "        \"\"\"Create comprehensive summary dashboard with CORRECTED column references.\"\"\"\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Age distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        if 'age' in self.filtered_df.columns:\n",
    "            self.filtered_df['age'].hist(bins=15, ax=ax1, color=self.colors['primary'], alpha=0.7)\n",
    "        ax1.set_title('Age Distribution')\n",
    "        ax1.set_xlabel('Age (years)')\n",
    "        ax1.set_ylabel('Count')\n",
    "        \n",
    "        # 2. Motor noise distribution\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        if 'mot_noise' in self.filtered_df.columns:\n",
    "            self.filtered_df['mot_noise'].hist(bins=15, ax=ax2, color=self.colors['secondary'], alpha=0.7)\n",
    "            ax2.axvline(x=0.15, color='red', linestyle='--', label='Threshold')\n",
    "            ax2.legend()\n",
    "        ax2.set_title('Motor Noise Distribution')\n",
    "        ax2.set_xlabel('Motor Noise')\n",
    "        ax2.set_ylabel('Count')\n",
    "        \n",
    "        # 3. Success rates by trial type with CORRECTED column naming\n",
    "        ax3 = fig.add_subplot(gs[0, 2:])\n",
    "        trial_data = []\n",
    "        for trial in ['vis1', 'invis', 'vis2']:\n",
    "            for condition in ['max_const', 'min_const']:\n",
    "                # CORRECTED: Use exact column format\n",
    "                col = f'{trial}_sr_{condition}'\n",
    "                if col in self.filtered_df.columns:\n",
    "                    values = self.filtered_df[col].dropna()\n",
    "                    for val in values:\n",
    "                        trial_data.append({\n",
    "                            'trial_type': trial,\n",
    "                            'condition': condition.replace('_const', ''),\n",
    "                            'success_rate': val\n",
    "                        })\n",
    "        \n",
    "        if trial_data:\n",
    "            trial_df = pd.DataFrame(trial_data)\n",
    "            sns.boxplot(data=trial_df, x='trial_type', y='success_rate', \n",
    "                       hue='condition', ax=ax3)\n",
    "            ax3.set_title('Success Rates by Trial Type')\n",
    "            ax3.set_ylabel('Success Rate')\n",
    "        \n",
    "        # 4. Correlation matrix\n",
    "        ax4 = fig.add_subplot(gs[1, :2])\n",
    "        corr_cols = ['age']\n",
    "        if 'mot_noise' in self.filtered_df.columns:\n",
    "            corr_cols.append('mot_noise')\n",
    "        \n",
    "        # Add success rate columns with CORRECTED naming\n",
    "        sr_cols = [col for col in self.filtered_df.columns if '_sr_' in col and '_const' in col]\n",
    "        corr_cols.extend(sr_cols[:6])  # Limit to prevent overcrowding\n",
    "        \n",
    "        corr_data = self.filtered_df[corr_cols].corr()\n",
    "        sns.heatmap(corr_data, annot=True, cmap='RdBu_r', center=0, ax=ax4)\n",
    "        ax4.set_title('Correlation Matrix')\n",
    "        \n",
    "        # 5. Motor noise vs success rate scatter with CORRECTED column naming\n",
    "        ax5 = fig.add_subplot(gs[1, 2:])\n",
    "        target_col = 'invis_sr_max_const'  # CORRECTED format\n",
    "        if 'mot_noise' in self.filtered_df.columns and target_col in self.filtered_df.columns:\n",
    "            valid_data = self.filtered_df.dropna(subset=['mot_noise', target_col])\n",
    "            scatter = ax5.scatter(valid_data['mot_noise'], valid_data[target_col],\n",
    "                                c=valid_data['age'], cmap='viridis', alpha=0.7)\n",
    "            plt.colorbar(scatter, ax=ax5, label='Age (years)')\n",
    "            ax5.set_xlabel('Motor Noise')\n",
    "            ax5.set_ylabel('Success Rate (Invis)')\n",
    "            ax5.set_title('Motor Noise vs Learning Performance')\n",
    "        \n",
    "        # 6. Sample statistics\n",
    "        ax6 = fig.add_subplot(gs[2:, :])\n",
    "        \n",
    "        stats_data = {\n",
    "            'Metric': ['Sample Size', 'Age Range', 'Mean Age', 'Motor Noise Range', 'Mean Motor Noise'],\n",
    "            'Value': [\n",
    "                f\"{len(self.filtered_df)}\",\n",
    "                f\"{self.filtered_df['age'].min():.1f} - {self.filtered_df['age'].max():.1f} years\" if 'age' in self.filtered_df.columns else \"N/A\",\n",
    "                f\"{self.filtered_df['age'].mean():.1f} years\" if 'age' in self.filtered_df.columns else \"N/A\",\n",
    "                f\"{self.filtered_df['mot_noise'].min():.3f} - {self.filtered_df['mot_noise'].max():.3f}\" if 'mot_noise' in self.filtered_df.columns else \"N/A\",\n",
    "                f\"{self.filtered_df['mot_noise'].mean():.3f}\" if 'mot_noise' in self.filtered_df.columns else \"N/A\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        ax6.axis('tight')\n",
    "        ax6.axis('off')\n",
    "        table = ax6.table(cellText=[[stats_data['Metric'][i], stats_data['Value'][i]] \n",
    "                                   for i in range(len(stats_data['Metric']))],\n",
    "                         colLabels=['Metric', 'Value'],\n",
    "                         cellLoc='center',\n",
    "                         loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        table.scale(1.2, 1.5)\n",
    "        ax6.set_title('Dataset Summary Statistics', pad=20)\n",
    "        \n",
    "        plt.suptitle('Motor Learning Analysis Dashboard', fontsize=20, y=0.98)\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.figures_dir / 'summary_dashboard.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _add_trendline(self, ax, x, y):\n",
    "        \"\"\"Add trendline with correlation coefficient to plot.\"\"\"\n",
    "        x_clean = pd.to_numeric(x, errors='coerce')\n",
    "        y_clean = pd.to_numeric(y, errors='coerce')\n",
    "        valid = x_clean.notna() & y_clean.notna()\n",
    "        \n",
    "        if valid.sum() < 2:\n",
    "            return\n",
    "        \n",
    "        x_vals = x_clean[valid]\n",
    "        y_vals = y_clean[valid]\n",
    "        \n",
    "        # Fit line\n",
    "        coeffs = np.polyfit(x_vals, y_vals, 1)\n",
    "        trendline = np.poly1d(coeffs)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        r, p = pearsonr(x_vals, y_vals)\n",
    "        \n",
    "        # Plot trendline\n",
    "        ax.plot(x_vals, trendline(x_vals), 'r--', alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Add correlation text\n",
    "        ax.text(0.05, 0.95, f'r = {r:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. CORRECTED COMPLETE ANALYSIS PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "class MotorLearningAnalysis:\n",
    "    \"\"\"Complete analysis pipeline with all corrections.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_manager, metrics_df: pd.DataFrame):\n",
    "        self.data_manager = data_manager\n",
    "        self.metrics_df = metrics_df\n",
    "        self.analyzer = StatisticalAnalyzer(metrics_df)\n",
    "        self.visualizer = MotorLearningVisualizer(metrics_df)\n",
    "    \n",
    "    def run_comprehensive_analysis(self, save_all: bool = True) -> Dict:\n",
    "        \"\"\"Run complete analysis pipeline with all bug fixes.\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(\"🔬 Running Comprehensive Motor Learning Analysis...\")\n",
    "        \n",
    "        # Print age summary\n",
    "        if 'age' in self.metrics_df.columns:\n",
    "            ages = self.metrics_df['age'].dropna()\n",
    "            print(f\"📊 Age range: {ages.min():.1f} - {ages.max():.1f} years (n={len(ages)})\")\n",
    "        \n",
    "        # Print available columns for debugging\n",
    "        sr_cols = [col for col in self.metrics_df.columns if '_sr_' in col]\n",
    "        print(f\"🎯 Available success rate columns: {sr_cols}\")\n",
    "        \n",
    "        # 1. Regression Analysis\n",
    "        print(\"📊 Running regression analysis...\")\n",
    "        try:\n",
    "            regression_results = self.analyzer.run_regression_analysis()\n",
    "            results['regression'] = regression_results\n",
    "            print(f\"✅ Regression R² = {regression_results['metrics']['r2']:.3f}\")\n",
    "            \n",
    "            # Show feature effects\n",
    "            for feature, importance in regression_results['feature_importances'].items():\n",
    "                print(f\"   {feature}: {importance:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Regression analysis failed: {e}\")\n",
    "        \n",
    "        # 2. Classification Analysis\n",
    "        print(\"🎯 Running classification analysis...\")\n",
    "        try:\n",
    "            classification_results = self.analyzer.run_classification_analysis()\n",
    "            results['classification'] = classification_results\n",
    "            print(f\"✅ Classification AUC = {classification_results['metrics']['roc_auc']:.3f}\")\n",
    "            print(f\"   Accuracy = {classification_results['metrics']['accuracy']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Classification analysis failed: {e}\")\n",
    "        \n",
    "        # 3. Mixed Effects Analysis\n",
    "        print(\"📈 Running mixed-effects analysis...\")\n",
    "        try:\n",
    "            mixed_model = self.analyzer.run_mixed_effects_analysis()\n",
    "            results['mixed_effects'] = mixed_model\n",
    "            print(f\"✅ Mixed-effects R² = {mixed_model.rsquared:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Mixed-effects analysis failed: {e}\")\n",
    "        \n",
    "        # 4. Visualization Suite\n",
    "        print(\"📊 Generating visualizations...\")\n",
    "        try:\n",
    "            self.visualizer.plot_age_performance_relationship(save=save_all)\n",
    "            self.visualizer.create_summary_dashboard(save=save_all)\n",
    "            print(\"✅ Visualizations complete\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Visualization failed: {e}\")\n",
    "        \n",
    "        print(\"🎉 Comprehensive analysis complete!\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. CORRECTED COMPLETE USAGE FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def run_complete_motor_learning_analysis_corrected(metadata_path: str, data_root_dir: str, \n",
    "                                                  force_reprocess: bool = False) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Complete corrected workflow for motor learning analysis.\n",
    "    ALL COLUMN NAMING BUGS FIXED.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting CORRECTED Motor Learning Analysis...\")\n",
    "    print(\"🔧 All column naming bugs fixed\")\n",
    "    print(\"📏 Age handling: Consistent use of age_months/12 as 'age' throughout\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Data Loading and Processing\n",
    "    print(\"📂 Loading and processing data...\")\n",
    "    data_manager = MotorLearningDataManager(\n",
    "        metadata_path=metadata_path,\n",
    "        data_root_dir=data_root_dir,\n",
    "        force_reprocess=force_reprocess,\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "    # Apply quality filters\n",
    "    filtered_data_manager = data_manager.filter_trials(\n",
    "        max_target_size=0.31,\n",
    "        max_strides=415,\n",
    "        required_trial_types=['pref', 'invis']\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Data loaded: {len(filtered_data_manager.processed_data)} subjects after filtering\")\n",
    "    \n",
    "    # 2. Calculate Metrics with CORRECTED naming\n",
    "    print(\"🧮 Calculating performance metrics...\")\n",
    "    calculator = MetricsCalculator(filtered_data_manager)\n",
    "    metrics_df = calculator.calculate_all_metrics()\n",
    "    \n",
    "    print(f\"✅ Metrics calculated: {len(metrics_df)} subjects, {len(metrics_df.columns)} metrics\")\n",
    "    \n",
    "    # 3. Run CORRECTED Comprehensive Analysis\n",
    "    print(\"🔬 Running corrected comprehensive analysis...\")\n",
    "    analysis = MotorLearningAnalysis(filtered_data_manager, metrics_df)\n",
    "    results = analysis.run_comprehensive_analysis(save_all=True)\n",
    "    \n",
    "    # 4. Print Final Summary\n",
    "    print(\"📊 Final Summary:\")\n",
    "    print(f\"   Total subjects analyzed: {len(metrics_df)}\")\n",
    "    if 'age' in metrics_df.columns:\n",
    "        ages = metrics_df['age'].dropna()\n",
    "        print(f\"   Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "        print(f\"   Mean age: {ages.mean():.1f} ± {ages.std():.1f} years\")\n",
    "    \n",
    "    success_cols = [col for col in metrics_df.columns if '_sr_' in col]\n",
    "    print(f\"   Success rate metrics: {len(success_cols)} columns\")\n",
    "    print(f\"   Figures saved to: {analysis.visualizer.figures_dir}\")\n",
    "    \n",
    "    if 'regression' in results:\n",
    "        print(f\"   🎯 Key finding: Age effect = {results['regression']['feature_importances'].get('age', 0):.4f} per year\")\n",
    "    \n",
    "    return metrics_df, results\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. SIMPLE USAGE INSTRUCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Use your existing data paths\n",
    "metadata_path = 'E:/muh_data/muh_metadata.csv'\n",
    "data_root_dir = 'E:/muh_data/'\n",
    "\n",
    "# Run the CORRECTED analysis\n",
    "metrics_df, results = run_complete_motor_learning_analysis_corrected(metadata_path, data_root_dir)\n",
    "\n",
    "# Now everything should work perfectly:\n",
    "print(\"Success rate columns:\", [col for col in metrics_df.columns if '_sr_' in col])\n",
    "print(\"Regression R²:\", results.get('regression', {}).get('metrics', {}).get('r2', 'N/A'))\n",
    "print(\"Classification AUC:\", results.get('classification', {}).get('metrics', {}).get('roc_auc', 'N/A'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79419b0e-8f41-49e6-87fe-903472bb0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot_individual_stride_change_distribution('MUH1172', 'vis2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
