{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d5fd3-6c9d-4c53-8aac-a5de3902599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED MOTOR LEARNING ANALYSIS WITH STRIDE CHANGE DISTRIBUTION\n",
    "# Incorporates the stride change distribution plotting functionality\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import tempfile\n",
    "import webbrowser\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, mannwhitneyu, gaussian_kde\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, confusion_matrix, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b028b-f0a9-4cb1-bd4a-a59726b15c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Enhanced configuration settings for the motor learning analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_output_dir: str = 'motor_learning_output'):\n",
    "        # Base directory for all outputs\n",
    "        self.BASE_OUTPUT_DIR = Path(base_output_dir)\n",
    "        self.BASE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create organized subdirectory structure\n",
    "        self.FIGURES_DIR = self.BASE_OUTPUT_DIR / 'figures'\n",
    "        self.INDIVIDUAL_PLOTS_DIR = self.FIGURES_DIR / 'individual_plots'\n",
    "        self.POPULATION_PLOTS_DIR = self.FIGURES_DIR / 'population_plots'\n",
    "        self.STATISTICAL_PLOTS_DIR = self.FIGURES_DIR / 'statistical_plots'\n",
    "        self.REPORTS_DIR = self.BASE_OUTPUT_DIR / 'reports'\n",
    "        self.EXPORTS_DIR = self.BASE_OUTPUT_DIR / 'exports'\n",
    "        self.PROCESSED_DATA_DIR = self.BASE_OUTPUT_DIR / 'processed_data'\n",
    "        \n",
    "        # Create all directories\n",
    "        for directory in [self.FIGURES_DIR, self.INDIVIDUAL_PLOTS_DIR, \n",
    "                         self.POPULATION_PLOTS_DIR, self.STATISTICAL_PLOTS_DIR,\n",
    "                         self.REPORTS_DIR, self.EXPORTS_DIR, self.PROCESSED_DATA_DIR]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # File processing parameters\n",
    "        self.MIN_COMPLETE_STRIDES = 20\n",
    "        self.PROCESSED_DATA_FILE = self.PROCESSED_DATA_DIR / 'processed_data.pkl'\n",
    "        \n",
    "        # Trial type mappings\n",
    "        self.TRIAL_TYPE_MAPPING = {\n",
    "            'primer': 'vis1',\n",
    "            'trial': 'invis', \n",
    "            'vis': 'vis2',\n",
    "            'pref': 'pref'\n",
    "        }\n",
    "        \n",
    "        # Analysis parameters\n",
    "        self.MOTOR_NOISE_THRESHOLD = 0.3\n",
    "        self.SUCCESS_RATE_THRESHOLD = 0.68\n",
    "        self.TARGET_SIZE_THRESHOLD = 0.31\n",
    "        self.MAX_STRIDES_THRESHOLD = 415\n",
    "        \n",
    "        # Visualization parameters\n",
    "        self.FIGURE_DPI = 300\n",
    "        self.ALPHA_LEVEL = 0.05\n",
    "        self.AGE_BINS = [7, 10, 13, 16, 18]\n",
    "        self.AGE_LABELS = ['7-10', '10-13', '13-16', '16-18']\n",
    "        \n",
    "        print(f\"📁 Config initialized with base directory: {self.BASE_OUTPUT_DIR}\")\n",
    "        print(f\"   📊 Figures: {self.FIGURES_DIR}\")\n",
    "        print(f\"   📋 Reports: {self.REPORTS_DIR}\")\n",
    "        print(f\"   💾 Exports: {self.EXPORTS_DIR}\")\n",
    "\n",
    "    def get_figure_path(self, filename: str, subdir: str = 'general') -> Path:\n",
    "        \"\"\"Get standardized figure path with automatic subdirectory creation.\"\"\"\n",
    "        if subdir == 'individual':\n",
    "            target_dir = self.INDIVIDUAL_PLOTS_DIR\n",
    "        elif subdir == 'population':\n",
    "            target_dir = self.POPULATION_PLOTS_DIR\n",
    "        elif subdir == 'statistical':\n",
    "            target_dir = self.STATISTICAL_PLOTS_DIR\n",
    "        else:\n",
    "            target_dir = self.FIGURES_DIR\n",
    "        \n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return target_dir / filename\n",
    "    \n",
    "    def get_report_path(self, filename: str) -> Path:\n",
    "        \"\"\"Get standardized report path.\"\"\"\n",
    "        return self.REPORTS_DIR / filename\n",
    "    \n",
    "    def get_export_path(self, filename: str) -> Path:\n",
    "        \"\"\"Get standardized export path.\"\"\"\n",
    "        return self.EXPORTS_DIR / filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0725fd-e206-4ee8-ab35-2ad451620009",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "class DataUtils:\n",
    "    \"\"\"Utility functions for data processing.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_and_validate_file(file_path: Path, required_cols: set = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load and validate a single data file.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            if required_cols and not required_cols.issubset(df.columns):\n",
    "                return None\n",
    "                \n",
    "            # Basic cleaning\n",
    "            if 'Stride Number' in df.columns:\n",
    "                df['Stride Number'] = pd.to_numeric(df['Stride Number'], errors='coerce')\n",
    "                df = df.dropna(subset=['Stride Number'])\n",
    "                df = df.drop_duplicates(subset=['Stride Number'])\n",
    "            \n",
    "            return df if not df.empty else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file_path.name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_anomalies(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Detect and flag anomalies in stride data.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return df, {}\n",
    "\n",
    "        df = df.copy()\n",
    "        df['Anomalous'] = False\n",
    "        anomalies = {}\n",
    "\n",
    "        # Time-based anomalies\n",
    "        time_col = next((col for col in ['Time', 'Timestamp', 'Time (s)'] \n",
    "                        if col in df.columns), None)\n",
    "        if time_col:\n",
    "            df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "            time_diff = df[time_col].diff()\n",
    "            jump_mask = time_diff > time_diff.quantile(0.99) * 5\n",
    "            \n",
    "            for idx in df.index[jump_mask.fillna(False)]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('time_jump')\n",
    "\n",
    "        # Sum of gains and steps anomalies\n",
    "        if 'Sum of gains and steps' in df.columns:\n",
    "            high_mask = df['Sum of gains and steps'] > 4\n",
    "            zero_mask = df['Sum of gains and steps'] == 0\n",
    "\n",
    "            for idx in df.index[high_mask]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('sum_gain_step_high')\n",
    "            \n",
    "            for idx in df.index[zero_mask]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('sum_gain_step_zero')\n",
    "\n",
    "        # Duplicate rows\n",
    "        duplicated_mask = df.duplicated()\n",
    "        for idx in df.index[duplicated_mask]:\n",
    "            df.at[idx, 'Anomalous'] = True\n",
    "            anomalies.setdefault(idx, []).append('duplicate_row')\n",
    "\n",
    "        return df, anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaa9d3-4239-477d-a5dc-d824c0684962",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3. TRIAL PROCESSING\n",
    "# ==============================================================================\n",
    "\n",
    "class TrialProcessor:\n",
    "    \"\"\"Handles loading, combining, and processing of trial data.\"\"\"\n",
    "    \n",
    "    def __init__(self, debug: bool = True):\n",
    "        self.debug = debug\n",
    "        \n",
    "    def find_and_combine_trial_files(self, subject_dir: Path, trial_prefix: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Find and combine trial files for a given trial type.\"\"\"\n",
    "        all_files = sorted(subject_dir.glob(f\"{trial_prefix}*.txt\"))\n",
    "        \n",
    "        if not all_files:\n",
    "            if self.debug:\n",
    "                print(f\"  ⚠️ No files found for {trial_prefix}\")\n",
    "            return None\n",
    "        \n",
    "        # Special handling for preference trials\n",
    "        if trial_prefix == 'pref':\n",
    "            return self._handle_pref_trial(all_files)\n",
    "        \n",
    "        # Single file case\n",
    "        if len(all_files) == 1:\n",
    "            return DataUtils.load_and_validate_file(all_files[0])\n",
    "        \n",
    "        # Multiple files - combine them\n",
    "        return self._combine_trial_fragments(all_files)\n",
    "    \n",
    "    def _handle_pref_trial(self, files: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Handle preference trial - select largest file.\"\"\"\n",
    "        largest_file = max(files, key=lambda f: f.stat().st_size)\n",
    "        if self.debug and len(files) > 1:\n",
    "            print(f\"  ⚡ pref trial - selected largest of {len(files)} files\")\n",
    "        return DataUtils.load_and_validate_file(largest_file)\n",
    "    \n",
    "    def _combine_trial_fragments(self, files: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Combine multiple trial fragments intelligently.\"\"\"\n",
    "        file_info = []\n",
    "        \n",
    "        for f in files:\n",
    "            try:\n",
    "                with open(f, 'r') as file:\n",
    "                    header = file.readline().strip().split('\\t')\n",
    "                    stride_col = next((i for i, col in enumerate(header) \n",
    "                                     if 'stride' in col.lower() and \n",
    "                                     ('num' in col.lower() or 'no' in col.lower())), None)\n",
    "                    \n",
    "                    if stride_col is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get stride range\n",
    "                    lines = file.readlines()\n",
    "                    first_stride = float(lines[0].split('\\t')[stride_col])\n",
    "                    last_stride = float(lines[-1].split('\\t')[stride_col])\n",
    "                    \n",
    "                    file_info.append({\n",
    "                        'path': f,\n",
    "                        'first': first_stride,\n",
    "                        'last': last_stride,\n",
    "                        'size': f.stat().st_size\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not file_info:\n",
    "            return DataUtils.load_and_validate_file(max(files, key=lambda f: f.stat().st_size))\n",
    "        \n",
    "        # Find best continuous sequence\n",
    "        file_info.sort(key=lambda x: x['first'])\n",
    "        best_sequence = self._find_best_sequence(file_info)\n",
    "        \n",
    "        if len(best_sequence) >= 2:\n",
    "            return self._merge_files([f['path'] for f in best_sequence])\n",
    "        \n",
    "        # Fallback to largest file\n",
    "        return DataUtils.load_and_validate_file(max(file_info, key=lambda x: x['size'])['path'])\n",
    "    \n",
    "    def _find_best_sequence(self, file_info: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find the best continuous sequence of files.\"\"\"\n",
    "        best_sequence = []\n",
    "        current_sequence = [file_info[0]]\n",
    "        \n",
    "        for file_data in file_info[1:]:\n",
    "            if file_data['first'] == current_sequence[-1]['last'] + 1:\n",
    "                current_sequence.append(file_data)\n",
    "            else:\n",
    "                if len(current_sequence) > len(best_sequence):\n",
    "                    best_sequence = current_sequence\n",
    "                current_sequence = [file_data]\n",
    "        \n",
    "        return max([best_sequence, current_sequence], key=len)\n",
    "    \n",
    "    def _merge_files(self, file_paths: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Merge multiple files into a single DataFrame.\"\"\"\n",
    "        dfs = []\n",
    "        for f in file_paths:\n",
    "            df = DataUtils.load_and_validate_file(f)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "        \n",
    "        if not dfs:\n",
    "            return None\n",
    "        \n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Clean and sort\n",
    "        if 'Stride Number' in combined.columns:\n",
    "            combined = combined.sort_values('Stride Number')\n",
    "            combined = combined.drop_duplicates('Stride Number')\n",
    "        \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737c435-0348-4a95-9052-77cc5cb51e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MAIN DATA MANAGER\n",
    "# ==============================================================================\n",
    "\n",
    "class MotorLearningDataManager:\n",
    "    \"\"\"Updated to use centralized config.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path: str, data_root_dir: str, \n",
    "                 config: Config = None, force_reprocess: bool = False, debug: bool = True):\n",
    "        self.metadata_path = metadata_path\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.debug = debug\n",
    "        \n",
    "        # KEY CHANGE: Use provided config or create default\n",
    "        self.config = config if config else Config()\n",
    "        \n",
    "        # Initialize components (unchanged)\n",
    "        self.trial_processor = TrialProcessor(debug=debug)\n",
    "        \n",
    "        # Data storage (unchanged)\n",
    "        self.metadata = None\n",
    "        self.processed_data = {}\n",
    "        \n",
    "        # KEY CHANGE: Use config for processed data file path\n",
    "        if not force_reprocess and self.config.PROCESSED_DATA_FILE.exists():\n",
    "            self._load_processed_data()\n",
    "        else:\n",
    "            self._process_all_data()\n",
    "            self._save_processed_data()\n",
    "    \n",
    "    def _load_processed_data(self):\n",
    "        \"\"\"Updated to use config path.\"\"\"\n",
    "        with open(self.config.PROCESSED_DATA_FILE, 'rb') as f:\n",
    "            self.processed_data = pickle.load(f)\n",
    "            \n",
    "        # Rebuild metadata DataFrame (unchanged)\n",
    "        self.metadata = pd.DataFrame.from_dict(\n",
    "            {subj: data['metadata'] for subj, data in self.processed_data.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "    \n",
    "    def _save_processed_data(self):\n",
    "        \"\"\"Updated to use config path.\"\"\"\n",
    "        with open(self.config.PROCESSED_DATA_FILE, 'wb') as f:\n",
    "            pickle.dump(self.processed_data, f)\n",
    "    \n",
    "    def _process_all_data(self):\n",
    "        \"\"\"Process all subject data.\"\"\"\n",
    "        self._load_metadata()\n",
    "        total_subjects = len(self.metadata)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"🔄 Processing {total_subjects} subjects...\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(self.metadata.iterrows(), 1):\n",
    "            subject_id = row['ID']\n",
    "            if self.debug:\n",
    "                print(f\"\\n[{i}/{total_subjects}] Processing {subject_id}...\")\n",
    "            \n",
    "            subject_data = self._process_subject_data(subject_id, row)\n",
    "            if subject_data:\n",
    "                self.processed_data[subject_id] = subject_data\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load and clean metadata.\"\"\"\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        self.metadata['DOB'] = pd.to_datetime(self.metadata['DOB'], errors='coerce')\n",
    "        self.metadata['Session Date'] = pd.to_datetime(self.metadata['Session Date'], errors='coerce')\n",
    "        self.metadata = self.metadata.dropna(subset=['ID', 'age_months'])\n",
    "    \n",
    "    def _process_subject_data(self, subject_id: str, metadata_row: pd.Series) -> Optional[Dict]:\n",
    "        \"\"\"Process data for a single subject.\"\"\"\n",
    "        subject_dir = Path(self.data_root_dir) / subject_id\n",
    "        if not subject_dir.exists():\n",
    "            if self.debug:\n",
    "                print(f\"❌ Directory not found: {subject_dir}\")\n",
    "            return None\n",
    "        \n",
    "        trial_data = {}\n",
    "        \n",
    "        for original_type in ['primer', 'trial', 'vis', 'pref']:\n",
    "            try:\n",
    "                # Load trial data\n",
    "                df = self.trial_processor.find_and_combine_trial_files(subject_dir, original_type)\n",
    "                \n",
    "                if df is not None:\n",
    "                    # Process the data\n",
    "                    processed_df, anomalies = self._process_trial_data(df, original_type)\n",
    "                    \n",
    "                    # Store with mapped name\n",
    "                    new_type = self.config.TRIAL_TYPE_MAPPING[original_type]\n",
    "                    trial_data[new_type] = {\n",
    "                        'data': processed_df,\n",
    "                        'anomalies': anomalies\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {subject_id}/{original_type}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            'metadata': metadata_row.to_dict(),\n",
    "            'trial_data': trial_data\n",
    "        } if trial_data else None\n",
    "    \n",
    "    def _process_trial_data(self, df: pd.DataFrame, trial_type: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Process trial data and calculate metrics.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return None, {}\n",
    "        \n",
    "        # Skip processing for pref trials (just clean duplicates)\n",
    "        if trial_type == 'pref':\n",
    "            df = df.drop_duplicates(subset='Left heel strike', keep='last')\n",
    "            return df, {}\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = ['Stride Number', 'Success', 'Upper bound success', \n",
    "                        'Lower bound success', 'Constant']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Missing required columns: {missing_cols}\")\n",
    "            return None, {}\n",
    "        \n",
    "        try:\n",
    "            # Process trial data\n",
    "            df = df.sort_values('Stride Number')\n",
    "            df['Target size'] = df['Upper bound success'] - df['Lower bound success']\n",
    "            df = df.drop_duplicates(subset='Stride Number', keep='last')\n",
    "            \n",
    "            # Scale sum of gains and steps\n",
    "            if 'Sum of gains and steps' in df.columns:\n",
    "                df['Sum of gains and steps'] = 1.5 * df['Sum of gains and steps']\n",
    "            \n",
    "            # Detect anomalies\n",
    "            df, anomalies = DataUtils.detect_anomalies(df)\n",
    "            \n",
    "            return df, anomalies\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {trial_type} data: {str(e)}\")\n",
    "            return None, {}\n",
    "    \n",
    "    def get_trial_df(self, trial_dict: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get trial data from trial dictionary.\"\"\"\n",
    "        if trial_dict and 'data' in trial_dict:\n",
    "            return trial_dict['data']\n",
    "        return None\n",
    "    \n",
    "    def filter_trials(self, max_target_size=None, min_age=None, max_age=None, \n",
    "                     required_trial_types=None, min_strides=None, max_strides=None):\n",
    "        \"\"\"Filter trials based on specified criteria.\"\"\"\n",
    "        filtered_data = {}\n",
    "        \n",
    "        for subject_id, subject_data in self.processed_data.items():\n",
    "            # SIMPLIFIED AGE FILTERING - always use age_months/12\n",
    "            age = subject_data['metadata']['age_months'] / 12\n",
    "            if min_age is not None and age < min_age:\n",
    "                continue\n",
    "            if max_age is not None and age > max_age:\n",
    "                continue\n",
    "            \n",
    "            # Check required trial types\n",
    "            if required_trial_types:\n",
    "                missing_trials = [\n",
    "                    t for t in required_trial_types \n",
    "                    if t not in subject_data['trial_data'] or \n",
    "                       subject_data['trial_data'][t] is None or\n",
    "                       subject_data['trial_data'][t]['data'] is None\n",
    "                ]\n",
    "                if missing_trials:\n",
    "                    continue\n",
    "            \n",
    "            # Check other criteria\n",
    "            valid_subject = True\n",
    "            filtered_trial_data = {}\n",
    "            \n",
    "            for trial_type, trial_dict in subject_data['trial_data'].items():\n",
    "                if trial_dict and trial_dict['data'] is not None:\n",
    "                    df = trial_dict['data']\n",
    "                    \n",
    "                    # Apply filters\n",
    "                    if (max_target_size is not None and \n",
    "                        'Target size' in df.columns and \n",
    "                        df['Target size'].min() > max_target_size):\n",
    "                        valid_subject = False\n",
    "                        break\n",
    "                    \n",
    "                    n_strides = len(df)\n",
    "                    if ((min_strides is not None and n_strides < min_strides) or\n",
    "                        (max_strides is not None and n_strides > max_strides)):\n",
    "                        valid_subject = False\n",
    "                        break\n",
    "                    \n",
    "                    # Include valid trial\n",
    "                    filtered_trial_data[trial_type] = {\n",
    "                        'data': df.copy(),\n",
    "                        'anomalies': trial_dict['anomalies'].copy()\n",
    "                    }\n",
    "            \n",
    "            if valid_subject and filtered_trial_data:\n",
    "                filtered_data[subject_id] = {\n",
    "                    'metadata': subject_data['metadata'].copy(),\n",
    "                    'trial_data': filtered_trial_data\n",
    "                }\n",
    "        \n",
    "        # Create new instance with filtered data\n",
    "        new_instance = MotorLearningDataManager.__new__(MotorLearningDataManager)\n",
    "        new_instance.config = self.config\n",
    "        new_instance.trial_processor = self.trial_processor\n",
    "        new_instance.metadata_path = self.metadata_path\n",
    "        new_instance.data_root_dir = self.data_root_dir\n",
    "        new_instance.debug = self.debug\n",
    "        new_instance.processed_data = filtered_data\n",
    "        new_instance.metadata = pd.DataFrame.from_dict(\n",
    "            {subj: data['metadata'] for subj, data in filtered_data.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "        \n",
    "        return new_instance\n",
    "    \n",
    "    def get_trial_data(self, subject_id: str, trial_type: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get trial data for a specific subject and trial type.\"\"\"\n",
    "        try:\n",
    "            trial_dict = self.processed_data[subject_id]['trial_data'][trial_type]\n",
    "            return trial_dict['data'] if trial_dict else None\n",
    "        except KeyError:\n",
    "            return None\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary statistics of the dataset.\"\"\"\n",
    "        print(f\"📊 Dataset Summary:\")\n",
    "        print(f\"Total subjects: {len(self.processed_data)}\")\n",
    "        \n",
    "        if self.metadata is not None:\n",
    "            # SIMPLIFIED AGE DISPLAY - always use age_months/12\n",
    "            ages = self.metadata['age_months'] / 12\n",
    "            print(f\"Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "            print(f\"Mean age: {ages.mean():.1f} years\")\n",
    "        \n",
    "        # Trial type counts\n",
    "        trial_counts = defaultdict(int)\n",
    "        for subject_data in self.processed_data.values():\n",
    "            for trial_type in subject_data['trial_data'].keys():\n",
    "                trial_counts[trial_type] += 1\n",
    "        \n",
    "        print(f\"Trial type counts: {dict(trial_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d5704-0c04-4d6e-990e-d46391979b3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 5. METRICS CALCULATOR\n",
    "# ==============================================================================\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Updated to use config thresholds.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_manager: MotorLearningDataManager):\n",
    "        self.data_manager = data_manager\n",
    "        # KEY CHANGE: Get config from data_manager\n",
    "        self.config = data_manager.config\n",
    "    \n",
    "    def calculate_all_metrics(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate comprehensive performance metrics for all subjects.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"🧮 Calculating metrics for {len(self.data_manager.processed_data)} subjects...\")\n",
    "        \n",
    "        for i, (subject_id, data) in enumerate(self.data_manager.processed_data.items(), 1):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"   Processed {i}/{len(self.data_manager.processed_data)} subjects...\")\n",
    "            \n",
    "            try:\n",
    "                subject_result = self._calculate_subject_metrics(subject_id, data)\n",
    "                if subject_result:\n",
    "                    results.append(subject_result)\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error processing {subject_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            print(\"❌ No valid metrics calculated!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(results).infer_objects()\n",
    "        print(f\"✅ Successfully calculated metrics for {len(df)} subjects\")\n",
    "        \n",
    "        # Print age and column summary\n",
    "        if 'age' in df.columns:\n",
    "            ages = df['age'].dropna()\n",
    "            print(f\"📊 Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "        \n",
    "        sr_cols = [col for col in df.columns if '_sr_' in col]\n",
    "        print(f\"🎯 Success rate columns created: {sr_cols}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _calculate_subject_metrics(self, subject_id: str, subject_data: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Calculate metrics for a single subject with simplified age handling.\"\"\"\n",
    "        \n",
    "        # SIMPLIFIED: Only store age as age_months/12, call it 'age'\n",
    "        result = {\n",
    "            'ID': subject_id,\n",
    "            'age': subject_data['metadata'].get('age_months', np.nan) / 12,  # Single age field\n",
    "            'session_date': subject_data['metadata'].get('Session Date')\n",
    "        }\n",
    "        \n",
    "        # Process each trial type\n",
    "        for trial_type in ['vis1', 'invis', 'vis2']:\n",
    "            try:\n",
    "                trial_dict = subject_data['trial_data'].get(trial_type)\n",
    "                df = trial_dict['data'] if trial_dict else None\n",
    "                \n",
    "                if df is None or df.empty or 'Success' not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate metrics for both conditions\n",
    "                for condition in ['max', 'min']:\n",
    "                    period_data, indices = self._get_period_data(df, condition)\n",
    "                    if period_data is not None and not period_data.empty:\n",
    "                        metrics = self._calculate_period_metrics(period_data, trial_type, condition)\n",
    "                        result.update(metrics)\n",
    "                        result[f'{trial_type}_{condition}_const_indices'] = indices\n",
    "                \n",
    "                # Add trial metadata - ONLY if df is not None\n",
    "                if df is not None:\n",
    "                    result.update({\n",
    "                        f'{trial_type}_min_target_size': df['Target size'].min() if 'Target size' in df.columns else None,\n",
    "                        f'{trial_type}_max_constant': df['Constant'].max() if 'Constant' in df.columns else None,\n",
    "                        f'{trial_type}_min_constant': df['Constant'].min() if 'Constant' in df.columns else None\n",
    "                    })\n",
    "                    \n",
    "                    # Order information for invis trials\n",
    "                    if trial_type == 'invis':\n",
    "                        result.update(self._calculate_condition_order(df))\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Process preference trial\n",
    "        try:\n",
    "            pref_metrics = self._calculate_preference_metrics(subject_data['trial_data'].get('pref'))\n",
    "            result.update(pref_metrics)\n",
    "        except Exception as e:\n",
    "            result.update({'mot_noise': None, 'pref_asymmetry': None})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_period_data(self, df: pd.DataFrame, condition: str, length: int = 20):\n",
    "        \"\"\"Extract data for specific condition period.\"\"\"\n",
    "        try:\n",
    "            if df is None or df.empty:\n",
    "                return None, None\n",
    "            \n",
    "            if 'Target size' not in df.columns or 'Constant' not in df.columns:\n",
    "                return None, None\n",
    "            \n",
    "            min_target = df['Target size'].min()\n",
    "            target_tolerance = 0.001\n",
    "            \n",
    "            min_target_periods = df[df['Target size'] <= min_target + target_tolerance]\n",
    "            if min_target_periods.empty:\n",
    "                return None, None\n",
    "            \n",
    "            const_value = (min_target_periods['Constant'].max() if condition == 'max'\n",
    "                          else min_target_periods['Constant'].min())\n",
    "            \n",
    "            period_data = min_target_periods[\n",
    "                np.isclose(min_target_periods['Constant'], const_value, rtol=1e-5)\n",
    "            ]\n",
    "            \n",
    "            if period_data.empty:\n",
    "                return None, None\n",
    "            \n",
    "            return period_data.tail(length), period_data.index\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def _calculate_period_metrics(self, period_data: pd.DataFrame, trial_type: str, condition: str) -> Dict:\n",
    "        \"\"\"Calculate metrics for a specific period with naming.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # CORRECTED: Use the format that analysis expects\n",
    "            # Success rate - THE KEY METRIC  \n",
    "            metrics[f'{trial_type}_sr_{condition}_const'] = period_data['Success'].mean()\n",
    "            \n",
    "            # Other metrics\n",
    "            if 'Sum of gains and steps' in period_data.columns:\n",
    "                sogs = period_data['Sum of gains and steps']\n",
    "                metrics[f'{trial_type}_sd_{condition}_const'] = sogs.std()\n",
    "                metrics[f'{trial_type}_msl_{condition}_const'] = sogs.mean()\n",
    "                metrics[f'{trial_type}_error_{condition}_const'] = (sogs - period_data['Constant']).mean()\n",
    "            \n",
    "            # Asymmetry\n",
    "            if all(col in period_data.columns for col in ['Right step length', 'Left step length']):\n",
    "                right_steps = period_data['Right step length']\n",
    "                left_steps = period_data['Left step length']\n",
    "                denominator = right_steps + left_steps\n",
    "                \n",
    "                valid_mask = denominator != 0\n",
    "                if valid_mask.any():\n",
    "                    asymmetry = ((right_steps - left_steps) / denominator).abs()[valid_mask].mean()\n",
    "                    metrics[f'{trial_type}_asymmetry_{condition}_const'] = asymmetry\n",
    "            \n",
    "            # Strides between successes\n",
    "            strides_between = self._calculate_strides_between_successes(period_data)\n",
    "            if strides_between is not None:\n",
    "                metrics[f'{trial_type}_strides_between_success_{condition}_const'] = strides_between\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_strides_between_successes(self, df: pd.DataFrame) -> Optional[float]:\n",
    "        \"\"\"Calculate average strides between successful trials.\"\"\"\n",
    "        try:\n",
    "            if df is None or 'Success' not in df.columns:\n",
    "                return None\n",
    "            \n",
    "            df = df.reset_index(drop=True)\n",
    "            success_positions = df.index[df['Success'] == 1].tolist()\n",
    "            \n",
    "            if len(success_positions) < 2:\n",
    "                return None\n",
    "            \n",
    "            return np.mean(np.diff(success_positions))\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _calculate_condition_order(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Determine which condition came first for invis trials.\"\"\"\n",
    "        try:\n",
    "            all_max_indices = df.index[df['Constant'] == df['Constant'].max()].tolist()\n",
    "            all_min_indices = df.index[df['Constant'] == df['Constant'].min()].tolist()\n",
    "            \n",
    "            if all_max_indices and all_min_indices:\n",
    "                first_max = min(all_max_indices)\n",
    "                first_min = min(all_min_indices)\n",
    "                return {\n",
    "                    'invis_max_first': first_max < first_min,\n",
    "                    'invis_min_first': first_min < first_max\n",
    "                }\n",
    "            \n",
    "            return {'invis_max_first': False, 'invis_min_first': False}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'invis_max_first': False, 'invis_min_first': False}\n",
    "    \n",
    "    def _calculate_preference_metrics(self, pref_trial_dict: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Calculate metrics from preference trial.\"\"\"\n",
    "        metrics = {'mot_noise': None, 'pref_asymmetry': None}\n",
    "        \n",
    "        try:\n",
    "            if not pref_trial_dict or pref_trial_dict.get('data') is None:\n",
    "                return metrics\n",
    "            \n",
    "            pref_df = pref_trial_dict['data']\n",
    "            if pref_df is None or pref_df.empty:\n",
    "                return metrics\n",
    "            \n",
    "            # Check for required columns\n",
    "            if not all(col in pref_df.columns for col in ['Right step length', 'Left step length']):\n",
    "                return metrics\n",
    "            \n",
    "            # Get non-zero steps\n",
    "            right_steps = pref_df['Right step length']\n",
    "            left_steps = pref_df['Left step length']\n",
    "            \n",
    "            # Remove zeros and NaNs\n",
    "            right_clean = right_steps[(right_steps != 0) & (right_steps.notna())]\n",
    "            left_clean = left_steps[(left_steps != 0) & (left_steps.notna())]\n",
    "            \n",
    "            if len(right_clean) < 20 or len(left_clean) < 20:\n",
    "                return metrics\n",
    "            \n",
    "            # Calculate motor noise\n",
    "            final_right = right_clean.iloc[-1]\n",
    "            final_left = left_clean.iloc[-1]\n",
    "            \n",
    "            if final_right <= 0 or final_left <= 0:\n",
    "                return metrics\n",
    "            \n",
    "            # Normalize steps\n",
    "            norm_right = right_clean / final_right\n",
    "            norm_left = left_clean / final_left\n",
    "            \n",
    "            # Calculate sum of normalized steps\n",
    "            min_length = min(len(norm_right), len(norm_left))\n",
    "            if min_length < 20:\n",
    "                return metrics\n",
    "            \n",
    "            sum_steps = norm_right.iloc[:min_length] + norm_left.iloc[:min_length]\n",
    "            \n",
    "            # Motor noise from last 20 points\n",
    "            if len(sum_steps) >= 20:\n",
    "                noise = sum_steps.tail(20).std()\n",
    "                if not pd.isna(noise) and noise > 0:\n",
    "                    metrics['mot_noise'] = noise\n",
    "            \n",
    "            # Calculate step length asymmetry\n",
    "            if len(right_clean) >= 20 and len(left_clean) >= 20:\n",
    "                last_20_right = right_clean.tail(20) / final_right\n",
    "                last_20_left = left_clean.tail(20) / final_left\n",
    "                \n",
    "                if len(last_20_right) == len(last_20_left):\n",
    "                    denominator = last_20_right.values + last_20_left.values\n",
    "                    valid_mask = denominator != 0\n",
    "                    \n",
    "                    if valid_mask.any():\n",
    "                        asymmetry_vals = np.abs((last_20_right.values - last_20_left.values) / denominator)[valid_mask]\n",
    "                        if len(asymmetry_vals) > 0:\n",
    "                            metrics['pref_asymmetry'] = np.mean(asymmetry_vals)\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16add11-dcf4-42a5-bccc-c1373ab52afa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 6. STATISTICAL ANALYZER\n",
    "# ==============================================================================\n",
    "\n",
    "class StatisticalAnalyzer:\n",
    "    \"\"\"Updated to use config thresholds.\"\"\"\n",
    "    \n",
    "    def __init__(self, metrics_df: pd.DataFrame, config: Config = None):\n",
    "        self.metrics_df = metrics_df\n",
    "        \n",
    "        # KEY CHANGE: Accept config parameter\n",
    "        self.config = config if config else Config()\n",
    "        \n",
    "        # Apply motor noise filter using config\n",
    "        if 'mot_noise' in metrics_df.columns:\n",
    "            self.filtered_df = metrics_df[metrics_df['mot_noise'] <= self.config.MOTOR_NOISE_THRESHOLD]\n",
    "            print(f\"📊 Filtered to {len(self.filtered_df)}/{len(metrics_df)} subjects (motor noise ≤ {self.config.MOTOR_NOISE_THRESHOLD})\")\n",
    "        else:\n",
    "            self.filtered_df = metrics_df\n",
    "    \n",
    "    def run_regression_analysis(self, trial_type: str = 'invis', condition: str = 'max',\n",
    "                               predictors: List[str] = None, model_type: str = 'linear') -> Dict:\n",
    "        \"\"\"Run regression analysis withtarget column naming.\"\"\"\n",
    "        \n",
    "        if predictors is None:\n",
    "            predictors = ['age', 'mot_noise', 'pref_asymmetry']\n",
    "        \n",
    "        # Filter available predictors\n",
    "        available_predictors = [p for p in predictors if p in self.filtered_df.columns]\n",
    "        \n",
    "        # CORRECTED: Use the exact column format that exists\n",
    "        target_col = f'{trial_type}_sr_{condition}_const'\n",
    "        \n",
    "        if target_col not in self.filtered_df.columns:\n",
    "            available_cols = [col for col in self.filtered_df.columns if '_sr_' in col]\n",
    "            raise ValueError(f\"Target column {target_col} not found. Available: {available_cols}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        valid_data = self.filtered_df[available_predictors + [target_col]].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            raise ValueError(f\"Insufficient data: only {len(valid_data)} valid samples\")\n",
    "        \n",
    "        X = valid_data[available_predictors]\n",
    "        y = valid_data[target_col]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'linear':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', LinearRegression())\n",
    "            ])\n",
    "        elif model_type == 'random_forest':\n",
    "            model = RandomForestRegressor(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'linear' or 'random_forest'\")\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        # Feature importance\n",
    "        if model_type == 'random_forest':\n",
    "            importances = dict(zip(available_predictors, model.feature_importances_))\n",
    "        else:\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 np.abs(model.named_steps['regressor'].coef_)))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'metrics': {\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                'n_samples': len(valid_data),\n",
    "                'trial_type': trial_type,\n",
    "                'condition': condition,\n",
    "                'predictors': available_predictors\n",
    "            },\n",
    "            'feature_importances': importances\n",
    "        }\n",
    "    \n",
    "    def run_classification_analysis(self, trial_type: str = 'invis', condition: str = 'max',\n",
    "                                   threshold: float = 0.68, model_type: str = 'logistic') -> Dict:\n",
    "        \"\"\"Run binary classification withtarget column naming.\"\"\"\n",
    "        \n",
    "        predictors = ['age', 'mot_noise']\n",
    "        available_predictors = [p for p in predictors if p in self.filtered_df.columns]\n",
    "        \n",
    "        # CORRECTED: Use the exact column format that exists\n",
    "        target_col = f'{trial_type}_sr_{condition}_const'\n",
    "        \n",
    "        if target_col not in self.filtered_df.columns:\n",
    "            available_cols = [col for col in self.filtered_df.columns if '_sr_' in col]\n",
    "            raise ValueError(f\"Target column {target_col} not found. Available: {available_cols}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        valid_data = self.filtered_df[available_predictors + [target_col]].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            raise ValueError(f\"Insufficient data: only {len(valid_data)} valid samples\")\n",
    "        \n",
    "        # Create binary target\n",
    "        y = (valid_data[target_col] >= threshold).astype(int)\n",
    "        X = valid_data[available_predictors]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'logistic':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', LogisticRegression(random_state=42))\n",
    "            ])\n",
    "        elif model_type == 'random_forest':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', RandomForestClassifier(random_state=42))\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'logistic' or 'random_forest'\")\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'n_samples': len(valid_data),\n",
    "            'threshold': threshold,\n",
    "            'trial_type': trial_type,\n",
    "            'condition': condition,\n",
    "            'predictors': available_predictors\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if model_type == 'random_forest':\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 model.named_steps['classifier'].feature_importances_))\n",
    "        else:\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 model.named_steps['classifier'].coef_[0]))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'y_proba': y_proba,\n",
    "            'metrics': metrics,\n",
    "            'feature_importances': importances\n",
    "        }\n",
    "    \n",
    "    def run_mixed_effects_analysis(self, trial_types: Union[str, List[str]] = 'all') -> object:\n",
    "        \"\"\"Run mixed-effects analysis withcolumn naming.\"\"\"\n",
    "        \n",
    "        # Prepare long-format data\n",
    "        long_rows = []\n",
    "        \n",
    "        if trial_types == 'all':\n",
    "            trials_to_include = ['vis1', 'invis', 'vis2']\n",
    "        elif isinstance(trial_types, str):\n",
    "            trials_to_include = [trial_types]\n",
    "        else:\n",
    "            trials_to_include = trial_types\n",
    "        \n",
    "        for trial in trials_to_include:\n",
    "            for condition in ['max_const', 'min_const']:\n",
    "                # CORRECTED: Use the exact column format\n",
    "                sr_col = f\"{trial}_sr_{condition}\"\n",
    "                if sr_col in self.filtered_df.columns:\n",
    "                    sub_df = self.filtered_df[['ID', sr_col, 'mot_noise', 'age']].copy()\n",
    "                    sub_df = sub_df.rename(columns={sr_col: 'success_rate'})\n",
    "                    sub_df['trial_type'] = trial\n",
    "                    sub_df['condition'] = condition\n",
    "                    long_rows.append(sub_df)\n",
    "        \n",
    "        if not long_rows:\n",
    "            raise ValueError(\"No success rate data available\")\n",
    "        \n",
    "        df_long = pd.concat(long_rows, ignore_index=True)\n",
    "        df_long.dropna(subset=['success_rate', 'mot_noise', 'age'], inplace=True)\n",
    "        \n",
    "        # Fit model\n",
    "        if len(trials_to_include) == 1:\n",
    "            formula = 'success_rate ~ C(condition) + mot_noise + age'\n",
    "        else:\n",
    "            formula = 'success_rate ~ C(trial_type) + C(condition) + mot_noise + age'\n",
    "        \n",
    "        model = smf.ols(formula, data=df_long).fit()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a52a2-751e-445b-a081-5b2cf153751d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STANDALONE ENHANCED PLOTTING FOR MOTOR LEARNING ANALYSIS\n",
    "# ==============================================================================\n",
    "# This code works directly with your existing notebook without imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from scipy.stats import gaussian_kde, pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StandaloneEnhancedVisualizer:\n",
    "    \"\"\"Enhanced visualizer that works with your existing analysis objects.\"\"\"\n",
    "    \n",
    "    def __init__(self, analysis_instance, config: Config = None):\n",
    "        \"\"\"\n",
    "        Initialize with your existing analysis instance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        analysis_instance : Your existing MotorLearningAnalysis object\n",
    "            Should have .data_manager, .metrics_df attributes\n",
    "        \"\"\"\n",
    "        self.analysis = analysis_instance\n",
    "        self.metrics_df = analysis_instance.metrics_df\n",
    "        self.data_manager = analysis_instance.data_manager\n",
    "        \n",
    "        # Use provided config or get from data_manager or create default\n",
    "        if config:\n",
    "            self.config = config\n",
    "        elif hasattr(self.data_manager, 'config'):\n",
    "            self.config = self.data_manager.config\n",
    "        else:\n",
    "            self.config = Config()\n",
    "        \n",
    "        # FIXED: Set up directory attributes properly\n",
    "        self.individual_plots_dir = self.config.INDIVIDUAL_PLOTS_DIR\n",
    "        self.population_plots_dir = self.config.POPULATION_PLOTS_DIR\n",
    "        self.statistical_plots_dir = self.config.STATISTICAL_PLOTS_DIR\n",
    "        \n",
    "        # Apply motor noise filter\n",
    "        if 'mot_noise' in self.metrics_df.columns:\n",
    "            self.filtered_df = self.metrics_df[self.metrics_df['mot_noise'] <= 0.3]\n",
    "            print(f\"📊 Using {len(self.filtered_df)}/{len(self.metrics_df)} subjects (motor noise ≤ 0.3)\")\n",
    "        else:\n",
    "            self.filtered_df = self.metrics_df\n",
    "            print(f\"📊 Using all {len(self.filtered_df)} subjects\")\n",
    "        \n",
    "        # Set colors\n",
    "        self.colors = {\n",
    "            'primary': '#667eea',\n",
    "            'secondary': '#764ba2',\n",
    "            'success': '#28a745',\n",
    "            'warning': '#ffc107',\n",
    "            'danger': '#dc3545',\n",
    "            'vis1': '#1f77b4',\n",
    "            'invis': '#ff7f0e', \n",
    "            'vis2': '#2ca02c'\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Enhanced visualizer initialized\")\n",
    "        print(f\"📁 Individual plots will be saved to: {self.individual_plots_dir}\")\n",
    "        print(f\"📁 Population plots will be saved to: {self.population_plots_dir}\")\n",
    "\n",
    "    def plot_all_individual_stride_changes(self, trial_types: List[str] = None, \n",
    "                                         subject_ids: List[str] = None,\n",
    "                                         save_summary: bool = True,\n",
    "                                         max_subjects: int = None) -> Dict:\n",
    "        \"\"\"Generate stride change distribution plots for all participants.\"\"\"\n",
    "        \n",
    "        if trial_types is None:\n",
    "            trial_types = ['vis1', 'invis', 'vis2']\n",
    "        \n",
    "        if subject_ids is None:\n",
    "            subject_ids = list(self.data_manager.processed_data.keys())\n",
    "        \n",
    "        # Limit subjects if requested\n",
    "        if max_subjects and len(subject_ids) > max_subjects:\n",
    "            subject_ids = subject_ids[:max_subjects]\n",
    "            print(f\"🔄 Limited to first {max_subjects} subjects for testing\")\n",
    "        \n",
    "        print(f\"🎯 Generating individual stride change plots...\")\n",
    "        print(f\"   📊 {len(subject_ids)} subjects\")\n",
    "        print(f\"   🎮 Trial types: {trial_types}\")\n",
    "        print(f\"   💾 Saving to: {self.individual_plots_dir}\")\n",
    "        \n",
    "        all_stats = {}\n",
    "        successful_plots = 0\n",
    "        failed_plots = 0\n",
    "        \n",
    "        # Progress bar for subjects\n",
    "        for subject_id in tqdm(subject_ids, desc=\"Processing subjects\"):\n",
    "            subject_stats = {}\n",
    "            \n",
    "            # Get subject metadata\n",
    "            if subject_id in self.data_manager.processed_data:\n",
    "                age = self.data_manager.processed_data[subject_id]['metadata'].get('age_months', np.nan) / 12\n",
    "                subject_stats['age'] = age\n",
    "            \n",
    "            # Process each trial type\n",
    "            for trial_type in trial_types:\n",
    "                try:\n",
    "                    # Use our own implementation\n",
    "                    stats = self._plot_individual_stride_change_internal(\n",
    "                        subject_id, trial_type, save=True, show_stats=False\n",
    "                    )\n",
    "                    \n",
    "                    if stats:\n",
    "                        subject_stats[trial_type] = stats\n",
    "                        successful_plots += 1\n",
    "                    else:\n",
    "                        failed_plots += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ Error plotting {subject_id} {trial_type}: {str(e)}\")\n",
    "                    failed_plots += 1\n",
    "                    continue\n",
    "            \n",
    "            if subject_stats:\n",
    "                all_stats[subject_id] = subject_stats\n",
    "        \n",
    "        print(f\"✅ Completed: {successful_plots} successful plots, {failed_plots} failed\")\n",
    "        \n",
    "        # Save summary if requested\n",
    "        if save_summary:\n",
    "            self._save_stride_analysis_summary(all_stats)\n",
    "        \n",
    "        return all_stats\n",
    "\n",
    "    def plot_population_analyses(self, save: bool = True) -> None:\n",
    "        \"\"\"Generate comprehensive population-level analysis plots.\"\"\"\n",
    "        \n",
    "        print(\"📊 Generating population-level analysis plots...\")\n",
    "        print(f\"💾 Saving to: {self.population_plots_dir}\")\n",
    "        \n",
    "        # 1. Age vs Motor Noise\n",
    "        print(\"   🧠 Age vs Motor Noise...\")\n",
    "        self._plot_age_vs_motor_noise(save)\n",
    "        \n",
    "        # 2. Age vs Success Rates for each target in each trial\n",
    "        print(\"   🎯 Age vs Success Rates...\")\n",
    "        self._plot_age_vs_success_rates(save)\n",
    "        \n",
    "        # 3. Age vs Standard Deviation of stride length for each target in each trial\n",
    "        print(\"   📏 Age vs Stride Variability...\")\n",
    "        self._plot_age_vs_stride_variability(save)\n",
    "        \n",
    "        # 4. Comprehensive correlation matrix\n",
    "        print(\"   🔗 Correlation Matrix...\")\n",
    "        self._plot_comprehensive_correlation_matrix(save)\n",
    "        \n",
    "        # 5. Success rate comparisons across trials and targets\n",
    "        print(\"   📊 Success Rate Comparisons...\")\n",
    "        self._plot_success_rate_comparisons(save)\n",
    "        \n",
    "        print(\"✅ Population analysis plots completed!\")\n",
    "\n",
    "    def _plot_age_vs_motor_noise(self, save: bool = True) -> None:\n",
    "        \"\"\"Plot age vs motor noise relationship.\"\"\"\n",
    "        \n",
    "        if 'mot_noise' not in self.filtered_df.columns or 'age' not in self.filtered_df.columns:\n",
    "            print(\"⚠️ Missing motor noise or age data for age vs motor noise plot\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Main scatter plot\n",
    "        valid_data = self.filtered_df.dropna(subset=['age', 'mot_noise'])\n",
    "        \n",
    "        if valid_data.empty:\n",
    "            plt.text(0.5, 0.5, 'No valid data available', ha='center', va='center',\n",
    "                    transform=plt.gca().transAxes, fontsize=14)\n",
    "            plt.title('Age vs Motor Noise - No Data')\n",
    "            if save:\n",
    "                plt.savefig(self.population_plots_dir / 'age_vs_motor_noise.png', \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            return\n",
    "        \n",
    "        scatter = plt.scatter(valid_data['age'], valid_data['mot_noise'], \n",
    "                            alpha=0.6, s=60, edgecolors='white', linewidths=0.5,\n",
    "                            c=self.colors['primary'])\n",
    "        \n",
    "        # Add trend line\n",
    "        self._add_trendline(plt.gca(), valid_data['age'], valid_data['mot_noise'])\n",
    "        \n",
    "        # Add motor noise threshold line\n",
    "        plt.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, \n",
    "                   label='Motor Noise Threshold (0.3)')\n",
    "        \n",
    "        plt.xlabel('Age (years)', fontsize=12)\n",
    "        plt.ylabel('Motor Noise', fontsize=12)\n",
    "        plt.title('Age vs Motor Noise Relationship', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add statistics text\n",
    "        r, p = pearsonr(valid_data['age'], valid_data['mot_noise'])\n",
    "        plt.text(0.05, 0.95, f'r = {r:.3f}\\np = {p:.3f}\\nn = {len(valid_data)}',\n",
    "                transform=plt.gca().transAxes, fontsize=11,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                verticalalignment='top')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.population_plots_dir / 'age_vs_motor_noise.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_age_vs_success_rates(self, save: bool = True) -> None:\n",
    "        \"\"\"Plot age vs success rates for each target in each trial.\"\"\"\n",
    "        \n",
    "        # Find all success rate columns\n",
    "        sr_columns = [col for col in self.filtered_df.columns if '_sr_' in col and '_const' in col]\n",
    "        \n",
    "        if not sr_columns:\n",
    "            print(\"⚠️ No success rate columns found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   Found {len(sr_columns)} success rate columns: {sr_columns}\")\n",
    "        \n",
    "        # Create subplot grid\n",
    "        n_cols = min(len(sr_columns), 4)  # Max 4 columns to keep readable\n",
    "        n_rows = int(np.ceil(len(sr_columns) / n_cols))\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 6*n_rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif n_cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        # Flatten for easy iteration\n",
    "        axes_flat = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "        \n",
    "        # Age groups for additional analysis\n",
    "        age_bins = [7, 10, 13, 16, 18]\n",
    "        age_labels = ['7-10', '10-13', '13-16', '16-18']\n",
    "        \n",
    "        for i, col in enumerate(sr_columns):\n",
    "            if i >= len(axes_flat):\n",
    "                break\n",
    "                \n",
    "            ax = axes_flat[i]\n",
    "            \n",
    "            # Parse column name for title\n",
    "            parts = col.split('_')\n",
    "            trial_type = parts[0]\n",
    "            condition = parts[2]  # 'max' or 'min'\n",
    "            title = f'{trial_type.upper()}: {condition.capitalize()} Target'\n",
    "            \n",
    "            valid_data = self.filtered_df.dropna(subset=['age', col])\n",
    "            \n",
    "            if valid_data.empty:\n",
    "                ax.text(0.5, 0.5, 'No Data', ha='center', va='center',\n",
    "                       transform=ax.transAxes, fontsize=12)\n",
    "                ax.set_title(title)\n",
    "                continue\n",
    "            \n",
    "            # Scatter plot\n",
    "            color = self.colors.get(trial_type, self.colors['primary'])\n",
    "            ax.scatter(valid_data['age'], valid_data[col], \n",
    "                      alpha=0.6, s=50, color=color, edgecolors='white', linewidths=0.5)\n",
    "            \n",
    "            # Add trend line\n",
    "            self._add_trendline(ax, valid_data['age'], valid_data[col])\n",
    "            \n",
    "            ax.set_xlabel('Age (years)')\n",
    "            ax.set_ylabel('Success Rate')\n",
    "            ax.set_title(title)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_ylim(-0.05, 1.05)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(sr_columns), len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Age vs Success Rates Analysis', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.population_plots_dir / 'age_vs_success_rates.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_age_vs_stride_variability(self, save: bool = True) -> None:\n",
    "        \"\"\"Plot age vs standard deviation of stride length for each target in each trial.\"\"\"\n",
    "        \n",
    "        # Find all standard deviation columns\n",
    "        sd_columns = [col for col in self.filtered_df.columns if '_sd_' in col and '_const' in col]\n",
    "        \n",
    "        if not sd_columns:\n",
    "            print(\"⚠️ No stride variability columns found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   Found {len(sd_columns)} stride variability columns: {sd_columns}\")\n",
    "        \n",
    "        # Create subplot grid\n",
    "        n_cols = min(len(sd_columns), 4)  # Max 4 columns\n",
    "        n_rows = int(np.ceil(len(sd_columns) / n_cols))\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 6*n_rows))\n",
    "        \n",
    "        # Handle subplot cases\n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif n_cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        axes_flat = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "        \n",
    "        for i, col in enumerate(sd_columns):\n",
    "            if i >= len(axes_flat):\n",
    "                break\n",
    "                \n",
    "            ax = axes_flat[i]\n",
    "            \n",
    "            # Parse column name for title\n",
    "            parts = col.split('_')\n",
    "            trial_type = parts[0]\n",
    "            condition = parts[2]  # 'max' or 'min'\n",
    "            title = f'{trial_type.upper()}: {condition.capitalize()}\\nStride Variability'\n",
    "            \n",
    "            valid_data = self.filtered_df.dropna(subset=['age', col])\n",
    "            \n",
    "            if valid_data.empty:\n",
    "                ax.text(0.5, 0.5, 'No Data', ha='center', va='center',\n",
    "                       transform=ax.transAxes, fontsize=12)\n",
    "                ax.set_title(title)\n",
    "                continue\n",
    "            \n",
    "            # Scatter plot\n",
    "            color = self.colors.get(trial_type, self.colors['primary'])\n",
    "            ax.scatter(valid_data['age'], valid_data[col], \n",
    "                      alpha=0.6, s=50, color=color, edgecolors='white', linewidths=0.5)\n",
    "            \n",
    "            # Add trend line\n",
    "            self._add_trendline(ax, valid_data['age'], valid_data[col])\n",
    "            \n",
    "            ax.set_xlabel('Age (years)')\n",
    "            ax.set_ylabel('Stride Length SD')\n",
    "            ax.set_title(title)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(sd_columns), len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Age vs Stride Length Variability', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.population_plots_dir / 'age_vs_stride_variability.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_comprehensive_correlation_matrix(self, save: bool = True) -> None:\n",
    "        \"\"\"Plot comprehensive correlation matrix of all key variables.\"\"\"\n",
    "        \n",
    "        # Select key columns for correlation\n",
    "        key_columns = []\n",
    "        \n",
    "        if 'age' in self.filtered_df.columns:\n",
    "            key_columns.append('age')\n",
    "        if 'mot_noise' in self.filtered_df.columns:\n",
    "            key_columns.append('mot_noise')\n",
    "        if 'pref_asymmetry' in self.filtered_df.columns:\n",
    "            key_columns.append('pref_asymmetry')\n",
    "        \n",
    "        # Add success rate columns (limit to avoid overcrowding)\n",
    "        sr_columns = [col for col in self.filtered_df.columns if '_sr_' in col and '_const' in col]\n",
    "        key_columns.extend(sr_columns[:6])  # Limit to 6 SR columns\n",
    "        \n",
    "        # Add stride variability columns (limit to avoid overcrowding)\n",
    "        sd_columns = [col for col in self.filtered_df.columns if '_sd_' in col and '_const' in col]\n",
    "        key_columns.extend(sd_columns[:4])  # Limit to 4 SD columns\n",
    "        \n",
    "        if len(key_columns) < 2:\n",
    "            print(\"⚠️ Not enough variables for correlation matrix\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   Using {len(key_columns)} variables for correlation matrix\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_data = self.filtered_df[key_columns].corr()\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Create heatmap\n",
    "        mask = np.triu(np.ones_like(corr_data, dtype=bool))\n",
    "        sns.heatmap(corr_data, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                   square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "                   fmt='.2f')\n",
    "        \n",
    "        plt.title('Comprehensive Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.population_plots_dir / 'correlation_matrix.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_success_rate_comparisons(self, save: bool = True) -> None:\n",
    "        \"\"\"Plot success rate comparisons across trials and targets.\"\"\"\n",
    "        \n",
    "        # Prepare data for comparison\n",
    "        comparison_data = []\n",
    "        \n",
    "        for trial_type in ['vis1', 'invis', 'vis2']:\n",
    "            for condition in ['max', 'min']:\n",
    "                col = f'{trial_type}_sr_{condition}_const'\n",
    "                if col in self.filtered_df.columns:\n",
    "                    values = self.filtered_df[col].dropna()\n",
    "                    for val in values:\n",
    "                        comparison_data.append({\n",
    "                            'trial_type': trial_type,\n",
    "                            'condition': condition,\n",
    "                            'success_rate': val\n",
    "                        })\n",
    "        \n",
    "        if not comparison_data:\n",
    "            print(\"⚠️ No success rate data found for comparison\")\n",
    "            return\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(f\"   Using {len(comparison_df)} data points for success rate comparisons\")\n",
    "        \n",
    "        # Create comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Box plot by trial type\n",
    "        sns.boxplot(data=comparison_df, x='trial_type', y='success_rate', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Success Rates by Trial Type')\n",
    "        axes[0, 0].set_ylabel('Success Rate')\n",
    "        axes[0, 0].set_ylim(-0.05, 1.05)\n",
    "        \n",
    "        # 2. Box plot by condition\n",
    "        sns.boxplot(data=comparison_df, x='condition', y='success_rate', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Success Rates by Target Condition')\n",
    "        axes[0, 1].set_ylabel('Success Rate')\n",
    "        axes[0, 1].set_ylim(-0.05, 1.05)\n",
    "        \n",
    "        # 3. Box plot by trial type and condition\n",
    "        sns.boxplot(data=comparison_df, x='trial_type', y='success_rate', \n",
    "                   hue='condition', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Success Rates by Trial Type and Condition')\n",
    "        axes[1, 0].set_ylabel('Success Rate')\n",
    "        axes[1, 0].set_ylim(-0.05, 1.05)\n",
    "        \n",
    "        # 4. Violin plot for distribution shapes\n",
    "        sns.violinplot(data=comparison_df, x='trial_type', y='success_rate', \n",
    "                      hue='condition', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Success Rate Distributions')\n",
    "        axes[1, 1].set_ylabel('Success Rate')\n",
    "        axes[1, 1].set_ylim(-0.05, 1.05)\n",
    "        \n",
    "        plt.suptitle('Success Rate Comparisons Across Conditions', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.population_plots_dir / 'success_rate_comparisons.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _add_trendline(self, ax, x, y):\n",
    "        \"\"\"Add trendline with correlation coefficient to plot.\"\"\"\n",
    "        x_clean = pd.to_numeric(x, errors='coerce')\n",
    "        y_clean = pd.to_numeric(y, errors='coerce')\n",
    "        valid = x_clean.notna() & y_clean.notna()\n",
    "        \n",
    "        if valid.sum() < 2:\n",
    "            return\n",
    "        \n",
    "        x_vals = x_clean[valid]\n",
    "        y_vals = y_clean[valid]\n",
    "        \n",
    "        # Fit line\n",
    "        coeffs = np.polyfit(x_vals, y_vals, 1)\n",
    "        trendline = np.poly1d(coeffs)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        r, p = pearsonr(x_vals, y_vals)\n",
    "        \n",
    "        # Plot trendline\n",
    "        ax.plot(x_vals, trendline(x_vals), 'r--', alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Add correlation text\n",
    "        ax.text(0.05, 0.95, f'r = {r:.3f}\\np = {p:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                verticalalignment='top')\n",
    "\n",
    "    def _save_stride_analysis_summary(self, all_stats: Dict) -> None:\n",
    "        \"\"\"Save summary of stride analysis to CSV.\"\"\"\n",
    "        \n",
    "        summary_rows = []\n",
    "        \n",
    "        for subject_id, subject_data in all_stats.items():\n",
    "            base_row = {\n",
    "                'subject_id': subject_id,\n",
    "                'age': subject_data.get('age', np.nan)\n",
    "            }\n",
    "            \n",
    "            for trial_type in ['vis1', 'invis', 'vis2']:\n",
    "                if trial_type in subject_data:\n",
    "                    trial_data = subject_data[trial_type]\n",
    "                    for condition in ['max', 'min']:\n",
    "                        row = base_row.copy()\n",
    "                        row['trial_type'] = trial_type\n",
    "                        row['condition'] = condition\n",
    "                        \n",
    "                        # Add condition-specific data\n",
    "                        for key, value in trial_data.items():\n",
    "                            if key.startswith(f'{condition}_'):\n",
    "                                clean_key = key.replace(f'{condition}_', '')\n",
    "                                row[clean_key] = value\n",
    "                        \n",
    "                        summary_rows.append(row)\n",
    "        \n",
    "        if summary_rows:\n",
    "            summary_df = pd.DataFrame(summary_rows)\n",
    "            summary_path = self.individual_plots_dir / 'stride_analysis_summary.csv'\n",
    "            summary_df.to_csv(summary_path, index=False)\n",
    "            print(f\"📊 Stride analysis summary saved to: {summary_path}\")\n",
    "\n",
    "    def generate_all_plots(self, subject_ids: List[str] = None, \n",
    "                          trial_types: List[str] = None,\n",
    "                          max_subjects: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate all plots: individual stride changes and population analyses.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_ids : List[str], optional\n",
    "            Specific subjects to analyze (default: all available subjects)\n",
    "        trial_types : List[str], optional\n",
    "            Trial types to analyze (default: ['vis1', 'invis', 'vis2'])\n",
    "        max_subjects : int, optional\n",
    "            Maximum number of subjects to process (useful for testing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict : Summary statistics from stride analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🚀 Starting comprehensive plotting analysis...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. Generate individual stride change plots\n",
    "        print(\"\\n1️⃣ INDIVIDUAL STRIDE CHANGE PLOTS\")\n",
    "        stride_stats = self.plot_all_individual_stride_changes(\n",
    "            trial_types=trial_types, \n",
    "            subject_ids=subject_ids,\n",
    "            save_summary=True,\n",
    "            max_subjects=max_subjects\n",
    "        )\n",
    "        \n",
    "        # 2. Generate population-level analyses\n",
    "        print(\"\\n2️⃣ POPULATION-LEVEL ANALYSES\")\n",
    "        self.plot_population_analyses(save=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ ALL PLOTS COMPLETED!\")\n",
    "        print(f\"📁 Individual plots saved to: {self.individual_plots_dir}\")\n",
    "        print(f\"📁 Population plots saved to: {self.population_plots_dir}\")\n",
    "        print(f\"📊 Summary statistics available for {len(stride_stats)} subjects\")\n",
    "        \n",
    "        return stride_stats\n",
    "\n",
    "    def _plot_individual_stride_change_internal(self, subject_id: str, trial_types: List[str] = None,\n",
    "                                               stride_col: str = 'Sum of gains and steps',\n",
    "                                               figsize: Tuple[int, int] = (16, 18), \n",
    "                                               save: bool = True, alpha: float = 0.7,\n",
    "                                               show_stats: bool = False) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Internal method for plotting individual stride changes with FIXED 3x2 grid layout.\n",
    "        Always creates exactly 6 subplots (3 trials × 2 conditions) regardless of data availability.\n",
    "        \"\"\"\n",
    "        \n",
    "        if trial_types is None:\n",
    "            trial_types = ['vis1', 'invis', 'vis2']\n",
    "        \n",
    "        # Check if subject exists\n",
    "        if subject_id not in self.data_manager.processed_data:\n",
    "            return None\n",
    "        \n",
    "        # Get subject age for title\n",
    "        subject_age = self.data_manager.processed_data[subject_id]['metadata'].get('age_months', np.nan) / 12\n",
    "        \n",
    "        # FIXED: Always create 3x2 grid regardless of data availability\n",
    "        fig, axes = plt.subplots(3, 2, figsize=figsize, sharey=False)\n",
    "        \n",
    "        # Define the fixed layout mapping\n",
    "        layout_mapping = {\n",
    "            'vis1': (0, 0, 0, 1),    # Row 0, columns 0 and 1\n",
    "            'invis': (1, 0, 1, 1),   # Row 1, columns 0 and 1  \n",
    "            'vis2': (2, 0, 2, 1)     # Row 2, columns 0 and 1\n",
    "        }\n",
    "        \n",
    "        # Condition names for column headers\n",
    "        condition_names = ['Upper Target (Max Constant)', 'Lower Target (Min Constant)']\n",
    "        \n",
    "        # Helper function to get period data\n",
    "        def get_period_data(df, constant_condition, length=20):\n",
    "            if df is None or df.empty:\n",
    "                return None\n",
    "                \n",
    "            min_target = df['Target size'].min()\n",
    "            target_tolerance = 0.001\n",
    "            \n",
    "            min_target_periods = df[df['Target size'] <= min_target + target_tolerance]\n",
    "            if min_target_periods.empty:\n",
    "                return None\n",
    "                \n",
    "            const_value = (\n",
    "                min_target_periods['Constant'].max() if constant_condition == 'max'\n",
    "                else min_target_periods['Constant'].min()\n",
    "            )\n",
    "            \n",
    "            period_data = min_target_periods[\n",
    "                np.isclose(min_target_periods['Constant'], const_value, rtol=1e-5)\n",
    "            ]\n",
    "            \n",
    "            if period_data.empty:\n",
    "                return None\n",
    "                \n",
    "            return period_data.tail(length)\n",
    "        \n",
    "        # Function to process and plot data for each condition\n",
    "        def plot_condition_data(ax, period_data, condition_name, const_type, trial_type, row, col):\n",
    "            # Set title and labels regardless of data availability\n",
    "            ax.set_title(f'{trial_type.upper()}: {condition_name}\\n(last 20 strides)', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(f'Change in {stride_col.replace(\"_\", \" \")}', fontsize=11)\n",
    "            ax.set_ylabel('Probability Density', fontsize=11)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            if period_data is None or period_data.empty:\n",
    "                ax.text(0.5, 0.5, f'No {condition_name.split()[0].lower()}\\ntarget data available', \n",
    "                       ha='center', va='center', transform=ax.transAxes,\n",
    "                       fontsize=12, style='italic', color='gray',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.5))\n",
    "                # Set reasonable axis limits for empty plots\n",
    "                ax.set_xlim(-2, 2)\n",
    "                ax.set_ylim(0, 1)\n",
    "                return None\n",
    "            \n",
    "            # Sort by stride number and calculate stride length changes\n",
    "            period_sorted = period_data.sort_values('Stride Number').copy()\n",
    "            period_sorted['Delta'] = period_sorted[stride_col].diff().shift(-1)\n",
    "            period_sorted = period_sorted[:-1]  # Remove last row\n",
    "            \n",
    "            # Separate changes after success vs failure\n",
    "            success_deltas = period_sorted[period_sorted['Success'] == 1]['Delta'].dropna()\n",
    "            failure_deltas = period_sorted[period_sorted['Success'] == 0]['Delta'].dropna()\n",
    "            \n",
    "            if len(success_deltas) == 0 and len(failure_deltas) == 0:\n",
    "                ax.text(0.5, 0.5, f'No valid stride changes\\nfor {condition_name.split()[0].lower()} target', \n",
    "                       ha='center', va='center', transform=ax.transAxes,\n",
    "                       fontsize=12, style='italic', color='gray',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.5))\n",
    "                ax.set_xlim(-2, 2)\n",
    "                ax.set_ylim(0, 1)\n",
    "                return None\n",
    "            \n",
    "            # Plot distributions using KDE or fallback methods\n",
    "            if len(success_deltas) > 0:\n",
    "                self._plot_distribution(ax, success_deltas, 'green', f'After Success (n={len(success_deltas)})', alpha)\n",
    "            \n",
    "            if len(failure_deltas) > 0:\n",
    "                self._plot_distribution(ax, failure_deltas, 'red', f'After Failure (n={len(failure_deltas)})', alpha)\n",
    "            \n",
    "            # Add mean lines and formatting\n",
    "            if len(success_deltas) > 0:\n",
    "                ax.axvline(success_deltas.mean(), color='darkgreen', linestyle='--', linewidth=2,\n",
    "                           label=f'Success Mean: {success_deltas.mean():.3f}')\n",
    "            \n",
    "            if len(failure_deltas) > 0:\n",
    "                ax.axvline(failure_deltas.mean(), color='darkred', linestyle='--', linewidth=2,\n",
    "                           label=f'Failure Mean: {failure_deltas.mean():.3f}')\n",
    "            \n",
    "            ax.axvline(0, color='black', linestyle='-', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Success rate annotation\n",
    "            if len(period_sorted) > 0:\n",
    "                success_rate = period_sorted['Success'].mean()\n",
    "                ax.text(0.02, 0.98, f'Success Rate: {success_rate:.1%}', \n",
    "                       transform=ax.transAxes, fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
    "                       verticalalignment='top', horizontalalignment='left')\n",
    "            \n",
    "            ax.legend(loc='upper right', fontsize=9)\n",
    "            \n",
    "            # Return statistics\n",
    "            return {\n",
    "                f'{const_type}_success_deltas': success_deltas.tolist() if len(success_deltas) > 0 else [],\n",
    "                f'{const_type}_failure_deltas': failure_deltas.tolist() if len(failure_deltas) > 0 else [],\n",
    "                f'{const_type}_success_mean': success_deltas.mean() if len(success_deltas) > 0 else None,\n",
    "                f'{const_type}_failure_mean': failure_deltas.mean() if len(failure_deltas) > 0 else None,\n",
    "                f'{const_type}_success_rate': period_sorted['Success'].mean(),\n",
    "                f'{const_type}_n_success': len(success_deltas),\n",
    "                f'{const_type}_n_failure': len(failure_deltas)\n",
    "            }\n",
    "        \n",
    "        # FIXED: Process all trial types in fixed positions\n",
    "        all_stats = {'subject_id': subject_id}\n",
    "        \n",
    "        for trial_type in trial_types:\n",
    "            # Get the fixed row and column positions for this trial type\n",
    "            if trial_type not in layout_mapping:\n",
    "                continue\n",
    "                \n",
    "            row_max, col_max, row_min, col_min = layout_mapping[trial_type]\n",
    "            \n",
    "            # Get trial data\n",
    "            trial_dict = self.data_manager.processed_data[subject_id]['trial_data'].get(trial_type)\n",
    "            if trial_dict is None:\n",
    "                df = None\n",
    "            else:\n",
    "                df = trial_dict.get('data')\n",
    "            \n",
    "            # Check required columns if data exists\n",
    "            if df is not None and not df.empty:\n",
    "                required_cols = [stride_col, 'Success', 'Stride Number', 'Target size', 'Constant']\n",
    "                missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    if show_stats:\n",
    "                        print(f\"Missing columns for {subject_id} {trial_type}: {missing_cols}\")\n",
    "                    df = None\n",
    "            \n",
    "            # Get data for both target conditions\n",
    "            max_const_data = get_period_data(df, 'max') if df is not None else None\n",
    "            min_const_data = get_period_data(df, 'min') if df is not None else None\n",
    "            \n",
    "            # Plot both conditions in their fixed positions\n",
    "            max_stats = plot_condition_data(\n",
    "                axes[row_max, col_max], max_const_data, condition_names[0], 'max', trial_type, row_max, col_max\n",
    "            )\n",
    "            min_stats = plot_condition_data(\n",
    "                axes[row_min, col_min], min_const_data, condition_names[1], 'min', trial_type, row_min, col_min\n",
    "            )\n",
    "            \n",
    "            # Combine statistics for this trial type\n",
    "            trial_stats = {}\n",
    "            if max_stats:\n",
    "                trial_stats.update(max_stats)\n",
    "            if min_stats:\n",
    "                trial_stats.update(min_stats)\n",
    "            \n",
    "            all_stats[trial_type] = trial_stats\n",
    "        \n",
    "        # FIXED: Add column headers at the top\n",
    "        for col, condition_name in enumerate(condition_names):\n",
    "            fig.text(0.25 + col * 0.5, 0.95, condition_name, ha='center', va='bottom', \n",
    "                    fontsize=14, fontweight='bold', transform=fig.transFigure)\n",
    "        \n",
    "        # Overall title\n",
    "        fig.suptitle(f'Stride Length Change Distributions\\n'\n",
    "                    f'Subject: {subject_id} (Age: {subject_age:.1f} years)', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])  # Leave space for title and headers\n",
    "        \n",
    "        # Save figure\n",
    "        if save:\n",
    "            filename = f\"stride_change_{subject_id}_fixed_grid.png\"\n",
    "            save_dir = self.individual_plots_dir / \"stride_change_after_success_vs_failure\"\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(save_dir / filename, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.close()  # Close to save memory\n",
    "        \n",
    "        return all_stats\n",
    "    \n",
    "    \n",
    "    # UPDATED: Modified plot_all_individual_stride_changes to use the fixed layout\n",
    "    def plot_all_individual_stride_changes(self, trial_types: List[str] = None, \n",
    "                                         subject_ids: List[str] = None,\n",
    "                                         save_summary: bool = True,\n",
    "                                         max_subjects: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate stride change distribution plots for all participants.\n",
    "        Each participant gets one plot with FIXED 3x2 grid layout (3 trials × 2 conditions).\n",
    "        \"\"\"\n",
    "        \n",
    "        if trial_types is None:\n",
    "            trial_types = ['vis1', 'invis', 'vis2']\n",
    "        \n",
    "        if subject_ids is None:\n",
    "            subject_ids = list(self.data_manager.processed_data.keys())\n",
    "        \n",
    "        # Limit subjects if requested\n",
    "        if max_subjects and len(subject_ids) > max_subjects:\n",
    "            subject_ids = subject_ids[:max_subjects]\n",
    "            print(f\"🔄 Limited to first {max_subjects} subjects for testing\")\n",
    "        \n",
    "        print(f\"🎯 Generating individual stride change plots with FIXED 3x2 grid...\")\n",
    "        print(f\"   📊 {len(subject_ids)} subjects\")\n",
    "        print(f\"   🎮 Trial types: {trial_types}\")\n",
    "        print(f\"   📐 Layout: 3 rows (trials) × 2 columns (conditions)\")\n",
    "        print(f\"   💾 Saving to: {self.individual_plots_dir}\")\n",
    "        \n",
    "        all_stats = {}\n",
    "        successful_plots = 0\n",
    "        failed_plots = 0\n",
    "        \n",
    "        # Progress bar for subjects\n",
    "        for subject_id in tqdm(subject_ids, desc=\"Processing subjects\"):\n",
    "            try:\n",
    "                # Get subject metadata\n",
    "                if subject_id in self.data_manager.processed_data:\n",
    "                    age = self.data_manager.processed_data[subject_id]['metadata'].get('age_months', np.nan) / 12\n",
    "                    subject_stats = {'age': age}\n",
    "                else:\n",
    "                    subject_stats = {}\n",
    "                \n",
    "                # Use the updated method with fixed 3x2 grid\n",
    "                stats = self._plot_individual_stride_change_internal(\n",
    "                    subject_id, trial_types, save=True, show_stats=False\n",
    "                )\n",
    "                \n",
    "                if stats:\n",
    "                    # Merge the subject stats with trial stats\n",
    "                    subject_stats.update(stats)\n",
    "                    all_stats[subject_id] = subject_stats\n",
    "                    successful_plots += 1\n",
    "                else:\n",
    "                    failed_plots += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error plotting {subject_id}: {str(e)}\")\n",
    "                failed_plots += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ Completed: {successful_plots} successful plots, {failed_plots} failed\")\n",
    "        print(f\"📐 All plots use consistent 3×2 grid layout\")\n",
    "        \n",
    "        # Save summary if requested\n",
    "        if save_summary:\n",
    "            self._save_stride_analysis_summary(all_stats)\n",
    "        \n",
    "        return all_stats\n",
    "    \n",
    "    \n",
    "    # HELPER: Updated _plot_distribution method to handle edge cases better\n",
    "    def _plot_distribution(self, ax, data, color, label, alpha):\n",
    "        \"\"\"Helper method to plot distributions with KDE or fallback.\"\"\"\n",
    "        try:\n",
    "            if len(data) >= 2:\n",
    "                kde = gaussian_kde(data)\n",
    "                x_min, x_max = data.min(), data.max()\n",
    "                x_range = x_max - x_min\n",
    "                if x_range > 0:\n",
    "                    x_min -= x_range * 0.1\n",
    "                    x_max += x_range * 0.1\n",
    "                else:\n",
    "                    x_min -= 0.1\n",
    "                    x_max += 0.1\n",
    "                \n",
    "                x_smooth = np.linspace(x_min, x_max, 200)\n",
    "                y_smooth = kde(x_smooth)\n",
    "                \n",
    "                ax.plot(x_smooth, y_smooth, color=color, linewidth=3, alpha=0.8, label=label)\n",
    "                ax.fill_between(x_smooth, y_smooth, alpha=alpha*0.5, color=color)\n",
    "            else:\n",
    "                for i, val in enumerate(data):\n",
    "                    ax.axvline(val, color=color, alpha=0.7, linewidth=2,\n",
    "                              label=label if i == 0 else \"\")\n",
    "        except Exception:\n",
    "            # Fallback to histogram\n",
    "            if len(data) >= 2:\n",
    "                ax.hist(data, bins=min(10, len(data)), alpha=alpha, color=color, \n",
    "                        label=label, density=True, edgecolor='black', linewidth=0.5)\n",
    "            else:\n",
    "                for i, val in enumerate(data):\n",
    "                    ax.axvline(val, color=color, alpha=0.7, linewidth=3,\n",
    "                              label=label if i == 0 else \"\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b65ce6-705a-4a3a-8bf2-1dd1aaeacfea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 8. ANALYSIS PIPELINE\n",
    "# ==============================================================================\n",
    "# ORIGINAL ANALYSIS CLASS (RESTORED)\n",
    "# ==================================\n",
    "# This version uses the fixed StandaloneEnhancedVisualizer\n",
    "\n",
    "class MotorLearningAnalysis:\n",
    "    \"\"\"Updated to use centralized config and new visualizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_manager, metrics_df: pd.DataFrame):\n",
    "        self.data_manager = data_manager\n",
    "        self.metrics_df = metrics_df\n",
    "        \n",
    "        # KEY CHANGE: Get config from data_manager\n",
    "        self.config = data_manager.config\n",
    "        \n",
    "        # KEY CHANGE: Pass config to analyzer\n",
    "        self.analyzer = StatisticalAnalyzer(metrics_df, self.config)\n",
    "        \n",
    "        # KEY CHANGE: Use StandaloneEnhancedVisualizer with config\n",
    "        self.visualizer = StandaloneEnhancedVisualizer(self)\n",
    "    \n",
    "    def run_comprehensive_analysis(self, save_all: bool = True) -> Dict:\n",
    "        \"\"\"Updated to use config for saving.\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(\"🔬 Running Comprehensive Motor Learning Analysis...\")\n",
    "        print(f\"📁 Outputs will be saved to: {self.config.BASE_OUTPUT_DIR}\")\n",
    "        \n",
    "        # Print age summary (unchanged)\n",
    "        if 'age' in self.metrics_df.columns:\n",
    "            ages = self.metrics_df['age'].dropna()\n",
    "            print(f\"📊 Age range: {ages.min():.1f} - {ages.max():.1f} years (n={len(ages)})\")\n",
    "        \n",
    "        # 1. Regression Analysis (unchanged logic, but could use config thresholds)\n",
    "        print(\"📊 Running regression analysis...\")\n",
    "        try:\n",
    "            regression_results = self.analyzer.run_regression_analysis()\n",
    "            results['regression'] = regression_results\n",
    "            print(f\"✅ Regression R² = {regression_results['metrics']['r2']:.3f}\")\n",
    "            \n",
    "            for feature, importance in regression_results['feature_importances'].items():\n",
    "                print(f\"   {feature}: {importance:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Regression analysis failed: {e}\")\n",
    "        \n",
    "        # 2. Classification Analysis (unchanged)\n",
    "        print(\"🎯 Running classification analysis...\")\n",
    "        try:\n",
    "            classification_results = self.analyzer.run_classification_analysis()\n",
    "            results['classification'] = classification_results\n",
    "            print(f\"✅ Classification AUC = {classification_results['metrics']['roc_auc']:.3f}\")\n",
    "            print(f\"   Accuracy = {classification_results['metrics']['accuracy']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Classification analysis failed: {e}\")\n",
    "        \n",
    "        # 3. Mixed Effects Analysis (unchanged)\n",
    "        print(\"📈 Running mixed-effects analysis...\")\n",
    "        try:\n",
    "            mixed_model = self.analyzer.run_mixed_effects_analysis()\n",
    "            results['mixed_effects'] = mixed_model\n",
    "            print(f\"✅ Mixed-effects R² = {mixed_model.rsquared:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Mixed-effects analysis failed: {e}\")\n",
    "        \n",
    "        # 4. Enhanced Visualizations - KEY CHANGE: Uses config-managed paths\n",
    "        print(\"📊 Generating enhanced visualizations...\")\n",
    "        try:\n",
    "            # The visualizer now automatically saves to config-managed directories\n",
    "            self.visualizer.plot_population_analyses(save=save_all)\n",
    "            print(\"✅ Enhanced visualizations complete\")\n",
    "            print(f\"📁 Plots saved to: {self.config.POPULATION_PLOTS_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Visualization failed: {e}\")\n",
    "        \n",
    "        print(\"🎉 Comprehensive analysis complete!\")\n",
    "        print(f\"📁 All outputs in: {self.config.BASE_OUTPUT_DIR}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3cf4e-a145-4a1d-949a-7e8c019569ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTEGRATED MOTOR LEARNING PIPELINE\n",
    "# ==============================================================================\n",
    "# Works directly with your existing analysis object from the notebook\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from scipy.stats import gaussian_kde, pearsonr, mannwhitneyu, ttest_ind, f_oneway\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class IntegratedMotorLearningPipeline:\n",
    "    \"\"\"\n",
    "    Integrated pipeline that works with your existing analysis object.\n",
    "    \n",
    "    This version doesn't try to import external modules but instead\n",
    "    uses your existing analysis, data_manager, and metrics_df objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, analysis_instance, output_dir: str = 'motor_learning_analysis'):\n",
    "        \"\"\"\n",
    "        Initialize with your existing analysis instance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        analysis_instance : Your existing analysis object\n",
    "            Should have .data_manager, .metrics_df, .visualizer attributes\n",
    "        output_dir : str\n",
    "            Directory to save all outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        self.analysis = analysis_instance\n",
    "        self.data_manager = analysis_instance.data_manager\n",
    "        self.metrics_df = analysis_instance.metrics_df\n",
    "        self.output_dir = Path(output_dir)\n",
    "        \n",
    "        # Create output directory structure\n",
    "        self._setup_output_directories()\n",
    "        \n",
    "        # Set configuration\n",
    "        self.config = self._get_default_config()\n",
    "        \n",
    "        # Initialize containers\n",
    "        self.analysis_results = {}\n",
    "        self.quality_report = {}\n",
    "        \n",
    "        # Analysis timestamp\n",
    "        self.analysis_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(f\"🚀 Integrated Motor Learning Pipeline initialized\")\n",
    "        print(f\"📊 Data: {len(self.metrics_df)} subjects with {len(self.metrics_df.columns)} metrics\")\n",
    "        print(f\"📁 Output directory: {self.output_dir}\")\n",
    "\n",
    "    def _setup_output_directories(self):\n",
    "        \"\"\"Create organized output directory structure.\"\"\"\n",
    "        \n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        self.dirs = {\n",
    "            'figures': self.output_dir / 'figures',\n",
    "            'individual_plots': self.output_dir / 'figures' / 'individual_plots',\n",
    "            'population_plots': self.output_dir / 'figures' / 'population_plots',\n",
    "            'statistical_plots': self.output_dir / 'figures' / 'statistical_plots',\n",
    "            'reports': self.output_dir / 'reports',\n",
    "            'exports': self.output_dir / 'exports'\n",
    "        }\n",
    "        \n",
    "        for dir_path in self.dirs.values():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _get_default_config(self) -> Dict:\n",
    "        \"\"\"Get default configuration parameters.\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'motor_noise_threshold': 0.3,\n",
    "            'success_rate_threshold': 0.68,\n",
    "            'figure_dpi': 300,\n",
    "            'alpha_level': 0.05,\n",
    "            'age_bins': [7, 10, 13, 16, 18],\n",
    "            'age_labels': ['7-10', '10-13', '13-16', '16-18'],\n",
    "            'max_individual_subjects': None  # None = all subjects\n",
    "        }\n",
    "\n",
    "    def run_complete_analysis(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Run the complete integrated analysis pipeline.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict : Complete analysis results\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"🔄 Starting integrated motor learning analysis pipeline\")\n",
    "        print(f\"⏰ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        results = {\n",
    "            'timestamp': self.analysis_timestamp,\n",
    "            'steps_completed': [],\n",
    "            'step_results': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Quality control analysis\n",
    "            print(\"\\n🔍 STEP 1: QUALITY CONTROL ANALYSIS\")\n",
    "            print(\"-\" * 50)\n",
    "            step_result = self._step_quality_control()\n",
    "            results['step_results']['quality_control'] = step_result\n",
    "            results['steps_completed'].append('quality_control')\n",
    "            \n",
    "            # Step 2: Statistical analysis\n",
    "            print(\"\\n📊 STEP 2: STATISTICAL ANALYSIS\")\n",
    "            print(\"-\" * 50)\n",
    "            step_result = self._step_statistical_analysis()\n",
    "            results['step_results']['statistical_analysis'] = step_result\n",
    "            results['steps_completed'].append('statistical_analysis')\n",
    "            \n",
    "            # Step 3: Generate visualizations\n",
    "            print(\"\\n📈 STEP 3: GENERATING VISUALIZATIONS\")\n",
    "            print(\"-\" * 50)\n",
    "            step_result = self._step_generate_plots()\n",
    "            results['step_results']['generate_plots'] = step_result\n",
    "            results['steps_completed'].append('generate_plots')\n",
    "            \n",
    "            # Step 4: Export results\n",
    "            print(\"\\n💾 STEP 4: EXPORTING RESULTS\")\n",
    "            print(\"-\" * 50)\n",
    "            step_result = self._step_export_results()\n",
    "            results['step_results']['export_results'] = step_result\n",
    "            results['steps_completed'].append('export_results')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Pipeline failed: {e}\")\n",
    "            results['error'] = str(e)\n",
    "            raise\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🎉 INTEGRATED PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"⏰ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"✅ Steps completed: {len(results['steps_completed'])}/4\")\n",
    "        print(f\"📁 Results saved to: {self.output_dir}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _step_quality_control(self) -> Dict:\n",
    "        \"\"\"Step 1: Perform quality control analysis.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            df = self.metrics_df\n",
    "            \n",
    "            # Apply motor noise filter\n",
    "            if 'mot_noise' in df.columns:\n",
    "                before_filter = len(df)\n",
    "                filtered_df = df[df['mot_noise'] <= self.config['motor_noise_threshold']]\n",
    "                after_filter = len(filtered_df)\n",
    "                \n",
    "                print(f\"🔍 Motor noise filter (≤ {self.config['motor_noise_threshold']}):\")\n",
    "                print(f\"   Before: {before_filter} subjects\")\n",
    "                print(f\"   After: {after_filter} subjects\") \n",
    "                print(f\"   Excluded: {before_filter - after_filter} subjects ({100*(before_filter-after_filter)/before_filter:.1f}%)\")\n",
    "                \n",
    "                self.quality_report['motor_noise_filter'] = {\n",
    "                    'threshold': self.config['motor_noise_threshold'],\n",
    "                    'before': before_filter,\n",
    "                    'after': after_filter,\n",
    "                    'excluded': before_filter - after_filter,\n",
    "                    'exclusion_rate': (before_filter - after_filter) / before_filter\n",
    "                }\n",
    "            else:\n",
    "                filtered_df = df\n",
    "                print(\"⚠️ No motor noise data available for filtering\")\n",
    "            \n",
    "            # Analyze data completeness\n",
    "            completeness = {}\n",
    "            key_columns = ['age'] + [col for col in filtered_df.columns if '_sr_' in col or '_sd_' in col]\n",
    "            \n",
    "            print(f\"\\n📊 Data completeness analysis:\")\n",
    "            for col in key_columns:\n",
    "                if col in filtered_df.columns:\n",
    "                    complete_count = filtered_df[col].notna().sum()\n",
    "                    completeness[col] = {\n",
    "                        'complete': int(complete_count),\n",
    "                        'missing': int(len(filtered_df) - complete_count),\n",
    "                        'completeness_rate': float(complete_count / len(filtered_df))\n",
    "                    }\n",
    "                    print(f\"   {col}: {complete_count}/{len(filtered_df)} ({100*complete_count/len(filtered_df):.1f}%)\")\n",
    "            \n",
    "            # Age distribution analysis\n",
    "            if 'age' in filtered_df.columns:\n",
    "                age_stats = filtered_df['age'].describe()\n",
    "                print(f\"\\n👥 Age distribution:\")\n",
    "                print(f\"   Range: {age_stats['min']:.1f} - {age_stats['max']:.1f} years\")\n",
    "                print(f\"   Mean ± SD: {age_stats['mean']:.1f} ± {age_stats['std']:.1f} years\")\n",
    "                print(f\"   Median: {age_stats['50%']:.1f} years\")\n",
    "            \n",
    "            # Success rate distribution analysis\n",
    "            sr_cols = [col for col in filtered_df.columns if '_sr_' in col]\n",
    "            if sr_cols:\n",
    "                print(f\"\\n🎯 Success rate summary:\")\n",
    "                for col in sr_cols[:5]:  # Show first 5\n",
    "                    sr_data = filtered_df[col].dropna()\n",
    "                    if len(sr_data) > 0:\n",
    "                        print(f\"   {col}: {sr_data.mean():.3f} ± {sr_data.std():.3f} (n={len(sr_data)})\")\n",
    "            \n",
    "            self.quality_report.update({\n",
    "                'data_completeness': completeness,\n",
    "                'final_sample_size': len(filtered_df),\n",
    "                'age_distribution': age_stats.to_dict() if 'age' in filtered_df.columns else None\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'filtered_sample_size': len(filtered_df),\n",
    "                'quality_report': self.quality_report\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Quality control failed: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "\n",
    "    def _step_statistical_analysis(self) -> Dict:\n",
    "        \"\"\"Step 2: Perform comprehensive statistical analysis.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Use filtered data\n",
    "            if 'mot_noise' in self.metrics_df.columns:\n",
    "                analysis_df = self.metrics_df[self.metrics_df['mot_noise'] <= self.config['motor_noise_threshold']]\n",
    "            else:\n",
    "                analysis_df = self.metrics_df\n",
    "            \n",
    "            print(f\"📊 Running statistical analysis on {len(analysis_df)} subjects...\")\n",
    "            \n",
    "            # 1. Correlation analysis\n",
    "            print(\"\\n🔗 Correlation Analysis:\")\n",
    "            correlation_results = self._analyze_correlations(analysis_df)\n",
    "            \n",
    "            # 2. Age effects analysis\n",
    "            print(\"\\n👥 Age Effects Analysis:\")\n",
    "            age_effects = self._analyze_age_effects(analysis_df)\n",
    "            \n",
    "            # 3. Trial type comparisons\n",
    "            print(\"\\n🎮 Trial Type Comparisons:\")\n",
    "            trial_comparisons = self._analyze_trial_comparisons(analysis_df)\n",
    "            \n",
    "            # 4. Individual differences analysis\n",
    "            print(\"\\n🧠 Individual Differences:\")\n",
    "            individual_differences = self._analyze_individual_differences(analysis_df)\n",
    "            \n",
    "            self.analysis_results = {\n",
    "                'correlations': correlation_results,\n",
    "                'age_effects': age_effects,\n",
    "                'trial_comparisons': trial_comparisons,\n",
    "                'individual_differences': individual_differences\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'n_analyses': 4,\n",
    "                'summary': self._summarize_statistical_results()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Statistical analysis failed: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "\n",
    "    def _step_generate_plots(self) -> Dict:\n",
    "        \"\"\"Step 3: Generate comprehensive visualizations.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Use filtered data\n",
    "            if 'mot_noise' in self.metrics_df.columns:\n",
    "                plot_df = self.metrics_df[self.metrics_df['mot_noise'] <= self.config['motor_noise_threshold']]\n",
    "            else:\n",
    "                plot_df = self.metrics_df\n",
    "            \n",
    "            print(f\"📈 Generating visualizations for {len(plot_df)} subjects...\")\n",
    "            \n",
    "            plot_counts = {\n",
    "                'individual': 0,\n",
    "                'population': 0,\n",
    "                'statistical': 0\n",
    "            }\n",
    "            \n",
    "            # 1. Individual stride change plots\n",
    "            print(\"\\n🎯 Individual Stride Change Plots:\")\n",
    "            individual_result = self._generate_individual_plots()\n",
    "            plot_counts['individual'] = individual_result.get('n_plots', 0)\n",
    "            \n",
    "            # 2. Population-level plots\n",
    "            print(\"\\n👥 Population-Level Analysis Plots:\")\n",
    "            population_result = self._generate_population_plots(plot_df)\n",
    "            plot_counts['population'] = population_result.get('n_plots', 0)\n",
    "            \n",
    "            # 3. Statistical analysis plots\n",
    "            print(\"\\n📊 Statistical Analysis Plots:\")\n",
    "            statistical_result = self._generate_statistical_plots(plot_df)\n",
    "            plot_counts['statistical'] = statistical_result.get('n_plots', 0)\n",
    "            \n",
    "            # 4. Summary dashboard\n",
    "            print(\"\\n📋 Summary Dashboard:\")\n",
    "            dashboard_result = self._generate_summary_dashboard(plot_df)\n",
    "            \n",
    "            total_plots = sum(plot_counts.values())\n",
    "            print(f\"\\n✅ Generated {total_plots} plots total\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'plot_counts': plot_counts,\n",
    "                'total_plots': total_plots\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Plot generation failed: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "\n",
    "    def _step_export_results(self) -> Dict:\n",
    "        \"\"\"Step 4: Export results and generate reports.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            exported_files = []\n",
    "            \n",
    "            # 1. Export metrics CSV\n",
    "            metrics_file = self.dirs['exports'] / f'metrics_{self.analysis_timestamp}.csv'\n",
    "            self.metrics_df.to_csv(metrics_file, index=False)\n",
    "            exported_files.append(str(metrics_file))\n",
    "            print(f\"📊 Metrics CSV: {metrics_file.name}\")\n",
    "            \n",
    "            # 2. Export statistical results\n",
    "            if self.analysis_results:\n",
    "                stats_file = self.dirs['exports'] / f'statistical_results_{self.analysis_timestamp}.json'\n",
    "                with open(stats_file, 'w') as f:\n",
    "                    json.dump(self._make_json_serializable(self.analysis_results), f, indent=2)\n",
    "                exported_files.append(str(stats_file))\n",
    "                print(f\"📈 Statistical results: {stats_file.name}\")\n",
    "            \n",
    "            # 3. Generate HTML report\n",
    "            report_file = self._generate_html_report()\n",
    "            exported_files.append(str(report_file))\n",
    "            print(f\"📄 HTML report: {report_file.name}\")\n",
    "            \n",
    "            # 4. Export quality report\n",
    "            quality_file = self.dirs['reports'] / f'quality_report_{self.analysis_timestamp}.json'\n",
    "            with open(quality_file, 'w') as f:\n",
    "                json.dump(self._make_json_serializable(self.quality_report), f, indent=2)\n",
    "            exported_files.append(str(quality_file))\n",
    "            print(f\"🔍 Quality report: {quality_file.name}\")\n",
    "            \n",
    "            print(f\"\\n✅ Exported {len(exported_files)} files\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'exported_files': exported_files,\n",
    "                'n_files': len(exported_files)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Export failed: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "\n",
    "    # Statistical Analysis Methods\n",
    "    # ============================\n",
    "    \n",
    "    def _analyze_correlations(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze correlations between key variables.\"\"\"\n",
    "        \n",
    "        # Select key variables\n",
    "        key_vars = ['age']\n",
    "        if 'mot_noise' in df.columns:\n",
    "            key_vars.append('mot_noise')\n",
    "        if 'pref_asymmetry' in df.columns:\n",
    "            key_vars.append('pref_asymmetry')\n",
    "        \n",
    "        # Add success rate variables\n",
    "        sr_vars = [col for col in df.columns if '_sr_' in col and '_const' in col]\n",
    "        key_vars.extend(sr_vars[:6])  # Limit to prevent overcrowding\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_df = df[key_vars].corr()\n",
    "        \n",
    "        # Find strongest correlations\n",
    "        strong_corr = []\n",
    "        for i in range(len(corr_df.columns)):\n",
    "            for j in range(i+1, len(corr_df.columns)):\n",
    "                var1, var2 = corr_df.columns[i], corr_df.columns[j]\n",
    "                corr_val = corr_df.iloc[i, j]\n",
    "                if abs(corr_val) > 0.3:  # Threshold for \"strong\" correlation\n",
    "                    strong_corr.append({\n",
    "                        'var1': var1,\n",
    "                        'var2': var2,\n",
    "                        'correlation': float(corr_val),\n",
    "                        'strength': 'strong' if abs(corr_val) > 0.5 else 'moderate'\n",
    "                    })\n",
    "        \n",
    "        print(f\"   Found {len(strong_corr)} significant correlations (|r| > 0.3)\")\n",
    "        for corr in strong_corr[:5]:  # Show top 5\n",
    "            print(f\"   {corr['var1']} ↔ {corr['var2']}: r = {corr['correlation']:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'correlation_matrix': corr_df.to_dict(),\n",
    "            'strong_correlations': strong_corr,\n",
    "            'variables_analyzed': key_vars\n",
    "        }\n",
    "\n",
    "    def _analyze_age_effects(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze effects of age on performance measures.\"\"\"\n",
    "        \n",
    "        if 'age' not in df.columns:\n",
    "            return {'error': 'Age data not available'}\n",
    "        \n",
    "        age_effects = []\n",
    "        sr_cols = [col for col in df.columns if '_sr_' in col and '_const' in col]\n",
    "        \n",
    "        for col in sr_cols:\n",
    "            valid_data = df[['age', col]].dropna()\n",
    "            if len(valid_data) > 10:\n",
    "                r, p = pearsonr(valid_data['age'], valid_data[col])\n",
    "                age_effects.append({\n",
    "                    'measure': col,\n",
    "                    'correlation': float(r),\n",
    "                    'p_value': float(p),\n",
    "                    'significant': p < self.config['alpha_level'],\n",
    "                    'n_subjects': len(valid_data)\n",
    "                })\n",
    "        \n",
    "        # Sort by correlation strength\n",
    "        age_effects.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "        \n",
    "        print(f\"   Analyzed age effects for {len(age_effects)} measures\")\n",
    "        significant_effects = [e for e in age_effects if e['significant']]\n",
    "        print(f\"   Found {len(significant_effects)} significant age effects\")\n",
    "        \n",
    "        return {\n",
    "            'age_effects': age_effects,\n",
    "            'n_significant': len(significant_effects),\n",
    "            'strongest_effect': age_effects[0] if age_effects else None\n",
    "        }\n",
    "\n",
    "    def _analyze_trial_comparisons(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Compare performance across different trial types.\"\"\"\n",
    "        \n",
    "        comparisons = []\n",
    "        \n",
    "        # Compare vis1 vs invis vs vis2 for each condition\n",
    "        for condition in ['max_const', 'min_const']:\n",
    "            trial_data = {}\n",
    "            for trial in ['vis1', 'invis', 'vis2']:\n",
    "                col = f'{trial}_sr_{condition}'\n",
    "                if col in df.columns:\n",
    "                    trial_data[trial] = df[col].dropna().tolist()\n",
    "            \n",
    "            if len(trial_data) >= 2:\n",
    "                # Perform ANOVA if we have multiple trials\n",
    "                trial_values = list(trial_data.values())\n",
    "                trial_names = list(trial_data.keys())\n",
    "                \n",
    "                try:\n",
    "                    f_stat, p_val = f_oneway(*trial_values)\n",
    "                    comparisons.append({\n",
    "                        'condition': condition,\n",
    "                        'trials_compared': trial_names,\n",
    "                        'f_statistic': float(f_stat),\n",
    "                        'p_value': float(p_val),\n",
    "                        'significant': p_val < self.config['alpha_level'],\n",
    "                        'n_subjects': {name: len(data) for name, data in trial_data.items()}\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"   Performed {len(comparisons)} trial comparisons\")\n",
    "        significant_comparisons = [c for c in comparisons if c['significant']]\n",
    "        print(f\"   Found {len(significant_comparisons)} significant differences\")\n",
    "        \n",
    "        return {\n",
    "            'comparisons': comparisons,\n",
    "            'n_significant': len(significant_comparisons)\n",
    "        }\n",
    "\n",
    "    def _analyze_individual_differences(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze individual differences in motor learning patterns.\"\"\"\n",
    "        \n",
    "        individual_analysis = {}\n",
    "        \n",
    "        # 1. Motor noise distribution\n",
    "        if 'mot_noise' in df.columns:\n",
    "            noise_stats = df['mot_noise'].describe()\n",
    "            individual_analysis['motor_noise'] = {\n",
    "                'descriptive_stats': noise_stats.to_dict(),\n",
    "                'high_noise_subjects': int((df['mot_noise'] > self.config['motor_noise_threshold']).sum())\n",
    "            }\n",
    "        \n",
    "        # 2. Success rate variability\n",
    "        sr_cols = [col for col in df.columns if '_sr_' in col and '_const' in col]\n",
    "        if sr_cols:\n",
    "            # Calculate average success rate per subject\n",
    "            df_copy = df.copy()\n",
    "            df_copy['avg_success_rate'] = df_copy[sr_cols].mean(axis=1)\n",
    "            sr_variability = df_copy['avg_success_rate'].std()\n",
    "            \n",
    "            individual_analysis['success_rate_variability'] = {\n",
    "                'std_dev': float(sr_variability),\n",
    "                'range': float(df_copy['avg_success_rate'].max() - df_copy['avg_success_rate'].min()),\n",
    "                'high_performers': int((df_copy['avg_success_rate'] > 0.7).sum()),\n",
    "                'low_performers': int((df_copy['avg_success_rate'] < 0.3).sum())\n",
    "            }\n",
    "        \n",
    "        print(f\"   Analyzed individual differences across {len(df)} subjects\")\n",
    "        if 'motor_noise' in individual_analysis:\n",
    "            print(f\"   Motor noise range: {individual_analysis['motor_noise']['descriptive_stats']['min']:.3f} - {individual_analysis['motor_noise']['descriptive_stats']['max']:.3f}\")\n",
    "        \n",
    "        return individual_analysis\n",
    "\n",
    "    # Plotting Methods\n",
    "    # ================\n",
    "    \n",
    "    def _generate_individual_plots(self) -> Dict:\n",
    "        \"\"\"Generate individual stride change plots using the standalone enhanced visualizer.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Import and use the existing StandaloneEnhancedVisualizer\n",
    "            # Note: This assumes the StandaloneEnhancedVisualizer code is available in the notebook\n",
    "            enhanced_viz = StandaloneEnhancedVisualizer(self.analysis)\n",
    "            \n",
    "            # Get subject list\n",
    "            subject_ids = list(self.data_manager.processed_data.keys())\n",
    "            max_subjects = self.config.get('max_individual_subjects')\n",
    "            \n",
    "            if max_subjects and len(subject_ids) > max_subjects:\n",
    "                subject_ids = subject_ids[:max_subjects]\n",
    "                print(f\"   Limited to {max_subjects} subjects\")\n",
    "            \n",
    "            # Generate plots using the enhanced visualizer\n",
    "            stride_stats = enhanced_viz.plot_all_individual_stride_changes(\n",
    "                trial_types=['vis1', 'invis', 'vis2'],\n",
    "                subject_ids=subject_ids,\n",
    "                save_summary=True,\n",
    "                max_subjects=max_subjects\n",
    "            )\n",
    "            \n",
    "            # Count successful plots\n",
    "            n_plots = 0\n",
    "            for subject_data in stride_stats.values():\n",
    "                for key in subject_data.keys():\n",
    "                    if key != 'age':  # Skip the age field\n",
    "                        n_plots += 1\n",
    "            \n",
    "            print(f\"   Generated {n_plots} individual stride change plots\")\n",
    "            \n",
    "            return {\n",
    "                'n_plots': n_plots,\n",
    "                'n_subjects': len(stride_stats),\n",
    "                'output_dir': str(enhanced_viz.individual_plots_dir),\n",
    "                'stride_stats': stride_stats\n",
    "            }\n",
    "            \n",
    "        except NameError:\n",
    "            print(\"   ⚠️ StandaloneEnhancedVisualizer not found\")\n",
    "            print(\"   💡 Make sure you've run the StandaloneEnhancedVisualizer code block first\")\n",
    "            return {'n_plots': 0, 'error': 'StandaloneEnhancedVisualizer not available'}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Individual plots failed: {e}\")\n",
    "            return {'n_plots': 0, 'error': str(e)}\n",
    "\n",
    "    def _generate_population_plots(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Generate population-level analysis plots.\"\"\"\n",
    "        \n",
    "        plot_count = 0\n",
    "        \n",
    "        try:\n",
    "            # 1. Age vs Motor Noise\n",
    "            if 'age' in df.columns and 'mot_noise' in df.columns:\n",
    "                self._plot_age_vs_motor_noise(df)\n",
    "                plot_count += 1\n",
    "            \n",
    "            # 2. Age vs Success Rates\n",
    "            sr_cols = [col for col in df.columns if '_sr_' in col and '_const' in col]\n",
    "            if 'age' in df.columns and sr_cols:\n",
    "                self._plot_age_vs_success_rates(df, sr_cols)\n",
    "                plot_count += 1\n",
    "            \n",
    "            # 3. Correlation Matrix\n",
    "            self._plot_correlation_matrix(df)\n",
    "            plot_count += 1\n",
    "            \n",
    "            # 4. Success Rate Distributions\n",
    "            if sr_cols:\n",
    "                self._plot_success_rate_distributions(df, sr_cols)\n",
    "                plot_count += 1\n",
    "            \n",
    "            print(f\"   Generated {plot_count} population-level plots\")\n",
    "            \n",
    "            return {\n",
    "                'n_plots': plot_count,\n",
    "                'output_dir': str(self.dirs['population_plots'])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Population plots failed: {e}\")\n",
    "            return {'n_plots': 0, 'error': str(e)}\n",
    "\n",
    "    def _generate_statistical_plots(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Generate statistical analysis visualization plots.\"\"\"\n",
    "        \n",
    "        plot_count = 0\n",
    "        \n",
    "        try:\n",
    "            # 1. Age Effects Plot\n",
    "            if self.analysis_results.get('age_effects'):\n",
    "                self._plot_age_effects_summary()\n",
    "                plot_count += 1\n",
    "            \n",
    "            # 2. Correlation Network Plot\n",
    "            if self.analysis_results.get('correlations'):\n",
    "                self._plot_correlation_network()\n",
    "                plot_count += 1\n",
    "            \n",
    "            print(f\"   Generated {plot_count} statistical analysis plots\")\n",
    "            \n",
    "            return {\n",
    "                'n_plots': plot_count,\n",
    "                'output_dir': str(self.dirs['statistical_plots'])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Statistical plots failed: {e}\")\n",
    "            return {'n_plots': 0, 'error': str(e)}\n",
    "\n",
    "    def _generate_summary_dashboard(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Generate comprehensive summary dashboard.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            fig = plt.figure(figsize=(20, 16))\n",
    "            gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            # 1. Sample characteristics\n",
    "            ax1 = fig.add_subplot(gs[0, 0])\n",
    "            if 'age' in df.columns:\n",
    "                df['age'].hist(bins=range(7, 18), ax=ax1, alpha=0.7, color='skyblue')\n",
    "                ax1.set_title('Age Distribution')\n",
    "                ax1.set_xlabel('Age (years)')\n",
    "                ax1.set_ylabel('Count')\n",
    "            \n",
    "            # 2. Motor noise distribution\n",
    "            ax2 = fig.add_subplot(gs[0, 1])\n",
    "            if 'mot_noise' in df.columns:\n",
    "                df['mot_noise'].hist(bins=15, ax=ax2, alpha=0.7, color='lightcoral')\n",
    "                ax2.axvline(self.config['motor_noise_threshold'], color='red', linestyle='--', \n",
    "                           label=f\"Threshold ({self.config['motor_noise_threshold']})\")\n",
    "                ax2.set_title('Motor Noise Distribution')\n",
    "                ax2.set_xlabel('Motor Noise')\n",
    "                ax2.set_ylabel('Count')\n",
    "                ax2.legend()\n",
    "            \n",
    "            # 3. Success rates overview\n",
    "            ax3 = fig.add_subplot(gs[0, 2:])\n",
    "            sr_cols = [col for col in df.columns if '_sr_' in col and '_const' in col]\n",
    "            if sr_cols:\n",
    "                sr_means = [df[col].mean() for col in sr_cols[:6]]\n",
    "                sr_names = [col.replace('_sr_', ' ').replace('_const', '') for col in sr_cols[:6]]\n",
    "                \n",
    "                bars = ax3.bar(sr_names, sr_means, alpha=0.7)\n",
    "                ax3.set_title('Mean Success Rates by Condition')\n",
    "                ax3.set_ylabel('Success Rate')\n",
    "                ax3.set_ylim(0, 1)\n",
    "                plt.setp(ax3.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # 4. Key correlations heatmap\n",
    "            ax4 = fig.add_subplot(gs[1, :2])\n",
    "            key_cols = ['age']\n",
    "            if 'mot_noise' in df.columns:\n",
    "                key_cols.append('mot_noise')\n",
    "            key_cols.extend(sr_cols[:4] if sr_cols else [])\n",
    "            \n",
    "            if len(key_cols) > 1:\n",
    "                corr_matrix = df[key_cols].corr()\n",
    "                sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, ax=ax4, fmt='.2f')\n",
    "                ax4.set_title('Key Variable Correlations')\n",
    "            \n",
    "            # 5. Age vs Performance scatter\n",
    "            ax5 = fig.add_subplot(gs[1, 2:])\n",
    "            if 'age' in df.columns and sr_cols:\n",
    "                target_col = sr_cols[0]  # Use first available success rate\n",
    "                valid_data = df[['age', target_col]].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    ax5.scatter(valid_data['age'], valid_data[target_col], alpha=0.6, s=50)\n",
    "                    \n",
    "                    # Add trend line\n",
    "                    z = np.polyfit(valid_data['age'], valid_data[target_col], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    ax5.plot(valid_data['age'], p(valid_data['age']), \"r--\", alpha=0.8)\n",
    "                    \n",
    "                    # Add correlation\n",
    "                    r, p_val = pearsonr(valid_data['age'], valid_data[target_col])\n",
    "                    ax5.text(0.05, 0.95, f'r = {r:.3f}\\np = {p_val:.3f}', \n",
    "                            transform=ax5.transAxes, bbox=dict(boxstyle='round', facecolor='white'))\n",
    "                    \n",
    "                    ax5.set_xlabel('Age (years)')\n",
    "                    ax5.set_ylabel('Success Rate')\n",
    "                    ax5.set_title(f'Age vs Performance ({target_col})')\n",
    "                    ax5.set_ylim(0, 1)\n",
    "            \n",
    "            # 6. Summary statistics table\n",
    "            ax6 = fig.add_subplot(gs[2:, :])\n",
    "            ax6.axis('tight')\n",
    "            ax6.axis('off')\n",
    "            \n",
    "            # Create summary table\n",
    "            summary_data = [\n",
    "                ['Total Subjects', f\"{len(df)}\"],\n",
    "                ['Age Range', f\"{df['age'].min():.1f} - {df['age'].max():.1f} years\" if 'age' in df.columns else \"N/A\"],\n",
    "                ['Mean Age ± SD', f\"{df['age'].mean():.1f} ± {df['age'].std():.1f} years\" if 'age' in df.columns else \"N/A\"],\n",
    "                ['Motor Noise Range', f\"{df['mot_noise'].min():.3f} - {df['mot_noise'].max():.3f}\" if 'mot_noise' in df.columns else \"N/A\"],\n",
    "                ['Success Rate Measures', f\"{len(sr_cols)}\"],\n",
    "                ['High Motor Noise Subjects', f\"{(df['mot_noise'] > self.config['motor_noise_threshold']).sum()}\" if 'mot_noise' in df.columns else \"N/A\"],\n",
    "                ['Analysis Timestamp', self.analysis_timestamp]\n",
    "            ]\n",
    "            \n",
    "            table = ax6.table(cellText=summary_data, colLabels=['Metric', 'Value'],\n",
    "                             cellLoc='left', loc='center')\n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(12)\n",
    "            table.scale(1.2, 2)\n",
    "            ax6.set_title('Analysis Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "            \n",
    "            plt.suptitle('Motor Learning Analysis Dashboard', fontsize=20, y=0.95)\n",
    "            \n",
    "            # Save dashboard\n",
    "            dashboard_file = self.dirs['reports'] / f'analysis_dashboard_{self.analysis_timestamp}.png'\n",
    "            plt.savefig(dashboard_file, dpi=self.config['figure_dpi'], bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"   Generated summary dashboard: {dashboard_file.name}\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'file': str(dashboard_file)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Dashboard generation failed: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "\n",
    "    # Individual Plot Methods\n",
    "    # =======================\n",
    "    \n",
    "    def _plot_age_vs_motor_noise(self, df: pd.DataFrame):\n",
    "        \"\"\"Plot age vs motor noise relationship.\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        valid_data = df[['age', 'mot_noise']].dropna()\n",
    "        \n",
    "        plt.scatter(valid_data['age'], valid_data['mot_noise'], alpha=0.6, s=60)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(valid_data['age'], valid_data['mot_noise'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(valid_data['age'], p(valid_data['age']), \"r--\", alpha=0.8)\n",
    "        \n",
    "        # Add threshold line\n",
    "        plt.axhline(y=self.config['motor_noise_threshold'], color='red', linestyle='--', \n",
    "                   label=f'Threshold ({self.config[\"motor_noise_threshold\"]})')\n",
    "        \n",
    "        # Add correlation\n",
    "        r, p_val = pearsonr(valid_data['age'], valid_data['mot_noise'])\n",
    "        plt.text(0.05, 0.95, f'r = {r:.3f}\\np = {p_val:.3f}\\nn = {len(valid_data)}',\n",
    "                transform=plt.gca().transAxes, bbox=dict(boxstyle='round', facecolor='white'))\n",
    "        \n",
    "        plt.xlabel('Age (years)')\n",
    "        plt.ylabel('Motor Noise')\n",
    "        plt.title('Age vs Motor Noise Relationship')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.savefig(self.dirs['population_plots'] / 'age_vs_motor_noise.png', \n",
    "                   dpi=self.config['figure_dpi'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_age_vs_success_rates(self, df: pd.DataFrame, sr_cols: List[str]):\n",
    "        \"\"\"Plot age vs success rates for different conditions.\"\"\"\n",
    "        n_cols = min(len(sr_cols), 6)  # Limit for readability\n",
    "        n_rows = int(np.ceil(n_cols / 3))\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, min(3, n_cols), figsize=(15, 5*n_rows))\n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        axes_flat = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "        \n",
    "        for i, col in enumerate(sr_cols[:n_cols]):\n",
    "            ax = axes_flat[i]\n",
    "            \n",
    "            valid_data = df[['age', col]].dropna()\n",
    "            if len(valid_data) > 0:\n",
    "                ax.scatter(valid_data['age'], valid_data[col], alpha=0.6, s=50)\n",
    "                \n",
    "                # Add trend line\n",
    "                z = np.polyfit(valid_data['age'], valid_data[col], 1)\n",
    "                p = np.poly1d(z)\n",
    "                ax.plot(valid_data['age'], p(valid_data['age']), \"r--\", alpha=0.8)\n",
    "                \n",
    "                # Add correlation\n",
    "                r, p_val = pearsonr(valid_data['age'], valid_data[col])\n",
    "                ax.text(0.05, 0.95, f'r = {r:.3f}\\np = {p_val:.3f}',\n",
    "                       transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white'))\n",
    "                \n",
    "                ax.set_xlabel('Age (years)')\n",
    "                ax.set_ylabel('Success Rate')\n",
    "                ax.set_title(col.replace('_', ' ').title())\n",
    "                ax.set_ylim(0, 1)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_cols, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Age vs Success Rates', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(self.dirs['population_plots'] / 'age_vs_success_rates.png',\n",
    "                   dpi=self.config['figure_dpi'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_correlation_matrix(self, df: pd.DataFrame):\n",
    "        \"\"\"Plot correlation matrix of key variables.\"\"\"\n",
    "        key_cols = ['age']\n",
    "        if 'mot_noise' in df.columns:\n",
    "            key_cols.append('mot_noise')\n",
    "        if 'pref_asymmetry' in df.columns:\n",
    "            key_cols.append('pref_asymmetry')\n",
    "        \n",
    "        # Add success rate columns\n",
    "        sr_cols = [col for col in df.columns if '_sr_' in col and '_const' in col]\n",
    "        key_cols.extend(sr_cols[:6])\n",
    "        \n",
    "        if len(key_cols) > 1:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            corr_matrix = df[key_cols].corr()\n",
    "            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "            \n",
    "            sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                       square=True, linewidths=0.5, fmt='.2f')\n",
    "            \n",
    "            plt.title('Correlation Matrix of Key Variables')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plt.savefig(self.dirs['population_plots'] / 'correlation_matrix.png',\n",
    "                       dpi=self.config['figure_dpi'], bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "    def _plot_success_rate_distributions(self, df: pd.DataFrame, sr_cols: List[str]):\n",
    "        \"\"\"Plot success rate distributions across conditions.\"\"\"\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for col in sr_cols:\n",
    "            trial_type = col.split('_')[0]\n",
    "            condition = col.split('_')[2]\n",
    "            values = df[col].dropna()\n",
    "            for val in values:\n",
    "                plot_data.append({\n",
    "                    'trial_type': trial_type,\n",
    "                    'condition': condition,\n",
    "                    'success_rate': val\n",
    "                })\n",
    "        \n",
    "        if plot_data:\n",
    "            plot_df = pd.DataFrame(plot_data)\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # Box plot by trial type\n",
    "            sns.boxplot(data=plot_df, x='trial_type', y='success_rate', ax=axes[0, 0])\n",
    "            axes[0, 0].set_title('Success Rates by Trial Type')\n",
    "            axes[0, 0].set_ylim(0, 1)\n",
    "            \n",
    "            # Box plot by condition\n",
    "            sns.boxplot(data=plot_df, x='condition', y='success_rate', ax=axes[0, 1])\n",
    "            axes[0, 1].set_title('Success Rates by Condition')\n",
    "            axes[0, 1].set_ylim(0, 1)\n",
    "            \n",
    "            # Combined box plot\n",
    "            sns.boxplot(data=plot_df, x='trial_type', y='success_rate', hue='condition', ax=axes[1, 0])\n",
    "            axes[1, 0].set_title('Success Rates by Trial Type and Condition')\n",
    "            axes[1, 0].set_ylim(0, 1)\n",
    "            \n",
    "            # Violin plot\n",
    "            sns.violinplot(data=plot_df, x='trial_type', y='success_rate', hue='condition', ax=axes[1, 1])\n",
    "            axes[1, 1].set_title('Success Rate Distributions')\n",
    "            axes[1, 1].set_ylim(0, 1)\n",
    "            \n",
    "            plt.suptitle('Success Rate Analysis', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plt.savefig(self.dirs['population_plots'] / 'success_rate_distributions.png',\n",
    "                       dpi=self.config['figure_dpi'], bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "    def _plot_age_effects_summary(self):\n",
    "        \"\"\"Plot summary of age effects.\"\"\"\n",
    "        age_effects = self.analysis_results['age_effects']['age_effects']\n",
    "        \n",
    "        if not age_effects:\n",
    "            return\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        measures = [e['measure'] for e in age_effects]\n",
    "        correlations = [e['correlation'] for e in age_effects]\n",
    "        p_values = [e['p_value'] for e in age_effects]\n",
    "        significant = [e['significant'] for e in age_effects]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Correlation plot\n",
    "        colors = ['red' if sig else 'blue' for sig in significant]\n",
    "        bars = ax1.barh(range(len(measures)), correlations, color=colors, alpha=0.7)\n",
    "        ax1.set_yticks(range(len(measures)))\n",
    "        ax1.set_yticklabels([m.replace('_', ' ') for m in measures])\n",
    "        ax1.set_xlabel('Correlation with Age')\n",
    "        ax1.set_title('Age Effect Sizes')\n",
    "        ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # P-value plot\n",
    "        log_p = [-np.log10(p) for p in p_values]\n",
    "        bars2 = ax2.barh(range(len(measures)), log_p, color=colors, alpha=0.7)\n",
    "        ax2.set_yticks(range(len(measures)))\n",
    "        ax2.set_yticklabels([m.replace('_', ' ') for m in measures])\n",
    "        ax2.set_xlabel('-log10(p-value)')\n",
    "        ax2.set_title('Age Effect Significance')\n",
    "        ax2.axvline(x=-np.log10(0.05), color='red', linestyle='--', alpha=0.7, label='p = 0.05')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Age Effects Analysis', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(self.dirs['statistical_plots'] / 'age_effects_summary.png',\n",
    "                   dpi=self.config['figure_dpi'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_correlation_network(self):\n",
    "        \"\"\"Plot correlation network of strong correlations.\"\"\"\n",
    "        strong_corr = self.analysis_results['correlations']['strong_correlations']\n",
    "        \n",
    "        if not strong_corr:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create simple correlation plot\n",
    "        variables = set()\n",
    "        for corr in strong_corr:\n",
    "            variables.add(corr['var1'])\n",
    "            variables.add(corr['var2'])\n",
    "        \n",
    "        variables = list(variables)\n",
    "        n_vars = len(variables)\n",
    "        \n",
    "        if n_vars < 2:\n",
    "            return\n",
    "        \n",
    "        # Create correlation matrix for these variables\n",
    "        corr_matrix = np.zeros((n_vars, n_vars))\n",
    "        for i, var1 in enumerate(variables):\n",
    "            for j, var2 in enumerate(variables):\n",
    "                if i == j:\n",
    "                    corr_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    # Find correlation between var1 and var2\n",
    "                    for corr in strong_corr:\n",
    "                        if (corr['var1'] == var1 and corr['var2'] == var2) or \\\n",
    "                           (corr['var1'] == var2 and corr['var2'] == var1):\n",
    "                            corr_matrix[i, j] = corr['correlation']\n",
    "                            break\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0,\n",
    "                   xticklabels=[v.replace('_', ' ') for v in variables],\n",
    "                   yticklabels=[v.replace('_', ' ') for v in variables],\n",
    "                   fmt='.2f')\n",
    "        \n",
    "        plt.title('Strong Correlations Network')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(self.dirs['statistical_plots'] / 'correlation_network.png',\n",
    "                   dpi=self.config['figure_dpi'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Utility Methods\n",
    "    # ===============\n",
    "    \n",
    "    def _summarize_statistical_results(self) -> Dict:\n",
    "        \"\"\"Create summary of statistical analysis results.\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        if 'correlations' in self.analysis_results:\n",
    "            corr_results = self.analysis_results['correlations']\n",
    "            summary['correlations'] = {\n",
    "                'n_strong_correlations': len(corr_results.get('strong_correlations', [])),\n",
    "                'variables_analyzed': len(corr_results.get('variables_analyzed', []))\n",
    "            }\n",
    "        \n",
    "        if 'age_effects' in self.analysis_results:\n",
    "            age_results = self.analysis_results['age_effects']\n",
    "            summary['age_effects'] = {\n",
    "                'n_measures_tested': len(age_results.get('age_effects', [])),\n",
    "                'n_significant': age_results.get('n_significant', 0)\n",
    "            }\n",
    "        \n",
    "        if 'trial_comparisons' in self.analysis_results:\n",
    "            trial_results = self.analysis_results['trial_comparisons']\n",
    "            summary['trial_comparisons'] = {\n",
    "                'n_comparisons': len(trial_results.get('comparisons', [])),\n",
    "                'n_significant': trial_results.get('n_significant', 0)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def _generate_html_report(self) -> Path:\n",
    "        \"\"\"Generate comprehensive HTML report.\"\"\"\n",
    "        \n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Motor Learning Analysis Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "                .header {{ background-color: #f4f4f4; padding: 20px; border-radius: 5px; }}\n",
    "                .section {{ margin: 30px 0; }}\n",
    "                .metric {{ background-color: #e8f4f8; padding: 10px; margin: 10px 0; border-radius: 3px; }}\n",
    "                .significant {{ color: #d63384; font-weight: bold; }}\n",
    "                .warning {{ color: #fd7e14; }}\n",
    "                .success {{ color: #198754; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"header\">\n",
    "                <h1>Motor Learning Analysis Report</h1>\n",
    "                <p><strong>Analysis Date:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "                <p><strong>Analysis ID:</strong> {self.analysis_timestamp}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>1. Sample Characteristics</h2>\n",
    "                <div class=\"metric\">\n",
    "                    <strong>Total Subjects:</strong> {len(self.metrics_df)}\n",
    "                </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add age statistics if available\n",
    "        if 'age' in self.metrics_df.columns:\n",
    "            age_stats = self.metrics_df['age'].describe()\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric\">\n",
    "                    <strong>Age Range:</strong> {age_stats['min']:.1f} - {age_stats['max']:.1f} years\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <strong>Mean Age:</strong> {age_stats['mean']:.1f} ± {age_stats['std']:.1f} years\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Add motor noise statistics if available\n",
    "        if 'mot_noise' in self.metrics_df.columns:\n",
    "            noise_stats = self.metrics_df['mot_noise'].describe()\n",
    "            high_noise = (self.metrics_df['mot_noise'] > self.config['motor_noise_threshold']).sum()\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric\">\n",
    "                    <strong>Motor Noise Range:</strong> {noise_stats['min']:.3f} - {noise_stats['max']:.3f}\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <strong>High Motor Noise Subjects:</strong> {high_noise} ({100*high_noise/len(self.metrics_df):.1f}%)\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Add statistical results\n",
    "        if self.analysis_results:\n",
    "            html_content += \"\"\"\n",
    "                <h2>2. Statistical Analysis Results</h2>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Age effects\n",
    "            if 'age_effects' in self.analysis_results:\n",
    "                age_results = self.analysis_results['age_effects']\n",
    "                n_sig = age_results.get('n_significant', 0)\n",
    "                n_total = len(age_results.get('age_effects', []))\n",
    "                html_content += f\"\"\"\n",
    "                    <div class=\"metric\">\n",
    "                        <strong>Age Effects:</strong> {n_sig}/{n_total} measures show significant age effects\n",
    "                    </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Correlations\n",
    "            if 'correlations' in self.analysis_results:\n",
    "                corr_results = self.analysis_results['correlations']\n",
    "                n_strong = len(corr_results.get('strong_correlations', []))\n",
    "                html_content += f\"\"\"\n",
    "                    <div class=\"metric\">\n",
    "                        <strong>Strong Correlations:</strong> {n_strong} correlations with |r| > 0.3\n",
    "                    </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            <div class=\"section\">\n",
    "                <h2>3. Generated Visualizations</h2>\n",
    "                <p>The following visualizations were generated:</p>\n",
    "                <ul>\n",
    "                    <li>Individual stride change plots</li>\n",
    "                    <li>Population-level analysis plots</li>\n",
    "                    <li>Statistical analysis plots</li>\n",
    "                    <li>Summary dashboard</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <p><em>This report was automatically generated by the Integrated Motor Learning Analysis Pipeline.</em></p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save HTML report\n",
    "        report_file = self.dirs['reports'] / f'analysis_report_{self.analysis_timestamp}.html'\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        return report_file\n",
    "\n",
    "    def _make_json_serializable(self, obj):\n",
    "        \"\"\"Convert numpy objects to JSON serializable format.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._make_json_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._make_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de7f34-6d91-4e46-a2e0-3ba4aecad35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED USAGE CODE\n",
    "# The issue was passing config instead of a string path\n",
    "\n",
    "METADATA_PATH = \"muh_metadata.csv\"\n",
    "DATA_ROOT_DIR = \"muh_data/\"\n",
    "\n",
    "# 1. Create centralized config\n",
    "config = Config('analysis')\n",
    "\n",
    "# 2. Create data manager with config\n",
    "data_manager = MotorLearningDataManager(\n",
    "    METADATA_PATH, \n",
    "    DATA_ROOT_DIR, \n",
    "    config=config, \n",
    "    debug=True\n",
    ")\n",
    "\n",
    "data_manager = data_manager.filter_trials(required_trial_types=['vis1', 'invis', 'vis2'])\n",
    "\n",
    "# 3. Calculate metrics\n",
    "metrics_calculator = MetricsCalculator(data_manager)\n",
    "metrics_df = metrics_calculator.calculate_all_metrics()\n",
    "\n",
    "# 4. Create analysis with shared config\n",
    "analysis = MotorLearningAnalysis(data_manager, metrics_df)\n",
    "\n",
    "print(f\"✅ Analysis ready! {len(metrics_df)} subjects loaded.\")\n",
    "print(f\"📁 All outputs will be saved to: {config.BASE_OUTPUT_DIR}\")\n",
    "print(f\"🎯 Visualizer initialized successfully: {hasattr(analysis.visualizer, 'individual_plots_dir')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410bbb5-21a7-4925-b912-8158816d4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CORRECTED: Create integrated pipeline with STRING output directory\n",
    "# Option 1: Use the same directory as config\n",
    "pipeline = IntegratedMotorLearningPipeline(analysis, str(config.BASE_OUTPUT_DIR))\n",
    "\n",
    "# OR Option 2: Use a different directory name\n",
    "# pipeline = IntegratedMotorLearningPipeline(analysis, 'integrated_analysis_output')\n",
    "\n",
    "print(f\"✅ Pipeline ready!\")\n",
    "\n",
    "# 6. Run the full pipeline\n",
    "print(\"\\n🚀 Running integrated analysis pipeline...\")\n",
    "pipeline_results = pipeline.run_complete_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6162ccb-394a-4c39-8eba-1343c934d267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
