{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d5fd3-6c9d-4c53-8aac-a5de3902599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED MOTOR LEARNING ANALYSIS WITH STRIDE CHANGE DISTRIBUTION\n",
    "# Incorporates the stride change distribution plotting functionality\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import tempfile\n",
    "import webbrowser\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, mannwhitneyu, gaussian_kde\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, confusion_matrix, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b028b-f0a9-4cb1-bd4a-a59726b15c54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the motor learning analysis.\"\"\"\n",
    "    \n",
    "    # File processing\n",
    "    MIN_COMPLETE_STRIDES = 20\n",
    "    PROCESSED_DATA_FILE = 'processed_data.pkl'\n",
    "    FIGURES_DIR = Path('figures')\n",
    "    \n",
    "    # Trial type mappings\n",
    "    TRIAL_TYPE_MAPPING = {\n",
    "        'primer': 'vis1',\n",
    "        'trial': 'invis', \n",
    "        'vis': 'vis2',\n",
    "        'pref': 'pref'\n",
    "    }\n",
    "    \n",
    "    # Analysis parameters\n",
    "    MOTOR_NOISE_THRESHOLD = 0.3\n",
    "    SUCCESS_RATE_THRESHOLD = 0.68\n",
    "    TARGET_SIZE_THRESHOLD = 0.31\n",
    "    MAX_STRIDES_THRESHOLD = 415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0725fd-e206-4ee8-ab35-2ad451620009",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "class DataUtils:\n",
    "    \"\"\"Utility functions for data processing.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_and_validate_file(file_path: Path, required_cols: set = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load and validate a single data file.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            if required_cols and not required_cols.issubset(df.columns):\n",
    "                return None\n",
    "                \n",
    "            # Basic cleaning\n",
    "            if 'Stride Number' in df.columns:\n",
    "                df['Stride Number'] = pd.to_numeric(df['Stride Number'], errors='coerce')\n",
    "                df = df.dropna(subset=['Stride Number'])\n",
    "                df = df.drop_duplicates(subset=['Stride Number'])\n",
    "            \n",
    "            return df if not df.empty else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file_path.name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_anomalies(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Detect and flag anomalies in stride data.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return df, {}\n",
    "\n",
    "        df = df.copy()\n",
    "        df['Anomalous'] = False\n",
    "        anomalies = {}\n",
    "\n",
    "        # Time-based anomalies\n",
    "        time_col = next((col for col in ['Time', 'Timestamp', 'Time (s)'] \n",
    "                        if col in df.columns), None)\n",
    "        if time_col:\n",
    "            df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "            time_diff = df[time_col].diff()\n",
    "            jump_mask = time_diff > time_diff.quantile(0.99) * 5\n",
    "            \n",
    "            for idx in df.index[jump_mask.fillna(False)]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('time_jump')\n",
    "\n",
    "        # Sum of gains and steps anomalies\n",
    "        if 'Sum of gains and steps' in df.columns:\n",
    "            high_mask = df['Sum of gains and steps'] > 4\n",
    "            zero_mask = df['Sum of gains and steps'] == 0\n",
    "\n",
    "            for idx in df.index[high_mask]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('sum_gain_step_high')\n",
    "            \n",
    "            for idx in df.index[zero_mask]:\n",
    "                df.at[idx, 'Anomalous'] = True\n",
    "                anomalies.setdefault(idx, []).append('sum_gain_step_zero')\n",
    "\n",
    "        # Duplicate rows\n",
    "        duplicated_mask = df.duplicated()\n",
    "        for idx in df.index[duplicated_mask]:\n",
    "            df.at[idx, 'Anomalous'] = True\n",
    "            anomalies.setdefault(idx, []).append('duplicate_row')\n",
    "\n",
    "        return df, anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaa9d3-4239-477d-a5dc-d824c0684962",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3. TRIAL PROCESSING\n",
    "# ==============================================================================\n",
    "\n",
    "class TrialProcessor:\n",
    "    \"\"\"Handles loading, combining, and processing of trial data.\"\"\"\n",
    "    \n",
    "    def __init__(self, debug: bool = True):\n",
    "        self.debug = debug\n",
    "        \n",
    "    def find_and_combine_trial_files(self, subject_dir: Path, trial_prefix: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Find and combine trial files for a given trial type.\"\"\"\n",
    "        all_files = sorted(subject_dir.glob(f\"{trial_prefix}*.txt\"))\n",
    "        \n",
    "        if not all_files:\n",
    "            if self.debug:\n",
    "                print(f\"  ⚠️ No files found for {trial_prefix}\")\n",
    "            return None\n",
    "        \n",
    "        # Special handling for preference trials\n",
    "        if trial_prefix == 'pref':\n",
    "            return self._handle_pref_trial(all_files)\n",
    "        \n",
    "        # Single file case\n",
    "        if len(all_files) == 1:\n",
    "            return DataUtils.load_and_validate_file(all_files[0])\n",
    "        \n",
    "        # Multiple files - combine them\n",
    "        return self._combine_trial_fragments(all_files)\n",
    "    \n",
    "    def _handle_pref_trial(self, files: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Handle preference trial - select largest file.\"\"\"\n",
    "        largest_file = max(files, key=lambda f: f.stat().st_size)\n",
    "        if self.debug and len(files) > 1:\n",
    "            print(f\"  ⚡ pref trial - selected largest of {len(files)} files\")\n",
    "        return DataUtils.load_and_validate_file(largest_file)\n",
    "    \n",
    "    def _combine_trial_fragments(self, files: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Combine multiple trial fragments intelligently.\"\"\"\n",
    "        file_info = []\n",
    "        \n",
    "        for f in files:\n",
    "            try:\n",
    "                with open(f, 'r') as file:\n",
    "                    header = file.readline().strip().split('\\t')\n",
    "                    stride_col = next((i for i, col in enumerate(header) \n",
    "                                     if 'stride' in col.lower() and \n",
    "                                     ('num' in col.lower() or 'no' in col.lower())), None)\n",
    "                    \n",
    "                    if stride_col is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get stride range\n",
    "                    lines = file.readlines()\n",
    "                    first_stride = float(lines[0].split('\\t')[stride_col])\n",
    "                    last_stride = float(lines[-1].split('\\t')[stride_col])\n",
    "                    \n",
    "                    file_info.append({\n",
    "                        'path': f,\n",
    "                        'first': first_stride,\n",
    "                        'last': last_stride,\n",
    "                        'size': f.stat().st_size\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not file_info:\n",
    "            return DataUtils.load_and_validate_file(max(files, key=lambda f: f.stat().st_size))\n",
    "        \n",
    "        # Find best continuous sequence\n",
    "        file_info.sort(key=lambda x: x['first'])\n",
    "        best_sequence = self._find_best_sequence(file_info)\n",
    "        \n",
    "        if len(best_sequence) >= 2:\n",
    "            return self._merge_files([f['path'] for f in best_sequence])\n",
    "        \n",
    "        # Fallback to largest file\n",
    "        return DataUtils.load_and_validate_file(max(file_info, key=lambda x: x['size'])['path'])\n",
    "    \n",
    "    def _find_best_sequence(self, file_info: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Find the best continuous sequence of files.\"\"\"\n",
    "        best_sequence = []\n",
    "        current_sequence = [file_info[0]]\n",
    "        \n",
    "        for file_data in file_info[1:]:\n",
    "            if file_data['first'] == current_sequence[-1]['last'] + 1:\n",
    "                current_sequence.append(file_data)\n",
    "            else:\n",
    "                if len(current_sequence) > len(best_sequence):\n",
    "                    best_sequence = current_sequence\n",
    "                current_sequence = [file_data]\n",
    "        \n",
    "        return max([best_sequence, current_sequence], key=len)\n",
    "    \n",
    "    def _merge_files(self, file_paths: List[Path]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Merge multiple files into a single DataFrame.\"\"\"\n",
    "        dfs = []\n",
    "        for f in file_paths:\n",
    "            df = DataUtils.load_and_validate_file(f)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "        \n",
    "        if not dfs:\n",
    "            return None\n",
    "        \n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Clean and sort\n",
    "        if 'Stride Number' in combined.columns:\n",
    "            combined = combined.sort_values('Stride Number')\n",
    "            combined = combined.drop_duplicates('Stride Number')\n",
    "        \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737c435-0348-4a95-9052-77cc5cb51e36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 4. MAIN DATA MANAGER\n",
    "# ==============================================================================\n",
    "\n",
    "class MotorLearningDataManager:\n",
    "    \"\"\"Main class for managing motor learning data.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path: str, data_root_dir: str, \n",
    "                 force_reprocess: bool = False, debug: bool = True):\n",
    "        self.metadata_path = metadata_path\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Initialize components\n",
    "        self.trial_processor = TrialProcessor(debug=debug)\n",
    "        self.config = Config()\n",
    "        \n",
    "        # Data storage\n",
    "        self.metadata = None\n",
    "        self.processed_data = {}\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.config.FIGURES_DIR, exist_ok=True)\n",
    "        \n",
    "        # Load or process data\n",
    "        if not force_reprocess and os.path.exists(self.config.PROCESSED_DATA_FILE):\n",
    "            self._load_processed_data()\n",
    "        else:\n",
    "            self._process_all_data()\n",
    "            self._save_processed_data()\n",
    "    \n",
    "    def _load_processed_data(self):\n",
    "        \"\"\"Load previously processed data.\"\"\"\n",
    "        with open(self.config.PROCESSED_DATA_FILE, 'rb') as f:\n",
    "            self.processed_data = pickle.load(f)\n",
    "            \n",
    "        # Rebuild metadata DataFrame\n",
    "        self.metadata = pd.DataFrame.from_dict(\n",
    "            {subj: data['metadata'] for subj, data in self.processed_data.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "    \n",
    "    def _save_processed_data(self):\n",
    "        \"\"\"Save processed data.\"\"\"\n",
    "        with open(self.config.PROCESSED_DATA_FILE, 'wb') as f:\n",
    "            pickle.dump(self.processed_data, f)\n",
    "    \n",
    "    def _process_all_data(self):\n",
    "        \"\"\"Process all subject data.\"\"\"\n",
    "        self._load_metadata()\n",
    "        total_subjects = len(self.metadata)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"🔄 Processing {total_subjects} subjects...\")\n",
    "        \n",
    "        for i, (_, row) in enumerate(self.metadata.iterrows(), 1):\n",
    "            subject_id = row['ID']\n",
    "            if self.debug:\n",
    "                print(f\"\\n[{i}/{total_subjects}] Processing {subject_id}...\")\n",
    "            \n",
    "            subject_data = self._process_subject_data(subject_id, row)\n",
    "            if subject_data:\n",
    "                self.processed_data[subject_id] = subject_data\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load and clean metadata.\"\"\"\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        self.metadata['DOB'] = pd.to_datetime(self.metadata['DOB'], errors='coerce')\n",
    "        self.metadata['Session Date'] = pd.to_datetime(self.metadata['Session Date'], errors='coerce')\n",
    "        self.metadata = self.metadata.dropna(subset=['ID', 'age_months'])\n",
    "    \n",
    "    def _process_subject_data(self, subject_id: str, metadata_row: pd.Series) -> Optional[Dict]:\n",
    "        \"\"\"Process data for a single subject.\"\"\"\n",
    "        subject_dir = Path(self.data_root_dir) / subject_id\n",
    "        if not subject_dir.exists():\n",
    "            if self.debug:\n",
    "                print(f\"❌ Directory not found: {subject_dir}\")\n",
    "            return None\n",
    "        \n",
    "        trial_data = {}\n",
    "        \n",
    "        for original_type in ['primer', 'trial', 'vis', 'pref']:\n",
    "            try:\n",
    "                # Load trial data\n",
    "                df = self.trial_processor.find_and_combine_trial_files(subject_dir, original_type)\n",
    "                \n",
    "                if df is not None:\n",
    "                    # Process the data\n",
    "                    processed_df, anomalies = self._process_trial_data(df, original_type)\n",
    "                    \n",
    "                    # Store with mapped name\n",
    "                    new_type = self.config.TRIAL_TYPE_MAPPING[original_type]\n",
    "                    trial_data[new_type] = {\n",
    "                        'data': processed_df,\n",
    "                        'anomalies': anomalies\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {subject_id}/{original_type}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            'metadata': metadata_row.to_dict(),\n",
    "            'trial_data': trial_data\n",
    "        } if trial_data else None\n",
    "    \n",
    "    def _process_trial_data(self, df: pd.DataFrame, trial_type: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Process trial data and calculate metrics.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return None, {}\n",
    "        \n",
    "        # Skip processing for pref trials (just clean duplicates)\n",
    "        if trial_type == 'pref':\n",
    "            df = df.drop_duplicates(subset='Left heel strike', keep='last')\n",
    "            return df, {}\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = ['Stride Number', 'Success', 'Upper bound success', \n",
    "                        'Lower bound success', 'Constant']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Missing required columns: {missing_cols}\")\n",
    "            return None, {}\n",
    "        \n",
    "        try:\n",
    "            # Process trial data\n",
    "            df = df.sort_values('Stride Number')\n",
    "            df['Target size'] = df['Upper bound success'] - df['Lower bound success']\n",
    "            df = df.drop_duplicates(subset='Stride Number', keep='last')\n",
    "            \n",
    "            # Scale sum of gains and steps\n",
    "            if 'Sum of gains and steps' in df.columns:\n",
    "                df['Sum of gains and steps'] = 1.5 * df['Sum of gains and steps']\n",
    "            \n",
    "            # Detect anomalies\n",
    "            df, anomalies = DataUtils.detect_anomalies(df)\n",
    "            \n",
    "            return df, anomalies\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {trial_type} data: {str(e)}\")\n",
    "            return None, {}\n",
    "    \n",
    "    def get_trial_df(self, trial_dict: Dict) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get trial data from trial dictionary.\"\"\"\n",
    "        if trial_dict and 'data' in trial_dict:\n",
    "            return trial_dict['data']\n",
    "        return None\n",
    "    \n",
    "    def filter_trials(self, max_target_size=None, min_age=None, max_age=None, \n",
    "                     required_trial_types=None, min_strides=None, max_strides=None):\n",
    "        \"\"\"Filter trials based on specified criteria.\"\"\"\n",
    "        filtered_data = {}\n",
    "        \n",
    "        for subject_id, subject_data in self.processed_data.items():\n",
    "            # SIMPLIFIED AGE FILTERING - always use age_months/12\n",
    "            age = subject_data['metadata']['age_months'] / 12\n",
    "            if min_age is not None and age < min_age:\n",
    "                continue\n",
    "            if max_age is not None and age > max_age:\n",
    "                continue\n",
    "            \n",
    "            # Check required trial types\n",
    "            if required_trial_types:\n",
    "                missing_trials = [\n",
    "                    t for t in required_trial_types \n",
    "                    if t not in subject_data['trial_data'] or \n",
    "                       subject_data['trial_data'][t] is None or\n",
    "                       subject_data['trial_data'][t]['data'] is None\n",
    "                ]\n",
    "                if missing_trials:\n",
    "                    continue\n",
    "            \n",
    "            # Check other criteria\n",
    "            valid_subject = True\n",
    "            filtered_trial_data = {}\n",
    "            \n",
    "            for trial_type, trial_dict in subject_data['trial_data'].items():\n",
    "                if trial_dict and trial_dict['data'] is not None:\n",
    "                    df = trial_dict['data']\n",
    "                    \n",
    "                    # Apply filters\n",
    "                    if (max_target_size is not None and \n",
    "                        'Target size' in df.columns and \n",
    "                        df['Target size'].min() > max_target_size):\n",
    "                        valid_subject = False\n",
    "                        break\n",
    "                    \n",
    "                    n_strides = len(df)\n",
    "                    if ((min_strides is not None and n_strides < min_strides) or\n",
    "                        (max_strides is not None and n_strides > max_strides)):\n",
    "                        valid_subject = False\n",
    "                        break\n",
    "                    \n",
    "                    # Include valid trial\n",
    "                    filtered_trial_data[trial_type] = {\n",
    "                        'data': df.copy(),\n",
    "                        'anomalies': trial_dict['anomalies'].copy()\n",
    "                    }\n",
    "            \n",
    "            if valid_subject and filtered_trial_data:\n",
    "                filtered_data[subject_id] = {\n",
    "                    'metadata': subject_data['metadata'].copy(),\n",
    "                    'trial_data': filtered_trial_data\n",
    "                }\n",
    "        \n",
    "        # Create new instance with filtered data\n",
    "        new_instance = MotorLearningDataManager.__new__(MotorLearningDataManager)\n",
    "        new_instance.config = self.config\n",
    "        new_instance.trial_processor = self.trial_processor\n",
    "        new_instance.metadata_path = self.metadata_path\n",
    "        new_instance.data_root_dir = self.data_root_dir\n",
    "        new_instance.debug = self.debug\n",
    "        new_instance.processed_data = filtered_data\n",
    "        new_instance.metadata = pd.DataFrame.from_dict(\n",
    "            {subj: data['metadata'] for subj, data in filtered_data.items()}, \n",
    "            orient='index'\n",
    "        )\n",
    "        \n",
    "        return new_instance\n",
    "    \n",
    "    def get_trial_data(self, subject_id: str, trial_type: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Get trial data for a specific subject and trial type.\"\"\"\n",
    "        try:\n",
    "            trial_dict = self.processed_data[subject_id]['trial_data'][trial_type]\n",
    "            return trial_dict['data'] if trial_dict else None\n",
    "        except KeyError:\n",
    "            return None\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary statistics of the dataset.\"\"\"\n",
    "        print(f\"📊 Dataset Summary:\")\n",
    "        print(f\"Total subjects: {len(self.processed_data)}\")\n",
    "        \n",
    "        if self.metadata is not None:\n",
    "            # SIMPLIFIED AGE DISPLAY - always use age_months/12\n",
    "            ages = self.metadata['age_months'] / 12\n",
    "            print(f\"Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "            print(f\"Mean age: {ages.mean():.1f} years\")\n",
    "        \n",
    "        # Trial type counts\n",
    "        trial_counts = defaultdict(int)\n",
    "        for subject_data in self.processed_data.values():\n",
    "            for trial_type in subject_data['trial_data'].keys():\n",
    "                trial_counts[trial_type] += 1\n",
    "        \n",
    "        print(f\"Trial type counts: {dict(trial_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d5704-0c04-4d6e-990e-d46391979b3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 5. METRICS CALCULATOR\n",
    "# ==============================================================================\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Handles calculation of performance metrics with column naming.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_manager: MotorLearningDataManager):\n",
    "        self.data_manager = data_manager\n",
    "        self.config = Config()\n",
    "    \n",
    "    def calculate_all_metrics(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate comprehensive performance metrics for all subjects.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"🧮 Calculating metrics for {len(self.data_manager.processed_data)} subjects...\")\n",
    "        \n",
    "        for i, (subject_id, data) in enumerate(self.data_manager.processed_data.items(), 1):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"   Processed {i}/{len(self.data_manager.processed_data)} subjects...\")\n",
    "            \n",
    "            try:\n",
    "                subject_result = self._calculate_subject_metrics(subject_id, data)\n",
    "                if subject_result:\n",
    "                    results.append(subject_result)\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error processing {subject_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            print(\"❌ No valid metrics calculated!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(results).infer_objects()\n",
    "        print(f\"✅ Successfully calculated metrics for {len(df)} subjects\")\n",
    "        \n",
    "        # Print age and column summary\n",
    "        if 'age' in df.columns:\n",
    "            ages = df['age'].dropna()\n",
    "            print(f\"📊 Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "        \n",
    "        sr_cols = [col for col in df.columns if '_sr_' in col]\n",
    "        print(f\"🎯 Success rate columns created: {sr_cols}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _calculate_subject_metrics(self, subject_id: str, subject_data: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Calculate metrics for a single subject with simplified age handling.\"\"\"\n",
    "        \n",
    "        # SIMPLIFIED: Only store age as age_months/12, call it 'age'\n",
    "        result = {\n",
    "            'ID': subject_id,\n",
    "            'age': subject_data['metadata'].get('age_months', np.nan) / 12,  # Single age field\n",
    "            'session_date': subject_data['metadata'].get('Session Date')\n",
    "        }\n",
    "        \n",
    "        # Process each trial type\n",
    "        for trial_type in ['vis1', 'invis', 'vis2']:\n",
    "            try:\n",
    "                trial_dict = subject_data['trial_data'].get(trial_type)\n",
    "                df = trial_dict['data'] if trial_dict else None\n",
    "                \n",
    "                if df is None or df.empty or 'Success' not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate metrics for both conditions\n",
    "                for condition in ['max', 'min']:\n",
    "                    period_data, indices = self._get_period_data(df, condition)\n",
    "                    if period_data is not None and not period_data.empty:\n",
    "                        metrics = self._calculate_period_metrics(period_data, trial_type, condition)\n",
    "                        result.update(metrics)\n",
    "                        result[f'{trial_type}_{condition}_const_indices'] = indices\n",
    "                \n",
    "                # Add trial metadata - ONLY if df is not None\n",
    "                if df is not None:\n",
    "                    result.update({\n",
    "                        f'{trial_type}_min_target_size': df['Target size'].min() if 'Target size' in df.columns else None,\n",
    "                        f'{trial_type}_max_constant': df['Constant'].max() if 'Constant' in df.columns else None,\n",
    "                        f'{trial_type}_min_constant': df['Constant'].min() if 'Constant' in df.columns else None\n",
    "                    })\n",
    "                    \n",
    "                    # Order information for invis trials\n",
    "                    if trial_type == 'invis':\n",
    "                        result.update(self._calculate_condition_order(df))\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Process preference trial\n",
    "        try:\n",
    "            pref_metrics = self._calculate_preference_metrics(subject_data['trial_data'].get('pref'))\n",
    "            result.update(pref_metrics)\n",
    "        except Exception as e:\n",
    "            result.update({'mot_noise': None, 'pref_asymmetry': None})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_period_data(self, df: pd.DataFrame, condition: str, length: int = 20):\n",
    "        \"\"\"Extract data for specific condition period.\"\"\"\n",
    "        try:\n",
    "            if df is None or df.empty:\n",
    "                return None, None\n",
    "            \n",
    "            if 'Target size' not in df.columns or 'Constant' not in df.columns:\n",
    "                return None, None\n",
    "            \n",
    "            min_target = df['Target size'].min()\n",
    "            target_tolerance = 0.001\n",
    "            \n",
    "            min_target_periods = df[df['Target size'] <= min_target + target_tolerance]\n",
    "            if min_target_periods.empty:\n",
    "                return None, None\n",
    "            \n",
    "            const_value = (min_target_periods['Constant'].max() if condition == 'max'\n",
    "                          else min_target_periods['Constant'].min())\n",
    "            \n",
    "            period_data = min_target_periods[\n",
    "                np.isclose(min_target_periods['Constant'], const_value, rtol=1e-5)\n",
    "            ]\n",
    "            \n",
    "            if period_data.empty:\n",
    "                return None, None\n",
    "            \n",
    "            return period_data.tail(length), period_data.index\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def _calculate_period_metrics(self, period_data: pd.DataFrame, trial_type: str, condition: str) -> Dict:\n",
    "        \"\"\"Calculate metrics for a specific period with naming.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # CORRECTED: Use the format that analysis expects\n",
    "            # Success rate - THE KEY METRIC  \n",
    "            metrics[f'{trial_type}_sr_{condition}_const'] = period_data['Success'].mean()\n",
    "            \n",
    "            # Other metrics\n",
    "            if 'Sum of gains and steps' in period_data.columns:\n",
    "                sogs = period_data['Sum of gains and steps']\n",
    "                metrics[f'{trial_type}_sd_{condition}_const'] = sogs.std()\n",
    "                metrics[f'{trial_type}_msl_{condition}_const'] = sogs.mean()\n",
    "                metrics[f'{trial_type}_error_{condition}_const'] = (sogs - period_data['Constant']).mean()\n",
    "            \n",
    "            # Asymmetry\n",
    "            if all(col in period_data.columns for col in ['Right step length', 'Left step length']):\n",
    "                right_steps = period_data['Right step length']\n",
    "                left_steps = period_data['Left step length']\n",
    "                denominator = right_steps + left_steps\n",
    "                \n",
    "                valid_mask = denominator != 0\n",
    "                if valid_mask.any():\n",
    "                    asymmetry = ((right_steps - left_steps) / denominator).abs()[valid_mask].mean()\n",
    "                    metrics[f'{trial_type}_asymmetry_{condition}_const'] = asymmetry\n",
    "            \n",
    "            # Strides between successes\n",
    "            strides_between = self._calculate_strides_between_successes(period_data)\n",
    "            if strides_between is not None:\n",
    "                metrics[f'{trial_type}_strides_between_success_{condition}_const'] = strides_between\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_strides_between_successes(self, df: pd.DataFrame) -> Optional[float]:\n",
    "        \"\"\"Calculate average strides between successful trials.\"\"\"\n",
    "        try:\n",
    "            if df is None or 'Success' not in df.columns:\n",
    "                return None\n",
    "            \n",
    "            df = df.reset_index(drop=True)\n",
    "            success_positions = df.index[df['Success'] == 1].tolist()\n",
    "            \n",
    "            if len(success_positions) < 2:\n",
    "                return None\n",
    "            \n",
    "            return np.mean(np.diff(success_positions))\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _calculate_condition_order(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Determine which condition came first for invis trials.\"\"\"\n",
    "        try:\n",
    "            all_max_indices = df.index[df['Constant'] == df['Constant'].max()].tolist()\n",
    "            all_min_indices = df.index[df['Constant'] == df['Constant'].min()].tolist()\n",
    "            \n",
    "            if all_max_indices and all_min_indices:\n",
    "                first_max = min(all_max_indices)\n",
    "                first_min = min(all_min_indices)\n",
    "                return {\n",
    "                    'invis_max_first': first_max < first_min,\n",
    "                    'invis_min_first': first_min < first_max\n",
    "                }\n",
    "            \n",
    "            return {'invis_max_first': False, 'invis_min_first': False}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'invis_max_first': False, 'invis_min_first': False}\n",
    "    \n",
    "    def _calculate_preference_metrics(self, pref_trial_dict: Optional[Dict]) -> Dict:\n",
    "        \"\"\"Calculate metrics from preference trial.\"\"\"\n",
    "        metrics = {'mot_noise': None, 'pref_asymmetry': None}\n",
    "        \n",
    "        try:\n",
    "            if not pref_trial_dict or pref_trial_dict.get('data') is None:\n",
    "                return metrics\n",
    "            \n",
    "            pref_df = pref_trial_dict['data']\n",
    "            if pref_df is None or pref_df.empty:\n",
    "                return metrics\n",
    "            \n",
    "            # Check for required columns\n",
    "            if not all(col in pref_df.columns for col in ['Right step length', 'Left step length']):\n",
    "                return metrics\n",
    "            \n",
    "            # Get non-zero steps\n",
    "            right_steps = pref_df['Right step length']\n",
    "            left_steps = pref_df['Left step length']\n",
    "            \n",
    "            # Remove zeros and NaNs\n",
    "            right_clean = right_steps[(right_steps != 0) & (right_steps.notna())]\n",
    "            left_clean = left_steps[(left_steps != 0) & (left_steps.notna())]\n",
    "            \n",
    "            if len(right_clean) < 20 or len(left_clean) < 20:\n",
    "                return metrics\n",
    "            \n",
    "            # Calculate motor noise\n",
    "            final_right = right_clean.iloc[-1]\n",
    "            final_left = left_clean.iloc[-1]\n",
    "            \n",
    "            if final_right <= 0 or final_left <= 0:\n",
    "                return metrics\n",
    "            \n",
    "            # Normalize steps\n",
    "            norm_right = right_clean / final_right\n",
    "            norm_left = left_clean / final_left\n",
    "            \n",
    "            # Calculate sum of normalized steps\n",
    "            min_length = min(len(norm_right), len(norm_left))\n",
    "            if min_length < 20:\n",
    "                return metrics\n",
    "            \n",
    "            sum_steps = norm_right.iloc[:min_length] + norm_left.iloc[:min_length]\n",
    "            \n",
    "            # Motor noise from last 20 points\n",
    "            if len(sum_steps) >= 20:\n",
    "                noise = sum_steps.tail(20).std()\n",
    "                if not pd.isna(noise) and noise > 0:\n",
    "                    metrics['mot_noise'] = noise\n",
    "            \n",
    "            # Calculate step length asymmetry\n",
    "            if len(right_clean) >= 20 and len(left_clean) >= 20:\n",
    "                last_20_right = right_clean.tail(20) / final_right\n",
    "                last_20_left = left_clean.tail(20) / final_left\n",
    "                \n",
    "                if len(last_20_right) == len(last_20_left):\n",
    "                    denominator = last_20_right.values + last_20_left.values\n",
    "                    valid_mask = denominator != 0\n",
    "                    \n",
    "                    if valid_mask.any():\n",
    "                        asymmetry_vals = np.abs((last_20_right.values - last_20_left.values) / denominator)[valid_mask]\n",
    "                        if len(asymmetry_vals) > 0:\n",
    "                            metrics['pref_asymmetry'] = np.mean(asymmetry_vals)\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16add11-dcf4-42a5-bccc-c1373ab52afa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 6. STATISTICAL ANALYZER\n",
    "# ==============================================================================\n",
    "\n",
    "class StatisticalAnalyzer:\n",
    "    \"\"\"Handles all statistical analyses withcolumn naming.\"\"\"\n",
    "    \n",
    "    def __init__(self, metrics_df: pd.DataFrame):\n",
    "        self.metrics_df = metrics_df\n",
    "        \n",
    "        # Apply motor noise filter if available\n",
    "        if 'mot_noise' in metrics_df.columns:\n",
    "            self.filtered_df = metrics_df[metrics_df['mot_noise'] <= 0.3]\n",
    "            print(f\"📊 Filtered to {len(self.filtered_df)}/{len(metrics_df)} subjects (motor noise ≤ 0.3)\")\n",
    "        else:\n",
    "            self.filtered_df = metrics_df\n",
    "    \n",
    "    def run_regression_analysis(self, trial_type: str = 'invis', condition: str = 'max',\n",
    "                               predictors: List[str] = None, model_type: str = 'linear') -> Dict:\n",
    "        \"\"\"Run regression analysis withtarget column naming.\"\"\"\n",
    "        \n",
    "        if predictors is None:\n",
    "            predictors = ['age', 'mot_noise', 'pref_asymmetry']\n",
    "        \n",
    "        # Filter available predictors\n",
    "        available_predictors = [p for p in predictors if p in self.filtered_df.columns]\n",
    "        \n",
    "        # CORRECTED: Use the exact column format that exists\n",
    "        target_col = f'{trial_type}_sr_{condition}_const'\n",
    "        \n",
    "        if target_col not in self.filtered_df.columns:\n",
    "            available_cols = [col for col in self.filtered_df.columns if '_sr_' in col]\n",
    "            raise ValueError(f\"Target column {target_col} not found. Available: {available_cols}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        valid_data = self.filtered_df[available_predictors + [target_col]].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            raise ValueError(f\"Insufficient data: only {len(valid_data)} valid samples\")\n",
    "        \n",
    "        X = valid_data[available_predictors]\n",
    "        y = valid_data[target_col]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'linear':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', LinearRegression())\n",
    "            ])\n",
    "        elif model_type == 'random_forest':\n",
    "            model = RandomForestRegressor(random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'linear' or 'random_forest'\")\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        # Feature importance\n",
    "        if model_type == 'random_forest':\n",
    "            importances = dict(zip(available_predictors, model.feature_importances_))\n",
    "        else:\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 np.abs(model.named_steps['regressor'].coef_)))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'metrics': {\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                'n_samples': len(valid_data),\n",
    "                'trial_type': trial_type,\n",
    "                'condition': condition,\n",
    "                'predictors': available_predictors\n",
    "            },\n",
    "            'feature_importances': importances\n",
    "        }\n",
    "    \n",
    "    def run_classification_analysis(self, trial_type: str = 'invis', condition: str = 'max',\n",
    "                                   threshold: float = 0.68, model_type: str = 'logistic') -> Dict:\n",
    "        \"\"\"Run binary classification withtarget column naming.\"\"\"\n",
    "        \n",
    "        predictors = ['age', 'mot_noise']\n",
    "        available_predictors = [p for p in predictors if p in self.filtered_df.columns]\n",
    "        \n",
    "        # CORRECTED: Use the exact column format that exists\n",
    "        target_col = f'{trial_type}_sr_{condition}_const'\n",
    "        \n",
    "        if target_col not in self.filtered_df.columns:\n",
    "            available_cols = [col for col in self.filtered_df.columns if '_sr_' in col]\n",
    "            raise ValueError(f\"Target column {target_col} not found. Available: {available_cols}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        valid_data = self.filtered_df[available_predictors + [target_col]].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            raise ValueError(f\"Insufficient data: only {len(valid_data)} valid samples\")\n",
    "        \n",
    "        # Create binary target\n",
    "        y = (valid_data[target_col] >= threshold).astype(int)\n",
    "        X = valid_data[available_predictors]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        if model_type == 'logistic':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', LogisticRegression(random_state=42))\n",
    "            ])\n",
    "        elif model_type == 'random_forest':\n",
    "            model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', RandomForestClassifier(random_state=42))\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'logistic' or 'random_forest'\")\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'n_samples': len(valid_data),\n",
    "            'threshold': threshold,\n",
    "            'trial_type': trial_type,\n",
    "            'condition': condition,\n",
    "            'predictors': available_predictors\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if model_type == 'random_forest':\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 model.named_steps['classifier'].feature_importances_))\n",
    "        else:\n",
    "            importances = dict(zip(available_predictors, \n",
    "                                 model.named_steps['classifier'].coef_[0]))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'y_proba': y_proba,\n",
    "            'metrics': metrics,\n",
    "            'feature_importances': importances\n",
    "        }\n",
    "    \n",
    "    def run_mixed_effects_analysis(self, trial_types: Union[str, List[str]] = 'all') -> object:\n",
    "        \"\"\"Run mixed-effects analysis withcolumn naming.\"\"\"\n",
    "        \n",
    "        # Prepare long-format data\n",
    "        long_rows = []\n",
    "        \n",
    "        if trial_types == 'all':\n",
    "            trials_to_include = ['vis1', 'invis', 'vis2']\n",
    "        elif isinstance(trial_types, str):\n",
    "            trials_to_include = [trial_types]\n",
    "        else:\n",
    "            trials_to_include = trial_types\n",
    "        \n",
    "        for trial in trials_to_include:\n",
    "            for condition in ['max_const', 'min_const']:\n",
    "                # CORRECTED: Use the exact column format\n",
    "                sr_col = f\"{trial}_sr_{condition}\"\n",
    "                if sr_col in self.filtered_df.columns:\n",
    "                    sub_df = self.filtered_df[['ID', sr_col, 'mot_noise', 'age']].copy()\n",
    "                    sub_df = sub_df.rename(columns={sr_col: 'success_rate'})\n",
    "                    sub_df['trial_type'] = trial\n",
    "                    sub_df['condition'] = condition\n",
    "                    long_rows.append(sub_df)\n",
    "        \n",
    "        if not long_rows:\n",
    "            raise ValueError(\"No success rate data available\")\n",
    "        \n",
    "        df_long = pd.concat(long_rows, ignore_index=True)\n",
    "        df_long.dropna(subset=['success_rate', 'mot_noise', 'age'], inplace=True)\n",
    "        \n",
    "        # Fit model\n",
    "        if len(trials_to_include) == 1:\n",
    "            formula = 'success_rate ~ C(condition) + mot_noise + age'\n",
    "        else:\n",
    "            formula = 'success_rate ~ C(trial_type) + C(condition) + mot_noise + age'\n",
    "        \n",
    "        model = smf.ols(formula, data=df_long).fit()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a52a2-751e-445b-a081-5b2cf153751d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 7. VISUALIZER\n",
    "# ==============================================================================\n",
    "\n",
    "class MotorLearningVisualizer:\n",
    "    \"\"\"Handles all visualization withcolumn naming and stride change distribution.\"\"\"\n",
    "    \n",
    "    def __init__(self, metrics_df: pd.DataFrame, data_manager: MotorLearningDataManager = None, \n",
    "                 figures_dir: Path = Path('figures')):\n",
    "        self.metrics_df = metrics_df\n",
    "        self.data_manager = data_manager\n",
    "        self.figures_dir = figures_dir\n",
    "        self.figures_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Apply motor noise filter for most plots\n",
    "        if 'mot_noise' in metrics_df.columns:\n",
    "            self.filtered_df = metrics_df[metrics_df['mot_noise'] <= 0.3]\n",
    "        else:\n",
    "            self.filtered_df = metrics_df\n",
    "        \n",
    "        # Set colors\n",
    "        self.colors = {\n",
    "            'primary': '#667eea',\n",
    "            'secondary': '#764ba2',\n",
    "            'success': '#28a745',\n",
    "            'warning': '#ffc107',\n",
    "            'danger': '#dc3545'\n",
    "        }\n",
    "    \n",
    "    def plot_individual_stride_change_distribution(self, subject_id, trial_type='invis', \n",
    "                                                   stride_col='Sum of gains and steps',\n",
    "                                                   figsize=(16, 6), save=True, alpha=0.7,\n",
    "                                                   show_stats=True):\n",
    "        \"\"\"\n",
    "        Plot the distribution of stride length changes after success vs failure for an individual participant,\n",
    "        with separate plots for lower and upper target periods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        subject_id : str\n",
    "            Subject ID to analyze\n",
    "        trial_type : str\n",
    "            Trial type ('vis1', 'invis', 'vis2')\n",
    "        stride_col : str\n",
    "            Column name for stride length data (default: 'Sum of gains and steps')\n",
    "        figsize : tuple\n",
    "            Figure size (width, height)\n",
    "        save : bool\n",
    "            Whether to save the figure\n",
    "        alpha : float\n",
    "            Transparency for the filled areas under curves\n",
    "        show_stats : bool\n",
    "            Whether to print summary statistics\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing the calculated deltas and statistics for both conditions\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if data manager is available\n",
    "        if not self.data_manager:\n",
    "            print(\"Error: Data manager required for individual participant analysis\")\n",
    "            return None\n",
    "        \n",
    "        # Check if subject exists\n",
    "        if subject_id not in self.data_manager.processed_data:\n",
    "            print(f\"Error: Subject {subject_id} not found in processed data\")\n",
    "            available_subjects = list(self.data_manager.processed_data.keys())[:5]\n",
    "            print(f\"Available subjects (first 5): {available_subjects}\")\n",
    "            return None\n",
    "        \n",
    "        # Get trial data for the subject\n",
    "        trial_dict = self.data_manager.processed_data[subject_id]['trial_data'].get(trial_type)\n",
    "        if trial_dict is None:\n",
    "            print(f\"Error: No {trial_type} trial data found for subject {subject_id}\")\n",
    "            return None\n",
    "        \n",
    "        df = self.data_manager.get_trial_df(trial_dict)\n",
    "        if df is None or df.empty:\n",
    "            print(f\"Error: Empty or invalid trial data for subject {subject_id}, trial {trial_type}\")\n",
    "            return None\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = [stride_col, 'Success', 'Stride Number', 'Target size', 'Constant']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Error: Missing required columns: {missing_cols}\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return None\n",
    "        \n",
    "        # Helper function to get period data\n",
    "        def get_period_data(df, constant_condition, length=20):\n",
    "            \"\"\"Get data for specific condition period\"\"\"\n",
    "            if df is None or df.empty:\n",
    "                return None\n",
    "                \n",
    "            min_target = df['Target size'].min()\n",
    "            target_tolerance = 0.001\n",
    "            \n",
    "            min_target_periods = df[df['Target size'] <= min_target + target_tolerance]\n",
    "            if min_target_periods.empty:\n",
    "                return None\n",
    "                \n",
    "            const_value = (\n",
    "                min_target_periods['Constant'].max() if constant_condition == 'max'\n",
    "                else min_target_periods['Constant'].min()\n",
    "            )\n",
    "            \n",
    "            period_data = min_target_periods[\n",
    "                np.isclose(min_target_periods['Constant'], const_value, rtol=1e-5)\n",
    "            ]\n",
    "            \n",
    "            if period_data.empty:\n",
    "                return None\n",
    "                \n",
    "            return period_data.tail(length)\n",
    "        \n",
    "        # Get data for both target conditions\n",
    "        max_const_data = get_period_data(df, 'max')\n",
    "        min_const_data = get_period_data(df, 'min')\n",
    "        \n",
    "        if max_const_data is None and min_const_data is None:\n",
    "            print(f\"Error: No valid period data found for subject {subject_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Create side-by-side plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True)\n",
    "        if not isinstance(axes, np.ndarray):\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Get subject age for title\n",
    "        subject_age = self.data_manager.processed_data[subject_id]['metadata'].get('age_months', np.nan) / 12\n",
    "        \n",
    "        # Function to process and plot data for each condition\n",
    "        def plot_condition_data(ax, period_data, condition_name, const_type):\n",
    "            if period_data is None or period_data.empty:\n",
    "                ax.text(0.5, 0.5, f'No {condition_name}\\ndata available', \n",
    "                       ha='center', va='center', transform=ax.transAxes,\n",
    "                       fontsize=12, style='italic')\n",
    "                ax.set_title(f'{condition_name} Target')\n",
    "                return None\n",
    "            \n",
    "            # Sort by stride number and calculate stride length changes\n",
    "            period_sorted = period_data.sort_values('Stride Number').copy()\n",
    "            period_sorted['Delta'] = period_sorted[stride_col].diff().shift(-1)\n",
    "            \n",
    "            # Remove the last row (no next stride to compare to)\n",
    "            period_sorted = period_sorted[:-1]\n",
    "            \n",
    "            # Separate changes after success vs failure\n",
    "            success_deltas = period_sorted[period_sorted['Success'] == 1]['Delta'].dropna()\n",
    "            failure_deltas = period_sorted[period_sorted['Success'] == 0]['Delta'].dropna()\n",
    "            \n",
    "            if len(success_deltas) == 0 and len(failure_deltas) == 0:\n",
    "                ax.text(0.5, 0.5, f'No valid stride changes\\nfor {condition_name}', \n",
    "                       ha='center', va='center', transform=ax.transAxes,\n",
    "                       fontsize=12, style='italic')\n",
    "                ax.set_title(f'{condition_name} Target')\n",
    "                return None\n",
    "            \n",
    "            # Plot smooth distributions using kernel density estimation\n",
    "            if len(success_deltas) > 0:\n",
    "                try:\n",
    "                    # Check if we have enough data points for KDE (need at least 2)\n",
    "                    if len(success_deltas) >= 2:\n",
    "                        # Create kernel density estimate\n",
    "                        kde_success = gaussian_kde(success_deltas)\n",
    "                        \n",
    "                        # Create x values for smooth curve\n",
    "                        x_min, x_max = success_deltas.min(), success_deltas.max()\n",
    "                        if len(failure_deltas) > 0:\n",
    "                            x_min = min(x_min, failure_deltas.min())\n",
    "                            x_max = max(x_max, failure_deltas.max())\n",
    "                        \n",
    "                        # Extend range slightly for better visualization\n",
    "                        x_range = x_max - x_min\n",
    "                        if x_range > 0:  # Avoid division by zero\n",
    "                            x_min -= x_range * 0.1\n",
    "                            x_max += x_range * 0.1\n",
    "                        else:\n",
    "                            # If all values are the same, create a small range\n",
    "                            x_min -= 0.1\n",
    "                            x_max += 0.1\n",
    "                        \n",
    "                        x_smooth = np.linspace(x_min, x_max, 200)\n",
    "                        y_success = kde_success(x_smooth)\n",
    "                        \n",
    "                        # Plot smooth curve\n",
    "                        ax.plot(x_smooth, y_success, color='green', linewidth=3, alpha=0.8,\n",
    "                                label=f'After Success (n={len(success_deltas)})')\n",
    "                        \n",
    "                        # Fill under curve for better visibility\n",
    "                        ax.fill_between(x_smooth, y_success, alpha=alpha*0.5, color='green')\n",
    "                        \n",
    "                    else:\n",
    "                        # Too few points for KDE, use scatter plot or simple line\n",
    "                        for i, val in enumerate(success_deltas):\n",
    "                            ax.axvline(val, color='green', alpha=0.7, linewidth=2,\n",
    "                                      label=f'After Success (n={len(success_deltas)})' if i == 0 else \"\")\n",
    "                    \n",
    "                except (ImportError, np.linalg.LinAlgError, ValueError) as e:\n",
    "                    # Fallback to histogram or scatter for any KDE-related errors\n",
    "                    if len(success_deltas) >= 2:\n",
    "                        ax.hist(success_deltas, bins=min(10, len(success_deltas)), alpha=alpha, color='green', \n",
    "                                label=f'After Success (n={len(success_deltas)})', density=True, \n",
    "                                edgecolor='darkgreen', linewidth=0.5)\n",
    "                    else:\n",
    "                        # Single point or very few points - use vertical lines\n",
    "                        for i, val in enumerate(success_deltas):\n",
    "                            ax.axvline(val, color='green', alpha=0.7, linewidth=3,\n",
    "                                      label=f'After Success (n={len(success_deltas)})' if i == 0 else \"\")\n",
    "            \n",
    "            if len(failure_deltas) > 0:\n",
    "                try:\n",
    "                    # Check if we have enough data points for KDE\n",
    "                    if len(failure_deltas) >= 2:\n",
    "                        # Create kernel density estimate\n",
    "                        kde_failure = gaussian_kde(failure_deltas)\n",
    "                        \n",
    "                        # Use same x range as success if it exists, otherwise create new range\n",
    "                        if len(success_deltas) > 0 and 'x_smooth' in locals():\n",
    "                            # x_smooth already defined above\n",
    "                            pass\n",
    "                        else:\n",
    "                            x_min, x_max = failure_deltas.min(), failure_deltas.max()\n",
    "                            x_range = x_max - x_min\n",
    "                            if x_range > 0:\n",
    "                                x_min -= x_range * 0.1\n",
    "                                x_max += x_range * 0.1\n",
    "                            else:\n",
    "                                x_min -= 0.1\n",
    "                                x_max += 0.1\n",
    "                            x_smooth = np.linspace(x_min, x_max, 200)\n",
    "                        \n",
    "                        y_failure = kde_failure(x_smooth)\n",
    "                        \n",
    "                        # Plot smooth curve\n",
    "                        ax.plot(x_smooth, y_failure, color='red', linewidth=3, alpha=0.8,\n",
    "                                label=f'After Failure (n={len(failure_deltas)})')\n",
    "                        \n",
    "                        # Fill under curve for better visibility\n",
    "                        ax.fill_between(x_smooth, y_failure, alpha=alpha*0.5, color='red')\n",
    "                        \n",
    "                    else:\n",
    "                        # Too few points for KDE, use scatter plot or simple line\n",
    "                        for i, val in enumerate(failure_deltas):\n",
    "                            ax.axvline(val, color='red', alpha=0.7, linewidth=2,\n",
    "                                      label=f'After Failure (n={len(failure_deltas)})' if i == 0 else \"\")\n",
    "                    \n",
    "                except (ImportError, np.linalg.LinAlgError, ValueError) as e:\n",
    "                    # Fallback to histogram or scatter for any KDE-related errors\n",
    "                    if len(failure_deltas) >= 2:\n",
    "                        ax.hist(failure_deltas, bins=min(10, len(failure_deltas)), alpha=alpha, color='red', \n",
    "                                label=f'After Failure (n={len(failure_deltas)})', density=True,\n",
    "                                edgecolor='darkred', linewidth=0.5)\n",
    "                    else:\n",
    "                        # Single point or very few points - use vertical lines\n",
    "                        for i, val in enumerate(failure_deltas):\n",
    "                            ax.axvline(val, color='red', alpha=0.7, linewidth=3,\n",
    "                                      label=f'After Failure (n={len(failure_deltas)})' if i == 0 else \"\")\n",
    "            \n",
    "            # Add vertical lines for means\n",
    "            if len(success_deltas) > 0:\n",
    "                success_mean = success_deltas.mean()\n",
    "                ax.axvline(success_mean, color='darkgreen', linestyle='--', linewidth=2,\n",
    "                           label=f'Success Mean: {success_mean:.3f}')\n",
    "            \n",
    "            if len(failure_deltas) > 0:\n",
    "                failure_mean = failure_deltas.mean()\n",
    "                ax.axvline(failure_mean, color='darkred', linestyle='--', linewidth=2,\n",
    "                           label=f'Failure Mean: {failure_mean:.3f}')\n",
    "            \n",
    "            # Add zero line for reference\n",
    "            ax.axvline(0, color='black', linestyle='-', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Formatting\n",
    "            ax.set_xlabel(f'Change in {stride_col.replace(\"_\", \" \")}', fontsize=12)\n",
    "            ax.set_ylabel('Probability Density', fontsize=12)  # Always density for smooth curves\n",
    "            \n",
    "            ax.set_title(f'{condition_name} Target\\n(last 20 strides)', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # Calculate and display success rate for this period\n",
    "            if len(period_sorted) > 0:\n",
    "                success_rate = period_sorted['Success'].mean()\n",
    "                ax.text(0.02, 0.98, f'Success Rate: {success_rate:.1%}', \n",
    "                       transform=ax.transAxes, fontsize=11, fontweight='bold',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
    "                       verticalalignment='top', horizontalalignment='left')\n",
    "            \n",
    "            ax.legend(loc='upper right', fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Return statistics for this condition\n",
    "            condition_stats = {\n",
    "                f'{const_type}_total_strides': len(period_sorted),\n",
    "                f'{const_type}_success_rate': period_sorted['Success'].mean() if len(period_sorted) > 0 else None,\n",
    "                f'{const_type}_success_deltas': success_deltas.tolist() if len(success_deltas) > 0 else [],\n",
    "                f'{const_type}_failure_deltas': failure_deltas.tolist() if len(failure_deltas) > 0 else [],\n",
    "                f'{const_type}_n_success': len(success_deltas),\n",
    "                f'{const_type}_n_failure': len(failure_deltas)\n",
    "            }\n",
    "            \n",
    "            if len(success_deltas) > 0:\n",
    "                condition_stats.update({\n",
    "                    f'{const_type}_success_mean': success_deltas.mean(),\n",
    "                    f'{const_type}_success_std': success_deltas.std(),\n",
    "                    f'{const_type}_success_median': success_deltas.median()\n",
    "                })\n",
    "            \n",
    "            if len(failure_deltas) > 0:\n",
    "                condition_stats.update({\n",
    "                    f'{const_type}_failure_mean': failure_deltas.mean(),\n",
    "                    f'{const_type}_failure_std': failure_deltas.std(),\n",
    "                    f'{const_type}_failure_median': failure_deltas.median()\n",
    "                })\n",
    "            \n",
    "            # Statistical tests if both groups have data\n",
    "            if len(success_deltas) > 0 and len(failure_deltas) > 0:\n",
    "                try:\n",
    "                    from scipy.stats import mannwhitneyu, ttest_ind\n",
    "                    \n",
    "                    u_stat, u_p = mannwhitneyu(success_deltas, failure_deltas, alternative='two-sided')\n",
    "                    t_stat, t_p = ttest_ind(success_deltas, failure_deltas, equal_var=False)\n",
    "                    \n",
    "                    condition_stats.update({\n",
    "                        f'{const_type}_mann_whitney_u': u_stat,\n",
    "                        f'{const_type}_mann_whitney_p': u_p,\n",
    "                        f'{const_type}_t_statistic': t_stat,\n",
    "                        f'{const_type}_t_test_p': t_p\n",
    "                    })\n",
    "                    \n",
    "                except ImportError:\n",
    "                    print(\"scipy not available for statistical tests\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in statistical testing: {e}\")\n",
    "            \n",
    "            return condition_stats\n",
    "        \n",
    "        # Plot both conditions\n",
    "        max_stats = plot_condition_data(axes[0], max_const_data, 'Upper', 'max')\n",
    "        min_stats = plot_condition_data(axes[1], min_const_data, 'Lower', 'min')\n",
    "        \n",
    "        # Overall title\n",
    "        fig.suptitle(f'Stride Length Change Distributions\\n'\n",
    "                    f'Subject: {subject_id} (Age: {subject_age:.1f} years) - {trial_type.upper()} Trial', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        if save:\n",
    "            filename = f\"stride_change_dist_{subject_id}_{trial_type}_both_targets.png\"\n",
    "            plt.savefig(self.figures_dir / filename, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Combine statistics\n",
    "        stats = {\n",
    "            'subject_id': subject_id,\n",
    "            'trial_type': trial_type,\n",
    "        }\n",
    "        \n",
    "        if max_stats:\n",
    "            stats.update(max_stats)\n",
    "        if min_stats:\n",
    "            stats.update(min_stats)\n",
    "        \n",
    "        # Print statistics if requested\n",
    "        if show_stats:\n",
    "            print(f\"\\n=== Stride Change Statistics for {subject_id} ({trial_type}) ===\")\n",
    "            \n",
    "            for condition, const_type in [('Upper Target', 'max'), ('Lower Target', 'min')]:\n",
    "                if f'{const_type}_total_strides' in stats:\n",
    "                    print(f\"\\n--- {condition} ---\")\n",
    "                    print(f\"Total strides analyzed: {stats[f'{const_type}_total_strides']}\")\n",
    "                    if f'{const_type}_success_rate' in stats and stats[f'{const_type}_success_rate'] is not None:\n",
    "                        print(f\"Success rate: {stats[f'{const_type}_success_rate']:.1%}\")\n",
    "                    \n",
    "                    if stats[f'{const_type}_n_success'] > 0:\n",
    "                        print(f\"After Success (n={stats[f'{const_type}_n_success']}):\")\n",
    "                        print(f\"  Mean: {stats[f'{const_type}_success_mean']:.3f}\")\n",
    "                        print(f\"  Std:  {stats[f'{const_type}_success_std']:.3f}\")\n",
    "                        print(f\"  Median: {stats[f'{const_type}_success_median']:.3f}\")\n",
    "                    \n",
    "                    if stats[f'{const_type}_n_failure'] > 0:\n",
    "                        print(f\"After Failure (n={stats[f'{const_type}_n_failure']}):\")\n",
    "                        print(f\"  Mean: {stats[f'{const_type}_failure_mean']:.3f}\")\n",
    "                        print(f\"  Std:  {stats[f'{const_type}_failure_std']:.3f}\")\n",
    "                        print(f\"  Median: {stats[f'{const_type}_failure_median']:.3f}\")\n",
    "                    \n",
    "                    if f'{const_type}_mann_whitney_p' in stats:\n",
    "                        print(f\"Statistical Tests:\")\n",
    "                        print(f\"  Mann-Whitney U test: p = {stats[f'{const_type}_mann_whitney_p']:.4f}\")\n",
    "                        print(f\"  T-test: p = {stats[f'{const_type}_t_test_p']:.4f}\")\n",
    "                        significance = \"significant\" if min(stats[f'{const_type}_mann_whitney_p'], stats[f'{const_type}_t_test_p']) < 0.05 else \"not significant\"\n",
    "                        print(f\"  Difference is {significance} (α = 0.05)\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    def plot_age_performance_relationship(self, trial_type: str = 'invis', \n",
    "                                        metric: str = 'sr', save: bool = True) -> None:\n",
    "        \"\"\"Plot relationship between age and performance withcolumn naming.\"\"\"\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # CORRECTED: Use the exact column format\n",
    "        max_col = f'{trial_type}_{metric}_max_const'\n",
    "        min_col = f'{trial_type}_{metric}_min_const'\n",
    "        \n",
    "        for ax, col, title in zip([ax1, ax2], [max_col, min_col], \n",
    "                                 ['Max Constant', 'Min Constant']):\n",
    "            \n",
    "            valid_data = self.filtered_df[self.filtered_df[col].notna()]\n",
    "            if valid_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Scatter plot\n",
    "            scatter = ax.scatter(\n",
    "                valid_data['age'],\n",
    "                valid_data[col],\n",
    "                c=valid_data['mot_noise'] if 'mot_noise' in valid_data.columns else self.colors['primary'],\n",
    "                cmap='plasma',\n",
    "                alpha=0.7,\n",
    "                edgecolors='white',\n",
    "                linewidths=0.5\n",
    "            )\n",
    "            \n",
    "            # Trend line\n",
    "            self._add_trendline(ax, valid_data['age'], valid_data[col])\n",
    "            \n",
    "            ax.set_title(f'{trial_type.upper()}: {title}')\n",
    "            ax.set_xlabel('Age (years)')\n",
    "            ax.set_ylabel(metric.upper().replace('_', ' '))\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Add colorbar if using motor noise coloring\n",
    "        if 'mot_noise' in self.filtered_df.columns:\n",
    "            plt.colorbar(scatter, ax=[ax1, ax2], label='Motor Noise')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.figures_dir / f'{trial_type}_{metric}_vs_age.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def create_summary_dashboard(self, save: bool = True) -> None:\n",
    "        \"\"\"Create comprehensive summary dashboard withcolumn references.\"\"\"\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Age distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        if 'age' in self.filtered_df.columns:\n",
    "            self.filtered_df['age'].hist(bins=15, ax=ax1, color=self.colors['primary'], alpha=0.7)\n",
    "        ax1.set_title('Age Distribution')\n",
    "        ax1.set_xlabel('Age (years)')\n",
    "        ax1.set_ylabel('Count')\n",
    "        \n",
    "        # 2. Motor noise distribution\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        if 'mot_noise' in self.filtered_df.columns:\n",
    "            self.filtered_df['mot_noise'].hist(bins=15, ax=ax2, color=self.colors['secondary'], alpha=0.7)\n",
    "            ax2.axvline(x=0.15, color='red', linestyle='--', label='Threshold')\n",
    "            ax2.legend()\n",
    "        ax2.set_title('Motor Noise Distribution')\n",
    "        ax2.set_xlabel('Motor Noise')\n",
    "        ax2.set_ylabel('Count')\n",
    "        \n",
    "        # 3. Success rates by trial type withcolumn naming\n",
    "        ax3 = fig.add_subplot(gs[0, 2:])\n",
    "        trial_data = []\n",
    "        for trial in ['vis1', 'invis', 'vis2']:\n",
    "            for condition in ['max_const', 'min_const']:\n",
    "                # CORRECTED: Use exact column format\n",
    "                col = f'{trial}_sr_{condition}'\n",
    "                if col in self.filtered_df.columns:\n",
    "                    values = self.filtered_df[col].dropna()\n",
    "                    for val in values:\n",
    "                        trial_data.append({\n",
    "                            'trial_type': trial,\n",
    "                            'condition': condition.replace('_const', ''),\n",
    "                            'success_rate': val\n",
    "                        })\n",
    "        \n",
    "        if trial_data:\n",
    "            trial_df = pd.DataFrame(trial_data)\n",
    "            sns.boxplot(data=trial_df, x='trial_type', y='success_rate', \n",
    "                       hue='condition', ax=ax3)\n",
    "            ax3.set_title('Success Rates by Trial Type')\n",
    "            ax3.set_ylabel('Success Rate')\n",
    "        \n",
    "        # 4. Correlation matrix\n",
    "        ax4 = fig.add_subplot(gs[1, :2])\n",
    "        corr_cols = ['age']\n",
    "        if 'mot_noise' in self.filtered_df.columns:\n",
    "            corr_cols.append('mot_noise')\n",
    "        \n",
    "        # Add success rate columns withnaming\n",
    "        sr_cols = [col for col in self.filtered_df.columns if '_sr_' in col and '_const' in col]\n",
    "        corr_cols.extend(sr_cols[:6])  # Limit to prevent overcrowding\n",
    "        \n",
    "        corr_data = self.filtered_df[corr_cols].corr()\n",
    "        sns.heatmap(corr_data, annot=True, cmap='RdBu_r', center=0, ax=ax4)\n",
    "        ax4.set_title('Correlation Matrix')\n",
    "        \n",
    "        # 5. Motor noise vs success rate scatter withcolumn naming\n",
    "        ax5 = fig.add_subplot(gs[1, 2:])\n",
    "        target_col = 'invis_sr_max_const'  #format\n",
    "        if 'mot_noise' in self.filtered_df.columns and target_col in self.filtered_df.columns:\n",
    "            valid_data = self.filtered_df.dropna(subset=['mot_noise', target_col])\n",
    "            scatter = ax5.scatter(valid_data['mot_noise'], valid_data[target_col],\n",
    "                                c=valid_data['age'], cmap='viridis', alpha=0.7)\n",
    "            plt.colorbar(scatter, ax=ax5, label='Age (years)')\n",
    "            ax5.set_xlabel('Motor Noise')\n",
    "            ax5.set_ylabel('Success Rate (Invis)')\n",
    "            ax5.set_title('Motor Noise vs Learning Performance')\n",
    "        \n",
    "        # 6. Sample statistics\n",
    "        ax6 = fig.add_subplot(gs[2:, :])\n",
    "        \n",
    "        stats_data = {\n",
    "            'Metric': ['Sample Size', 'Age Range', 'Mean Age', 'Motor Noise Range', 'Mean Motor Noise'],\n",
    "            'Value': [\n",
    "                f\"{len(self.filtered_df)}\",\n",
    "                f\"{self.filtered_df['age'].min():.1f} - {self.filtered_df['age'].max():.1f} years\" if 'age' in self.filtered_df.columns else \"N/A\",\n",
    "                f\"{self.filtered_df['age'].mean():.1f} years\" if 'age' in self.filtered_df.columns else \"N/A\",\n",
    "                f\"{self.filtered_df['mot_noise'].min():.3f} - {self.filtered_df['mot_noise'].max():.3f}\" if 'mot_noise' in self.filtered_df.columns else \"N/A\",\n",
    "                f\"{self.filtered_df['mot_noise'].mean():.3f}\" if 'mot_noise' in self.filtered_df.columns else \"N/A\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        ax6.axis('tight')\n",
    "        ax6.axis('off')\n",
    "        table = ax6.table(cellText=[[stats_data['Metric'][i], stats_data['Value'][i]] \n",
    "                                   for i in range(len(stats_data['Metric']))],\n",
    "                         colLabels=['Metric', 'Value'],\n",
    "                         cellLoc='center',\n",
    "                         loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        table.scale(1.2, 1.5)\n",
    "        ax6.set_title('Dataset Summary Statistics', pad=20)\n",
    "        \n",
    "        plt.suptitle('Motor Learning Analysis Dashboard', fontsize=20, y=0.98)\n",
    "        \n",
    "        if save:\n",
    "            plt.savefig(self.figures_dir / 'summary_dashboard.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _add_trendline(self, ax, x, y):\n",
    "        \"\"\"Add trendline with correlation coefficient to plot.\"\"\"\n",
    "        x_clean = pd.to_numeric(x, errors='coerce')\n",
    "        y_clean = pd.to_numeric(y, errors='coerce')\n",
    "        valid = x_clean.notna() & y_clean.notna()\n",
    "        \n",
    "        if valid.sum() < 2:\n",
    "            return\n",
    "        \n",
    "        x_vals = x_clean[valid]\n",
    "        y_vals = y_clean[valid]\n",
    "        \n",
    "        # Fit line\n",
    "        coeffs = np.polyfit(x_vals, y_vals, 1)\n",
    "        trendline = np.poly1d(coeffs)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        r, p = pearsonr(x_vals, y_vals)\n",
    "        \n",
    "        # Plot trendline\n",
    "        ax.plot(x_vals, trendline(x_vals), 'r--', alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Add correlation text\n",
    "        ax.text(0.05, 0.95, f'r = {r:.3f}', transform=ax.transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b65ce6-705a-4a3a-8bf2-1dd1aaeacfea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 8. ANALYSIS PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "class MotorLearningAnalysis:\n",
    "    \"\"\"Complete analysis pipeline with all corrections.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_manager, metrics_df: pd.DataFrame):\n",
    "        self.data_manager = data_manager\n",
    "        self.metrics_df = metrics_df\n",
    "        self.analyzer = StatisticalAnalyzer(metrics_df)\n",
    "        self.visualizer = MotorLearningVisualizer(metrics_df, data_manager)\n",
    "    \n",
    "    def run_comprehensive_analysis(self, save_all: bool = True) -> Dict:\n",
    "        \"\"\"Run complete analysis pipeline with all bug fixes.\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(\"🔬 Running Comprehensive Motor Learning Analysis...\")\n",
    "        \n",
    "        # Print age summary\n",
    "        if 'age' in self.metrics_df.columns:\n",
    "            ages = self.metrics_df['age'].dropna()\n",
    "            print(f\"📊 Age range: {ages.min():.1f} - {ages.max():.1f} years (n={len(ages)})\")\n",
    "        \n",
    "        # Print available columns for debugging\n",
    "        sr_cols = [col for col in self.metrics_df.columns if '_sr_' in col]\n",
    "        print(f\"🎯 Available success rate columns: {sr_cols}\")\n",
    "        \n",
    "        # 1. Regression Analysis\n",
    "        print(\"📊 Running regression analysis...\")\n",
    "        try:\n",
    "            regression_results = self.analyzer.run_regression_analysis()\n",
    "            results['regression'] = regression_results\n",
    "            print(f\"✅ Regression R² = {regression_results['metrics']['r2']:.3f}\")\n",
    "            \n",
    "            # Show feature effects\n",
    "            for feature, importance in regression_results['feature_importances'].items():\n",
    "                print(f\"   {feature}: {importance:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Regression analysis failed: {e}\")\n",
    "        \n",
    "        # 2. Classification Analysis\n",
    "        print(\"🎯 Running classification analysis...\")\n",
    "        try:\n",
    "            classification_results = self.analyzer.run_classification_analysis()\n",
    "            results['classification'] = classification_results\n",
    "            print(f\"✅ Classification AUC = {classification_results['metrics']['roc_auc']:.3f}\")\n",
    "            print(f\"   Accuracy = {classification_results['metrics']['accuracy']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Classification analysis failed: {e}\")\n",
    "        \n",
    "        # 3. Mixed Effects Analysis\n",
    "        print(\"📈 Running mixed-effects analysis...\")\n",
    "        try:\n",
    "            mixed_model = self.analyzer.run_mixed_effects_analysis()\n",
    "            results['mixed_effects'] = mixed_model\n",
    "            print(f\"✅ Mixed-effects R² = {mixed_model.rsquared:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Mixed-effects analysis failed: {e}\")\n",
    "        \n",
    "        # 4. Visualization Suite\n",
    "        print(\"📊 Generating visualizations...\")\n",
    "        try:\n",
    "            self.visualizer.plot_age_performance_relationship(save=save_all)\n",
    "            self.visualizer.create_summary_dashboard(save=save_all)\n",
    "            print(\"✅ Visualizations complete\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Visualization failed: {e}\")\n",
    "        \n",
    "        print(\"🎉 Comprehensive analysis complete!\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e728567-61cc-4bd5-98e6-a581c626a2c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 9. USAGE FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def run_complete_motor_learning_analysis_enhanced(metadata_path: str, data_root_dir: str, \n",
    "                                                  force_reprocess: bool = False) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Complete enhanced workflow for motor learning analysis with stride change distribution.\n",
    "    ALL COLUMN NAMING BUGS FIXED AND STRIDE ANALYSIS ADDED.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting ENHANCED Motor Learning Analysis...\")\n",
    "    print(\"🔧 All column naming bugs fixed\")\n",
    "    print(\"📊 Added individual stride change distribution plotting\")\n",
    "    print(\"📏 Age handling: Consistent use of age_months/12 as 'age' throughout\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Data Loading and Processing\n",
    "    print(\"📂 Loading and processing data...\")\n",
    "    data_manager = MotorLearningDataManager(\n",
    "        metadata_path=metadata_path,\n",
    "        data_root_dir=data_root_dir,\n",
    "        force_reprocess=force_reprocess,\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "    # Apply quality filters\n",
    "    filtered_data_manager = data_manager.filter_trials(\n",
    "        max_target_size=0.31,\n",
    "        max_strides=415,\n",
    "        required_trial_types=['pref', 'invis']\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Data loaded: {len(filtered_data_manager.processed_data)} subjects after filtering\")\n",
    "    \n",
    "    # 2. Calculate Metrics withnaming\n",
    "    print(\"🧮 Calculating performance metrics...\")\n",
    "    calculator = MetricsCalculator(filtered_data_manager)\n",
    "    metrics_df = calculator.calculate_all_metrics()\n",
    "    \n",
    "    print(f\"✅ Metrics calculated: {len(metrics_df)} subjects, {len(metrics_df.columns)} metrics\")\n",
    "    \n",
    "    # 3. Run ENHANCED Comprehensive Analysis\n",
    "    print(\"🔬 Running enhanced comprehensive analysis...\")\n",
    "    analysis = MotorLearningAnalysis(filtered_data_manager, metrics_df)\n",
    "    results = analysis.run_comprehensive_analysis(save_all=True)\n",
    "    \n",
    "    # 4. Print Final Summary\n",
    "    print(\"📊 Final Summary:\")\n",
    "    print(f\"   Total subjects analyzed: {len(metrics_df)}\")\n",
    "    if 'age' in metrics_df.columns:\n",
    "        ages = metrics_df['age'].dropna()\n",
    "        print(f\"   Age range: {ages.min():.1f} - {ages.max():.1f} years\")\n",
    "        print(f\"   Mean age: {ages.mean():.1f} ± {ages.std():.1f} years\")\n",
    "    \n",
    "    success_cols = [col for col in metrics_df.columns if '_sr_' in col]\n",
    "    print(f\"   Success rate metrics: {len(success_cols)} columns\")\n",
    "    print(f\"   Figures saved to: {analysis.visualizer.figures_dir}\")\n",
    "    \n",
    "    if 'regression' in results:\n",
    "        print(f\"   🎯 Key finding: Age effect = {results['regression']['feature_importances'].get('age', 0):.4f} per year\")\n",
    "    \n",
    "    print(\"\\n🆕 NEW FEATURE: Individual stride change distribution plotting\")\n",
    "    print(\"   Use: analysis.visualizer.plot_individual_stride_change_distribution('SUBJECT_ID', 'TRIAL_TYPE')\")\n",
    "    print(\"   Example: analysis.visualizer.plot_individual_stride_change_distribution('MUH1172', 'vis2')\")\n",
    "    \n",
    "    return metrics_df, results, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513a148-3157-417c-bb64-470f5f87f3f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 10. USAGE INSTRUCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def demonstrate_stride_analysis(analysis, subject_ids: List[str] = None):\n",
    "    \"\"\"Demonstrate the new stride change distribution analysis.\"\"\"\n",
    "    \n",
    "    if subject_ids is None:\n",
    "        # Get a few example subjects\n",
    "        available_subjects = list(analysis.data_manager.processed_data.keys())\n",
    "        subject_ids = available_subjects[:3] if len(available_subjects) >= 3 else available_subjects\n",
    "    \n",
    "    print(\"🔍 Demonstrating Individual Stride Change Distribution Analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        print(f\"\\n📊 Analyzing subject: {subject_id}\")\n",
    "        \n",
    "        for trial_type in ['vis1', 'invis', 'vis2']:\n",
    "            # Check if subject has this trial type\n",
    "            trial_dict = analysis.data_manager.processed_data[subject_id]['trial_data'].get(trial_type)\n",
    "            if trial_dict and trial_dict.get('data') is not None:\n",
    "                print(f\"   🎯 Plotting {trial_type} trial...\")\n",
    "                try:\n",
    "                    stats = analysis.visualizer.plot_individual_stride_change_distribution(\n",
    "                        subject_id, trial_type, save=True, show_stats=False\n",
    "                    )\n",
    "                    if stats:\n",
    "                        print(f\"      ✅ Analysis complete for {subject_id} {trial_type}\")\n",
    "                    else:\n",
    "                        print(f\"      ⚠️ No valid data for {subject_id} {trial_type}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"      ❌ Error analyzing {subject_id} {trial_type}: {e}\")\n",
    "            else:\n",
    "                print(f\"      ⚠️ No {trial_type} data for {subject_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479cda0-19a5-414a-b741-ec7b89b63f17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 11. SIMPLE USAGE WITH NEW FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use your existing data paths\n",
    "    metadata_path = 'muh_metadata.csv'\n",
    "    data_root_dir = 'E:/muh_data/'  # Update this path as needed\n",
    "    \n",
    "    print(\"🚀 Running Enhanced Motor Learning Analysis...\")\n",
    "    \n",
    "    # Run the ENHANCED analysis\n",
    "    metrics_df, results, analysis = run_complete_motor_learning_analysis_enhanced(\n",
    "        metadata_path, data_root_dir, force_reprocess=False\n",
    "    )\n",
    "    \n",
    "    # Demonstrate the new stride analysis feature\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🆕 DEMONSTRATING NEW STRIDE CHANGE ANALYSIS FEATURE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Example usage of the new function\n",
    "    example_subjects = ['MUH1172']  # You can add more subject IDs here\n",
    "    demonstrate_stride_analysis(analysis, example_subjects)\n",
    "    \n",
    "    print(\"\\n✅ Enhanced analysis complete!\")\n",
    "    print(\"📊 All standard analyses completed\")\n",
    "    print(\"🆕 Individual stride change distribution analysis demonstrated\")\n",
    "    print(f\"📁 All figures saved to: {analysis.visualizer.figures_dir}\")\n",
    "    \n",
    "    # Print usage instructions\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📖 HOW TO USE THE NEW STRIDE ANALYSIS FEATURE:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"# Plot stride change distribution for any subject and trial:\")\n",
    "    print(\"analysis.visualizer.plot_individual_stride_change_distribution('MUH1172', 'vis2')\")\n",
    "    print(\"\\n# Parameters you can adjust:\")\n",
    "    print(\"# - subject_id: Any subject ID from your dataset\")\n",
    "    print(\"# - trial_type: 'vis1', 'invis', or 'vis2'\")  \n",
    "    print(\"# - stride_col: Column to analyze (default: 'Sum of gains and steps')\")\n",
    "    print(\"# - save: Whether to save the figure (default: True)\")\n",
    "    print(\"# - show_stats: Whether to print detailed statistics (default: True)\")\n",
    "    print(\"# - figsize: Size of the figure (default: (16, 6))\")\n",
    "    print(\"# - alpha: Transparency of the filled areas (default: 0.7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b8ff0-eb09-4ae4-a8c8-f006e5c61fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
